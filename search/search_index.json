{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"LLM-IE Documentation","text":"<p>LLM-IE is a toolkit that provides robust information extraction utilities for named entity, entity attributes, and entity relation extraction. The flowchart below demonstrates the workflow:</p> <ol> <li>Prompt engineering with LLM agent: Prompt Editor</li> <li>Prompting algorithm design: Extractors</li> <li>Visualization &amp; Validation: Visualization</li> <li>Repeat step #1-#3 until achieves high accuracy</li> </ol> <p>For more information and benchmarks, please check our paper: <pre><code>@article{hsu2025llm,\n  title={LLM-IE: a python package for biomedical generative information extraction with large language models},\n  author={Hsu, Enshuo and Roberts, Kirk},\n  journal={JAMIA open},\n  volume={8},\n  number={2},\n  pages={ooaf012},\n  year={2025},\n  publisher={Oxford University Press}\n}\n</code></pre></p>"},{"location":"extractors/","title":"Extractors","text":"<p>An extractor implements a prompting algorithm for information extraction. There are two main extractor families: <code>FrameExtractor</code> and <code>RelationExtractor</code>.  The <code>FrameExtractor</code> extracts named entities with attributes (\"frames\"). The <code>RelationExtractor</code> extracts the relations (and relation types) between frames. Under <code>FrameExtractor</code>, we made pre-packaged extractors that does not require much configuation and are often sufficient for regular use case (Convenience FrameExtractor).</p>"},{"location":"extractors/#frameextractor","title":"FrameExtractor","text":"<p>Frame extractors in general adopts an unit-context schema. The purpose is to avoid having LLM to process long context and suffer from needle in the haystack challenge. We split an input document into multiple units. LLM only process a unit of text at a time. </p> <ul> <li>Unit: a text snippet that LLM extrator will process at a time. It could be a sentence, a line of text, or a paragraph. </li> <li>Context: the context around the unit. For exapmle, a slidewindow of 2 sentences before and after. Context is optional. </li> </ul> <p></p>"},{"location":"extractors/#directframeextractor","title":"DirectFrameExtractor","text":"<p>The <code>DirectFrameExtractor</code> implements the unit-context schema. We start by defining the unit using one of the <code>UnitChunker</code>. The <code>SentenceUnitChunker</code> chunks the input document into sentences. Then, we define how context should be provided by choosing one of the <code>ContextChunker</code>. The <code>SlideWindowContextChunker</code> parse 2 units (sentences in this case) before and after each unit as context. For more options, see Chunkers.</p> <pre><code>from llm_ie import DirectFrameExtractor, SentenceUnitChunker, SlideWindowContextChunker\n\nunit_chunker = SentenceUnitChunker()\ncontext_chunker = SlideWindowContextChunker(window_size=2)\nextractor = DirectFrameExtractor(inference_engine=llm, \n                                 unit_chunker=unit_chunker,\n                                 context_chunker=context_chunker,\n                                 prompt_template=prompt_template)\n</code></pre>"},{"location":"extractors/#reviewframeextractor","title":"ReviewFrameExtractor","text":"<p>The <code>ReviewFrameExtractor</code> is a child of <code>DirectFrameExtractor</code>. It adds a review step after the initial output. There are two review modes:</p> <ol> <li>Addition mode: add more frames while keeping current. This is efficient for boosting recall. </li> <li>Revision mode: regenerate frames (add new and delete existing). </li> </ol> <p>Under the Addition mode (<code>review_mode=\"addition\"</code>), the <code>review_prompt</code> needs to instruct the LLM not to regenerate existing extractions:</p> <p>... You should ONLY add new diagnoses. DO NOT regenerate the entire answer.</p> <p>Under the Revision mode (<code>review_mode=\"revision\"</code>), the <code>review_prompt</code> needs to instruct the LLM to regenerate:</p> <p>... Regenerate your output.</p> <p>It is recommended to leave the <code>review_prompt=None</code> and use the default, unless there are special needs. </p> <pre><code>from llm_ie import ReviewFrameExtractor, SentenceUnitChunker, SlideWindowContextChunker\n\nunit_chunker = SentenceUnitChunker()\ncontext_chunker = SlideWindowContextChunker(window_size=2)\nextractor = ReviewFrameExtractor(inference_engine=llm, \n                                 unit_chunker=unit_chunker,\n                                 context_chunker=context_chunker,\n                                 prompt_template=prompt_template,\n                                 review_mode=\"addition\")\n</code></pre>"},{"location":"extractors/#post-processing","title":"Post-processing","text":"<p>Since the output entity text from LLMs might not be consistent with the original text due to the limitations of LLMs, we apply JSON repair, case-sensitive, fuzzy search, and entity overlap settings in post-processing to find the accurate entity span. </p>"},{"location":"extractors/#json-repair","title":"JSON repair","text":"<p>Automatically detect and fix broken JSON format with json_repair.</p>"},{"location":"extractors/#case-sensitive","title":"Case sensitive","text":"<p>set <code>case_sensitive=False</code> to allow matching even when LLM generates inconsistent upper/lower cases. </p>"},{"location":"extractors/#fussy-match","title":"Fussy match","text":"<p>In the <code>extract_frames()</code> method, setting parameter <code>fuzzy_match=True</code> applies Jaccard similarity matching. The most likely spans will be returned as entity text.</p>"},{"location":"extractors/#entity-overlap","title":"Entity overlap","text":"<p>Set <code>allow_overlap_entities=True</code> to cpature overlapping entities. Note that this can cause multiple frames to be generated on the same entity span if they have same entity text.</p>"},{"location":"extractors/#concurrent-optimization","title":"Concurrent Optimization","text":"<p>For concurrent extraction (recommended), set <code>concurrent=True</code> in <code>FrameExtractor.extract_frames</code>. The <code>concurrent_batch_size</code> sets the batch size of units to be processed in cocurrent.</p>"},{"location":"extractors/#convenience-frameextractor","title":"Convenience FrameExtractor","text":"<p>The <code>DirectFrameExtractor</code> and <code>ReviewFrameExtractor</code> provide flexible interfaces for all settings. However, in most use cases, simple interface is preferred. We pre-package some common (and high performance) settings for convenience. </p>"},{"location":"extractors/#basicframeextractor","title":"BasicFrameExtractor","text":"<p>The <code>BasicFrameExtractor</code> prompts LLM with the entire document.</p> <pre><code>from llm_ie import BasicFrameExtractor\n\nextractor = BasicFrameExtractor(inference_engine, prompt_template)\n</code></pre> <p>It is equivalent to:</p> <pre><code>from llm_ie import DirectFrameExtractor, WholeDocumentUnitChunker, NoContextChunker\n\nunit_chunker = WholeDocumentUnitChunker()\ncontext_chunker = NoContextChunker()\nextractor = DirectFrameExtractor(inference_engine=llm, \n                                 unit_chunker=unit_chunker,\n                                 context_chunker=context_chunker,\n                                 prompt_template=prompt_template)\n</code></pre>"},{"location":"extractors/#basicreviewframeextractor","title":"BasicReviewFrameExtractor","text":"<p>Similar to the <code>BasicFrameExtractor</code>, but adds a revieww step after the initial outputs.</p> <pre><code>from llm_ie import BasicReviewFrameExtractor\n\nextractor = BasicReviewFrameExtractor(inference_engine, prompt_template, review_mode=\"revision\")\n</code></pre> <p>This is equivalent to:</p> <pre><code>from llm_ie import ReviewFrameExtractor, WholeDocumentUnitChunker, NoContextChunker\n\nunit_chunker = WholeDocumentUnitChunker()\ncontext_chunker = NoContextChunker()\nextractor = ReviewFrameExtractor(inference_engine=llm, \n                                 unit_chunker=unit_chunker,\n                                 context_chunker=context_chunker,\n                                 prompt_template=prompt_template,\n                                 review_mode=\"revision\")\n</code></pre>"},{"location":"extractors/#sentenceframeextractor","title":"SentenceFrameExtractor","text":"<p>The <code>SentenceFrameExtractor</code> prompts LLMs to extract sentence-by-sentence. The <code>context_sentences</code> sets number of sentences before and after the sentence of interest to provide additional context. When <code>context_sentences=2</code>, 2 sentences before and 2 sentences after are included in the user prompt as context. When <code>context_sentences=\"all\"</code>, the entire document is included as context. When <code>context_sentences=0</code>, no context is provided and LLM will only extract based on the current sentence of interest.</p> <pre><code>from llm_ie import SentenceFrameExtractor\n\n# slide window of 2 sentences as context\nextractor = SentenceFrameExtractor(inference_engine, prompt_template, context_sentences=2)\n</code></pre> <p>It is equivalent to:</p> <pre><code>from llm_ie import DirectFrameExtractor, SentenceUnitChunker, SlideWindowContextChunker\n\nunit_chunker = SentenceUnitChunker()\ncontext_chunker = SlideWindowContextChunker(window_size=2)\nextractor = DirectFrameExtractor(inference_engine=llm, \n                                 unit_chunker=unit_chunker,\n                                 context_chunker=context_chunker,\n                                 prompt_template=prompt_template)\n</code></pre>"},{"location":"extractors/#sentencereviewframeextractor","title":"SentenceReviewFrameExtractor","text":"<p>The <code>SentenceReviewFrameExtractor</code> performs sentence-level extraction and review. The example below use no context, revision review mode.</p> <pre><code>from llm_ie import SentenceReviewFrameExtractor\n\nextractor = SentenceReviewFrameExtractor(inference_engine, prompt_temp, context_sentences=0, review_mode=\"revision\")\n</code></pre> <p>It is equivalent to:</p> <pre><code>from llm_ie import ReviewFrameExtractor, SentenceUnitChunker, NoContextChunker\n\nunit_chunker = SentenceUnitChunker()\ncontext_chunker = NoContextChunker()\nextractor = ReviewFrameExtractor(inference_engine=llm, \n                                 unit_chunker=unit_chunker,\n                                 context_chunker=context_chunker,\n                                 prompt_template=prompt_template,\n                                 review_mode=\"revision\")\n</code></pre>"},{"location":"extractors/#relationextractor","title":"RelationExtractor","text":"<p>Relation extractors prompt LLM with combinations of two frames from a document (<code>LLMInformationExtractionDocument</code>) and extract relations. The <code>BinaryRelationExtractor</code> extracts binary relations (yes/no) between two frames. The <code>MultiClassRelationExtractor</code> extracts relations and assign relation types (\"multi-class\"). </p> <p>An important feature of the relation extractors is that users are required to define a <code>possible_relation_func</code> or <code>possible_relation_types_func</code> function for the extractors. The reason is, there are too many possible combinations of two frames (N choose 2 combinations). The <code>possible_relation_func</code> helps rule out impossible combinations and therefore, reduce the LLM inferencing burden.</p>"},{"location":"extractors/#binaryrelationextractor","title":"BinaryRelationExtractor","text":"<p>Use the get_prompt_guide() method to inspect the prompt template guideline for BinaryRelationExtractor. <pre><code>from llm_ie.extractors import BinaryRelationExtractor\n\nprint(BinaryRelationExtractor.get_prompt_guide())\n</code></pre></p> <pre><code>Prompt Template Design:\n\n1. Task description:\n   Provide a detailed description of the task, including the background and the type of task (e.g., binary relation extraction). Mention the region of interest (ROI) text. \n2. Schema definition: \n   List the criterion for relation (True) and for no relation (False).\n\n3. Output format definition:\n   The ouptut must be a dictionary with a key \"Relation\" (i.e., {\"Relation\": \"&lt;True or False&gt;\"}).\n\n4. (optional) Hints:\n   Provide itemized hints for the information extractors to guide the extraction process.\n\n5. (optional) Examples:\n   Include examples in the format:  \n    Input: ...  \n    Output: ...\n\n6. Entity 1 full information:\n   Include a placeholder in the format {{&lt;frame_1&gt;}}\n\n7. Entity 2 full information:\n   Include a placeholder in the format {{&lt;frame_2&gt;}}\n\n8. Input placeholders:\n   The template must include a placeholder \"{{roi_text}}\" for the ROI text.\n\n\nExample:\n\n    # Task description\n    This is a binary relation extraction task. Given a region of interest (ROI) text and two entities from a medical note, indicate the relation existence between the two entities.\n\n    # Schema definition\n        True: if there is a relationship between a medication name (one of the entities) and its strength or frequency (the other entity).\n        False: Otherwise.\n\n    # Output format definition\n    Your output should follow the JSON format:\n    {\"Relation\": \"&lt;True or False&gt;\"}\n\n    I am only interested in the content between []. Do not explain your answer. \n\n    # Hints\n        1. Your input always contains one medication entity and 1) one strength entity or 2) one frequency entity.\n        2. Pay attention to the medication entity and see if the strength or frequency is for it.\n        3. If the strength or frequency is for another medication, output False. \n        4. If the strength or frequency is for the same medication but at a different location (span), output False.\n\n    # Entity 1 full information:\n    {{frame_1}}\n\n    # Entity 2 full information:\n    {{frame_2}}\n\n    # Input placeholders\n    ROI Text with the two entities annotated with &lt;entity_1&gt; and &lt;entity_2&gt;:\n    \"{{roi_text}}\"\n</code></pre> <p>As an example, we define the <code>possible_relation_func</code> function:   - if the two frames are &gt; 500 characters apart, we assume no relation (False)   - if the two frames are \"Medication\" and \"Strength\", or \"Medication\" and \"Frequency\", there could be relations (True)</p> <pre><code>def possible_relation_func(frame_1, frame_2) -&gt; bool:\n    \"\"\"\n    This function pre-process two frames and outputs a bool indicating whether the two frames could be related.\n    \"\"\"\n    # if the distance between the two frames are &gt; 500 characters, assume no relation.\n    if abs(frame_1.start - frame_2.start) &gt; 500:\n        return False\n\n    # if the entity types are \"Medication\" and \"Strength\", there could be relations.\n    if (frame_1.attr[\"entity_type\"] == \"Medication\" and frame_2.attr[\"entity_type\"] == \"Strength\") or \\\n        (frame_2.attr[\"entity_type\"] == \"Medication\" and frame_1.attr[\"entity_type\"] == \"Strength\"):\n        return True\n\n    # if the entity types are \"Medication\" and \"Frequency\", there could be relations.\n    if (frame_1.attr[\"entity_type\"] == \"Medication\" and frame_2.attr[\"entity_type\"] == \"Frequency\") or \\\n        (frame_2.attr[\"entity_type\"] == \"Medication\" and frame_1.attr[\"entity_type\"] == \"Frequency\"):\n        return True\n\n    # Otherwise, no relation.\n    return False\n</code></pre> <p>In the <code>BinaryRelationExtractor</code> constructor, we pass in the prompt template and <code>possible_relation_func</code>.</p> <pre><code>from llm_ie.extractors import BinaryRelationExtractor\n\nextractor = BinaryRelationExtractor(inference_engine, prompt_template=prompt_template, possible_relation_func=possible_relation_func)\n# Extract binary relations with concurrent mode (faster)\nrelations = extractor.extract_relations(doc, concurrent=True)\n\n# To print out the step-by-step, use the `concurrent=False` and `stream=True` options\nrelations = extractor.extract_relations(doc, concurrent=False, stream=True)\n</code></pre>"},{"location":"extractors/#multiclassrelationextractor","title":"MultiClassRelationExtractor","text":"<p>The main difference from <code>BinaryRelationExtractor</code> is that the <code>MultiClassRelationExtractor</code> allows specifying relation types. The prompt template guideline has an additional placeholder for possible relation types <code>{{pos_rel_types}}</code>. </p> <pre><code>print(MultiClassRelationExtractor.get_prompt_guide())\n</code></pre> <pre><code>Prompt Template Design:\n\n1. Task description:\n   Provide a detailed description of the task, including the background and the type of task (e.g., binary relation extraction). Mention the region of interest (ROI) text. \n2. Schema definition: \n   List the criterion for relation (True) and for no relation (False).\n\n3. Output format definition:\n   This section must include a placeholder \"{{pos_rel_types}}\" for the possible relation types.\n   The ouptut must be a dictionary with a key \"RelationType\" (i.e., {\"RelationType\": \"&lt;relation type or No Relation&gt;\"}).\n\n4. (optional) Hints:\n   Provide itemized hints for the information extractors to guide the extraction process.\n\n5. (optional) Examples:\n   Include examples in the format:  \n    Input: ...  \n    Output: ...\n\n6. Entity 1 full information:\n   Include a placeholder in the format {{&lt;frame_1&gt;}}\n\n7. Entity 2 full information:\n   Include a placeholder in the format {{&lt;frame_2&gt;}}\n\n8. Input placeholders:\n   The template must include a placeholder \"{{roi_text}}\" for the ROI text.\n\n\n\nExample:\n\n    # Task description\n    This is a multi-class relation extraction task. Given a region of interest (ROI) text and two frames from a medical note, classify the relation types between the two frames. \n\n    # Schema definition\n        Strength-Drug: this is a relationship between the drug strength and its name. \n        Dosage-Drug: this is a relationship between the drug dosage and its name.\n        Duration-Drug: this is a relationship between a drug duration and its name.\n        Frequency-Drug: this is a relationship between a drug frequency and its name.\n        Form-Drug: this is a relationship between a drug form and its name.\n        Route-Drug: this is a relationship between the route of administration for a drug and its name.\n        Reason-Drug: this is a relationship between the reason for which a drug was administered (e.g., symptoms, diseases, etc.) and a drug name.\n        ADE-Drug: this is a relationship between an adverse drug event (ADE) and a drug name.\n\n    # Output format definition\n    Choose one of the relation types listed below or choose \"No Relation\":\n    {{pos_rel_types}}\n\n    Your output should follow the JSON format:\n    {\"RelationType\": \"&lt;relation type or No Relation&gt;\"}\n\n    I am only interested in the content between []. Do not explain your answer. \n\n    # Hints\n        1. Your input always contains one medication entity and 1) one strength entity or 2) one frequency entity.\n        2. Pay attention to the medication entity and see if the strength or frequency is for it.\n        3. If the strength or frequency is for another medication, output \"No Relation\". \n        4. If the strength or frequency is for the same medication but at a different location (span), output \"No Relation\".\n\n    # Entity 1 full information:\n    {{frame_1}}\n\n    # Entity 2 full information:\n    {{frame_2}}\n\n    # Input placeholders\n    ROI Text with the two entities annotated with &lt;entity_1&gt; and &lt;entity_2&gt;:\n    \"{{roi_text}}\"\n</code></pre> <p>As an example, we define the <code>possible_relation_types_func</code> :   - if the two frames are &gt; 500 characters apart, we assume \"No Relation\" (output [])   - if the two frames are \"Medication\" and \"Strength\", the only possible relation types are \"Strength-Drug\" or \"No Relation\"   - if the two frames are \"Medication\" and \"Frequency\", the only possible relation types are \"Frequency-Drug\" or \"No Relation\"</p> <pre><code>def possible_relation_types_func(frame_1, frame_2) -&gt; List[str]:\n    # If the two frames are &gt; 500 characters apart, we assume \"No Relation\"\n    if abs(frame_1.start - frame_2.start) &gt; 500:\n        return []\n\n    # If the two frames are \"Medication\" and \"Strength\", the only possible relation types are \"Strength-Drug\" or \"No Relation\"\n    if (frame_1.attr[\"entity_type\"] == \"Medication\" and frame_2.attr[\"entity_type\"] == \"Strength\") or \\\n        (frame_2.attr[\"entity_type\"] == \"Medication\" and frame_1.attr[\"entity_type\"] == \"Strength\"):\n        return ['Strength-Drug']\n\n    # If the two frames are \"Medication\" and \"Frequency\", the only possible relation types are \"Frequency-Drug\" or \"No Relation\"\n    if (frame_1.attr[\"entity_type\"] == \"Medication\" and frame_2.attr[\"entity_type\"] == \"Frequency\") or \\\n        (frame_2.attr[\"entity_type\"] == \"Medication\" and frame_1.attr[\"entity_type\"] == \"Frequency\"):\n        return ['Frequency-Drug']\n\n    return []\n</code></pre> <pre><code>from llm_ie.extractors import MultiClassRelationExtractor\n\nextractor = MultiClassRelationExtractor(inference_engine, prompt_template=re_prompt_template,\n                                        possible_relation_types_func=possible_relation_types_func)\n\n# Extract multi-class relations with concurrent mode (faster)\nrelations = extractor.extract_relations(doc, concurrent=True)\n\n# To print out the step-by-step, use the `concurrent=False` and `stream=True` options\nrelations = extractor.extract_relations(doc, concurrent=False, stream=True)\n</code></pre>"},{"location":"extractors/#concurrent-optimization_1","title":"Concurrent Optimization","text":"<p>For concurrent extraction (recommended), the <code>async/await</code> feature is used to speed up inferencing. Set <code>concurrent=True</code> in <code>RelationExtractor.extract_relations</code>. The <code>concurrent_batch_size</code> sets the batch size of frame pairs to be processed in cocurrent.</p>"},{"location":"llm_inference_engine/","title":"LLM Inference Engine","text":"<p>We provide an interface for different LLM inference engines to work in the information extraction workflow. The built-in engines are <code>LiteLLMInferenceEngine</code>, <code>OpenAIInferenceEngine</code>, <code>HuggingFaceHubInferenceEngine</code>, <code>OllamaInferenceEngine</code>, and <code>LlamaCppInferenceEngine</code>. For customization, see customize inference engine</p>"},{"location":"llm_inference_engine/#litellm","title":"LiteLLM","text":"<p>The LiteLLM is an adaptor project that unifies many proprietary and open-source LLM APIs. Popular inferncing servers, including OpenAI, Huggingface Hub, and Ollama are supported via its interface. For more details, refer to LiteLLM GitHub page. </p> <p>To use LiteLLM with LLM-IE, import the <code>LiteLLMInferenceEngine</code> and follow the required model naming. <pre><code>from llm_ie.engines import LiteLLMInferenceEngine\n\n# Huggingface serverless inferencing\nos.environ['HF_TOKEN']\ninference_engine = LiteLLMInferenceEngine(model=\"huggingface/meta-llama/Meta-Llama-3-8B-Instruct\")\n\n# OpenAI GPT models\nos.environ['OPENAI_API_KEY']\ninference_engine = LiteLLMInferenceEngine(model=\"openai/gpt-4o-mini\")\n\n# OpenAI compatible local server\ninference_engine = LiteLLMInferenceEngine(model=\"openai/Llama-3.1-8B-Instruct\", base_url=\"http://localhost:8000/v1\", api_key=\"EMPTY\")\n\n# Ollama \ninference_engine = LiteLLMInferenceEngine(model=\"ollama/llama3.1:8b-instruct-q8_0\")\n</code></pre></p>"},{"location":"llm_inference_engine/#openai-api-compatible-services","title":"OpenAI API &amp; Compatible Services","text":"<p>In bash, save API key to the environmental variable <code>OPENAI_API_KEY</code>. <pre><code>export OPENAI_API_KEY=&lt;your_API_key&gt;\n</code></pre></p> <p>In Python, create inference engine and specify model name. For the available models, refer to OpenAI webpage.  For more parameters, see OpenAI API reference.</p> <pre><code>from llm_ie.engines import OpenAIInferenceEngine\n\ninference_engine = OpenAIInferenceEngine(model=\"gpt-4o-mini\")\n</code></pre> <p>For reasoning models (\"o\" series), use the <code>reasoning_model=True</code> flag. The <code>max_completion_tokens</code> will be used instead of the <code>max_tokens</code>. <code>temperature</code> will be ignored.</p> <pre><code>from llm_ie.engines import OpenAIInferenceEngine\n\ninference_engine = OpenAIInferenceEngine(model=\"o1-mini\", reasoning_model=True)\n</code></pre> <p>For OpenAI compatible services (OpenRouter for example): <pre><code>from llm_ie.engines import OpenAIInferenceEngine\n\ninference_engine = OpenAIInferenceEngine(base_url=\"https://openrouter.ai/api/v1\", model=\"qwen/qwen3-30b-a3b\")\n</code></pre></p>"},{"location":"llm_inference_engine/#azure-openai-api","title":"Azure OpenAI API","text":"<p>In bash, save the endpoint name and API key to environmental variables <code>AZURE_OPENAI_ENDPOINT</code> and <code>AZURE_OPENAI_API_KEY</code>. <pre><code>export AZURE_OPENAI_API_KEY=\"&lt;your_API_key&gt;\"\nexport AZURE_OPENAI_ENDPOINT=\"&lt;your_endpoint&gt;\"\n</code></pre></p> <p>In Python, create inference engine and specify model name. For the available models, refer to OpenAI webpage.  For more parameters, see Azure OpenAI reference.</p> <pre><code>from llm_ie.engines import AzureOpenAIInferenceEngine\n\ninference_engine = AzureOpenAIInferenceEngine(model=\"gpt-4o-mini\")\n</code></pre> <p>For reasoning models (\"o\" series), use the <code>reasoning_model=True</code> flag. The <code>max_completion_tokens</code> will be used instead of the <code>max_tokens</code>. <code>temperature</code> will be ignored. </p> <pre><code>from llm_ie.engines import AzureOpenAIInferenceEngine\n\ninference_engine = AzureOpenAIInferenceEngine(model=\"o1-mini\", reasoning_model=True)\n</code></pre>"},{"location":"llm_inference_engine/#huggingface_hub","title":"Huggingface_hub","text":"<p>The <code>model</code> can be a model id hosted on the Hugging Face Hub or a URL to a deployed Inference Endpoint. Refer to the Inference Client documentation for more details. </p> <pre><code>from llm_ie.engines import HuggingFaceHubInferenceEngine\n\ninference_engine = HuggingFaceHubInferenceEngine(model=\"meta-llama/Meta-Llama-3-8B-Instruct\")\n</code></pre>"},{"location":"llm_inference_engine/#ollama","title":"Ollama","text":"<p>The <code>model_name</code> must match the names on the Ollama library. Use the command line <code>ollama ls</code> to check your local model list. <code>num_ctx</code> determines the context length LLM will consider during text generation. Empirically, longer context length gives better performance, while consuming more memory and increases computation. <code>keep_alive</code> regulates the lifespan of LLM. It indicates a number of seconds to hold the LLM after the last API call. Default is 5 minutes (300 sec).</p> <pre><code>from llm_ie.engines import OllamaInferenceEngine\n\ninference_engine = OllamaInferenceEngine(model_name=\"llama3.1:8b-instruct-q8_0\", num_ctx=4096, keep_alive=300)\n</code></pre>"},{"location":"llm_inference_engine/#vllm","title":"vLLM","text":"<p>The vLLM support follows the OpenAI Compatible Server. For more parameters, please refer to the documentation.</p> <p>Start the server <pre><code>CUDA_VISIBLE_DEVICES=&lt;GPU#&gt; vllm serve meta-llama/Meta-Llama-3.1-8B-Instruct --api-key MY_API_KEY --tensor-parallel-size &lt;# of GPUs to use&gt;\n</code></pre> Use <code>CUDA_VISIBLE_DEVICES</code> to specify GPUs to use. The <code>--tensor-parallel-size</code> should be set accordingly. The <code>--api-key</code> is optional.  the default port is 8000. <code>--port</code> sets the port. </p> <p>Define inference engine <pre><code>from llm_ie.engines import OpenAIInferenceEngine\ninference_engine = OpenAIInferenceEngine(base_url=\"http://localhost:8000/v1\",\n                               api_key=\"MY_API_KEY\",\n                               model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\")\n</code></pre> The <code>model</code> must match the repo name specified in the server.</p>"},{"location":"llm_inference_engine/#llama-cpp-python","title":"Llama-cpp-python","text":"<p>The <code>repo_id</code> and <code>gguf_filename</code> must match the ones on the Huggingface repo to ensure the correct model is loaded. <code>n_ctx</code> determines the context length LLM will consider during text generation. Empirically, longer context length gives better performance, while consuming more memory and increases computation. Note that when <code>n_ctx</code> is less than the prompt length, Llama.cpp throws exceptions. <code>n_gpu_layers</code> indicates a number of model layers to offload to GPU. Default is -1 for all layers (entire LLM). Flash attention <code>flash_attn</code> is supported by Llama.cpp. The <code>verbose</code> indicates whether model information should be displayed. For more input parameters, see \ud83e\udd99 Llama-cpp-python. </p> <pre><code>from llm_ie.engines import LlamaCppInferenceEngine\n\ninference_engine = LlamaCppInferenceEngine(repo_id=\"bullerwins/Meta-Llama-3.1-8B-Instruct-GGUF\",\n                                           gguf_filename=\"Meta-Llama-3.1-8B-Instruct-Q8_0.gguf\",\n                                           n_ctx=4096,\n                                           n_gpu_layers=-1,\n                                           flash_attn=True,\n                                           verbose=False)\n</code></pre>"},{"location":"llm_inference_engine/#test-inference-engine-configuration","title":"Test inference engine configuration","text":"<p>To test the inference engine, use the <code>chat()</code> method. </p> <p><pre><code>from llm_ie.engines import OllamaInferenceEngine\n\ninference_engine = OllamaInferenceEngine(model_name=\"llama3.1:8b-instruct-q8_0\")\ninference_engine.chat(messages=[{\"role\": \"user\", \"content\":\"Hi\"}], verbose=True)\n</code></pre> The output should be something like (might vary by LLMs and versions)</p> <pre><code>'How can I help you today?'\n</code></pre>"},{"location":"llm_inference_engine/#customize-inference-engine","title":"Customize inference engine","text":"<p>The abstract class <code>InferenceEngine</code> defines the interface and required method <code>chat()</code>. Inherit this class for customized API.  <pre><code>class InferenceEngine:\n    @abc.abstractmethod\n    def __init__(self):\n        \"\"\"\n        This is an abstract class to provide interfaces for LLM inference engines. \n        Children classes that inherits this class can be used in extractors. Must implement chat() method.\n        \"\"\"\n        return NotImplemented\n\n    @abc.abstractmethod\n    def chat(self, messages:List[Dict[str,str]], max_new_tokens:int=2048, temperature:float=0.0, verbose:bool=False, **kwrs) -&gt; str:\n        \"\"\"\n        This method inputs chat messages and outputs LLM generated text.\n\n        Parameters:\n        ----------\n        messages : List[Dict[str,str]]\n            a list of dict with role and content. role must be one of {\"system\", \"user\", \"assistant\"}\n        max_new_tokens : str, Optional\n            the max number of new tokens LLM can generate. \n        temperature : float, Optional\n            the temperature for token sampling. \n        verbose : bool, Optional\n            if True, LLM generated text will be printed in terminal in real-time. \n        \"\"\"\n        return NotImplemented\n</code></pre></p>"},{"location":"prompt_editor/","title":"Prompt Editor","text":"<p>The prompt editor is an LLM agent that help users write prompt templates following the defined schema and guideline of each extractor. It is recommanded to initiate prompt editors with larger/smarter LLMs for better quality!. </p>"},{"location":"prompt_editor/#access-prompt-editor-by-chat-interface","title":"Access Prompt Editor by chat interface","text":"<p>The easiest way to </p> <pre><code>from llm_ie import OllamaInferenceEngine, PromptEditor, DirectFrameExtractor\n\n# Define an LLM inference engine\ninference_engine = OllamaInferenceEngine(model_name=\"llama4:scout\")\n\n# Define editor\neditor = PromptEditor(inference_engine, DirectFrameExtractor)\n\neditor.chat()\n</code></pre> <p>In a terminal environment, an interactive chat session will start: </p> <p>In the Jupyter/IPython environment, an ipywidgets session will start: </p>"},{"location":"prompt_editor/#prompt-editor-can-make-mistakes-always-check-the-output","title":"\u26a0\ufe0fPrompt Editor can make mistakes. Always check the output.","text":"<p>The Prompt Editor agent does not have access to documents that we are about to process unless included in the chat. Also, It does not have information about our purpose and backgournd of the request/project. Therefore, it is possible for Prompt Editor agent to propose suboptimal schema, provide incorrect examples, or generate irrelevant hints. It is very important that we check its output. It is a good practice to point out errors and ask Prompt Editor to revise it. Manual amendment is also recommended for minor issues. </p>"},{"location":"prompt_editor/#access-prompt-editor-by-python-functions","title":"Access Prompt Editor by Python functions","text":"<p>We can also use the <code>rewrite()</code> and <code>comment()</code> methods to programmingly interact with the prompt editor: </p> <ol> <li>start with a short description of the task</li> <li>have the prompt editor generate a prompt template as a starting point</li> <li>manually revise the prompt template</li> <li>have the prompt editor to comment/ rewrite it</li> </ol> <pre><code>from llm_ie import OllamaInferenceEngine, PromptEditor, DirectFrameExtractor\n\n# Define an LLM inference engine\ninference_engine = OllamaInferenceEngine(model_name=\"llama4:scout\")\n\n# Define editor\neditor = PromptEditor(inference_engine, DirectFrameExtractor)\n\n# Have editor to generate initial prompt template\ninitial_version = editor.rewrite(\"Extract treatment events from the discharge summary.\")\nprint(initial_version)\n</code></pre>"},{"location":"prompt_templates/","title":"Prompt Templates","text":"<p>A prompt template is a string with one or many placeholders <code>{{&lt;placeholder_name&gt;}}</code>. When input to an extractor, the <code>text_content</code> will be inserted into the placeholders to construct a prompt. Below is a demo:</p> <pre><code>### Task description\nThe paragraph below contains a clinical note with diagnoses listed. Please carefully review it and extract the diagnoses, including the diagnosis date and status.\n\n### Schema definition\nYour output should contain: \n    \"entity_text\" which is the name of the diagnosis spelled as it appears in the text,\n    \"Date\" which is the date when the diagnosis was made,\n    \"Status\" which is the current status of the diagnosis (e.g. active, resolved, etc.)\n\n### Output format definition\nYour output should follow JSON format, for example:\n[\n    {\"entity_text\": \"&lt;Diagnosis text&gt;\", \"attr\": {\"Date\": \"&lt;date in YYYY-MM-DD format&gt;\", \"Status\": \"&lt;status&gt;\"}},\n    {\"entity_text\": \"&lt;Diagnosis text&gt;\", \"attr\": {\"Date\": \"&lt;date in YYYY-MM-DD format&gt;\", \"Status\": \"&lt;status&gt;\"}}\n]\n\n### Additional hints\n- Your output should be 100% based on the provided content. DO NOT output fake information.\n- If there is no specific date or status, just omit those keys.\n\n### Context\nThe text below is from the clinical note:\n\"{{input}}\"\n</code></pre>"},{"location":"prompt_templates/#placeholder","title":"Placeholder","text":"<p>When only one placeholder is defined in the prompt template, the <code>text_content</code> can be a string or a dictionary with one key (regardless of the key name). When multiple placeholders are defined in the prompt template, the <code>text_content</code> should be a dictionary with:</p> <p><pre><code>{\"&lt;placeholder 1&gt;\": \"&lt;some text&gt;\", \"&lt;placeholder 2&gt;\": \"&lt;some text&gt;\"...}\n</code></pre> This is commonly used for injecting external knowledge, for example,</p> <pre><code>    Below is a medical note. Your task is to extract diagnosis information. \n\n    # Backgound knowledge\n    {{knowledge}}\n    Your output should include: \n        \"Diagnosis\": extract diagnosis names, \n        \"Datetime\": date/ time of diagnosis, \n        \"Status\": status of present, history, or family history\n\n    Your output should follow a JSON format:\n    [\n        {\"Diagnosis\": &lt;exact words as in the document&gt;, \"Datetime\": &lt;diagnosis datetime&gt;, \"Status\": &lt;one of \"present\", \"history\"&gt;},\n        {\"Diagnosis\": &lt;exact words as in the document&gt;, \"Datetime\": &lt;diagnosis datetime&gt;, \"Status\": &lt;one of \"present\", \"history\"&gt;},\n        ...\n    ]\n\n    Below is the medical note:\n    \"{{note}}\"\n</code></pre>"},{"location":"prompt_templates/#prompt-writing-guide","title":"Prompt writing guide","text":"<p>The quality of the prompt template can significantly impact the performance of information extraction. Also, the schema defined in prompt templates is dependent on the choice of extractors. When designing a prompt template schema, it is important to consider which extractor will be used. </p> <p>The <code>Extractor</code> class provides documentation and examples for prompt template writing. This is used by the Pormpt Editor. </p> <pre><code>from llm_ie.extractors import DirectFrameExtractor\n\nprint(DirectFrameExtractor.get_prompt_guide())\n</code></pre>"},{"location":"quick_start/","title":"Quick Start","text":"<p>We use a synthesized medical note by ChatGPT to demo the information extraction process. Our task is to extract diagnosis names, spans, and corresponding attributes (i.e., diagnosis datetime, status).</p> Synthesized Clinical Note <pre><code>### Discharge Summary Note\n\n**Patient Name:** John Doe  \n**Medical Record Number:** 12345678  \n**Date of Birth:** January 15, 1975  \n**Date of Admission:** July 20, 2024  \n**Date of Discharge:** July 27, 2024  \n\n**Attending Physician:** Dr. Jane Smith, MD  \n**Consulting Physicians:** Dr. Emily Brown, MD (Cardiology), Dr. Michael Green, MD (Pulmonology)\n\n#### Reason for Admission\nJohn Doe, a 49-year-old male, was admitted to the hospital with complaints of chest pain, shortness of breath, and dizziness. The patient has a history of hypertension, hyperlipidemia, and Type 2 diabetes mellitus.\n\n#### History of Present Illness\nThe patient reported that the chest pain started two days prior to admission. The pain was described as a pressure-like sensation in the central chest, radiating to the left arm and jaw. He also experienced dyspnea on exertion and occasional palpitations. The patient denied any recent upper respiratory infection, cough, or fever.\n\n#### Past Medical History\n- Hypertension (diagnosed in 2010)\n- Hyperlipidemia (diagnosed in 2015)\n- Type 2 Diabetes Mellitus (diagnosed in 2018)\n\n#### Social History\n- Former smoker (quit in 2010)\n- Occasional alcohol consumption\n- Works as an accountant\n- Married with two children\n\n#### Family History\n- Father: myocardial infarction at age 55\n- Mother: Type 2 diabetes mellitus\n\n#### Physical Examination\n- **Vital Signs:** Blood pressure 160/95 mmHg, heart rate 88 bpm, respiratory rate 20 breaths/min, temperature 98.6\u00b0F, oxygen saturation 96% on room air.\n- **General:** Alert and oriented, in mild distress.\n- **Cardiovascular:** Regular rhythm, no murmurs, rubs, or gallops. Jugular venous pressure not elevated.\n- **Respiratory:** Clear to auscultation bilaterally, no wheezes, rales, or rhonchi.\n- **Abdomen:** Soft, non-tender, no hepatosplenomegaly.\n- **Extremities:** No edema, pulses 2+ bilaterally.\n\n#### Laboratory and Diagnostic Tests\n- **EKG:** ST-segment depression in leads V4-V6.\n- **Troponin I:** Elevated at 0.15 ng/mL (normal &lt;0.04 ng/mL).\n- **Chest X-ray:** No acute infiltrates, normal cardiac silhouette.\n- **Echocardiogram:** Mild left ventricular hypertrophy, ejection fraction 55%.\n- **CBC:** WBC 8.5 x 10^3/uL, Hgb 13.5 g/dL, Platelets 250 x 10^3/uL.\n- **CMP:** Na 138 mmol/L, K 4.0 mmol/L, BUN 15 mg/dL, Creatinine 0.9 mg/dL, Glucose 180 mg/dL, HbA1c 7.8%.\n\n#### Hospital Course\nJohn Doe was diagnosed with acute coronary syndrome (ACS). He was started on dual antiplatelet therapy with aspirin and clopidogrel, along with high-dose atorvastatin, and a beta-blocker. A cardiology consultation was obtained, and the patient underwent coronary angiography, which revealed a 70% stenosis in the left anterior descending artery. A drug-eluting stent was placed successfully.\n\nPost-procedure, the patient was monitored in the coronary care unit. He remained hemodynamically stable, with no recurrent chest pain. He was gradually advanced to a regular cardiac diet and was ambulating without difficulty by day three of hospitalization. Diabetes management was optimized with the addition of metformin, and his blood pressure was controlled with the continuation of his antihypertensive regimen.\n\n#### Discharge Medications\n- Aspirin 81 mg daily\n- Clopidogrel 75 mg daily\n- Atorvastatin 40 mg daily\n- Metoprolol 50 mg twice daily\n- Lisinopril 20 mg daily\n- Metformin 1000 mg twice daily\n\n#### Discharge Instructions\nJohn Doe was advised to follow a heart-healthy diet, engage in regular physical activity, and monitor his blood glucose levels. He was instructed to avoid smoking and limit alcohol intake. Follow-up appointments were scheduled with his primary care physician, cardiologist, and endocrinologist.\n\nThe patient was educated on the signs and symptoms of recurrent chest pain and instructed to seek immediate medical attention if they occur. He was provided with a prescription for a nitroglycerin tablet to use as needed for chest pain.\n\n#### Follow-Up Appointments\n- Primary Care Physician: August 3, 2024\n- Cardiology: August 10, 2024\n- Endocrinology: August 17, 2024\n\n**Discharge Summary Prepared by:**  \nDr. Jane Smith, MD  \nJuly 27, 2024\n</code></pre>"},{"location":"quick_start/#choose-an-llm-inference-engine","title":"Choose an LLM inference engine","text":"<p>LLM-IE works with both local and remote LLM deployments. In this quick start demo, we use OpenRouter to run Llama-4-Scout. The outputs might be slightly different with other inference engines, LLMs, or quantization. To use other inference engines and models, see LLM Inference Engine</p>"},{"location":"quick_start/#prompt-engineering-by-chatting-with-llm-agent","title":"Prompt engineering by chatting with LLM agent","text":"<p>We start with defining prompt editor LLM agent. We store OpenRouter API key in environmental variable <code>OPENROUTER_API_KEY</code>. <pre><code>export OPENROUTER_API_KEY=&lt;OpenRouter API key&gt;\n</code></pre></p> <p><pre><code>from llm_ie import OpenAIInferenceEngine, DirectFrameExtractor, PromptEditor, SentenceUnitChunker, SlideWindowContextChunker\n\n# Define a LLM inference engine\nllm = OpenAIInferenceEngine(base_url=\"https://openrouter.ai/api/v1\", model=\"meta-llama/llama-4-scout\", api_key=os.getenv(\"OPENROUTER_API_KEY\"))\n# Define LLM prompt editor\neditor = PromptEditor(inference_engine, DirectFrameExtractor)\n# Start chat\neditor.chat()\n</code></pre> This opens an interactive session where we can chat with the Prompt Editor agent: </p> <p>The agent drafts a prompt template following the schema required by the <code>DirectFrameExtractor</code>. After a few rounds of chatting, we have a prompt template to start with: <pre><code>### Task description\nThe paragraph below contains a clinical note with diagnoses listed. Please carefully review it and extract the diagnoses, including the diagnosis date and status.\n\n### Schema definition\nYour output should contain: \n    \"entity_text\" which is the diagnosis spelled as it appears in the text,\n    \"Date\" which is the date when the diagnosis was made,\n    \"Status\" which is the current status of the diagnosis (e.g. active, resolved, etc.)\n\n### Output format definition\nYour output should follow JSON format, for example:\n[\n    {\"entity_text\": \"&lt;Diagnosis&gt;\", \"attr\": {\"Date\": \"&lt;date in YYYY-MM-DD format&gt;\", \"Status\": \"&lt;status&gt;\"}},\n    {\"entity_text\": \"&lt;Diagnosis&gt;\", \"attr\": {\"Date\": \"&lt;date in YYYY-MM-DD format&gt;\", \"Status\": \"&lt;status&gt;\"}}\n]\n\n### Additional hints\n- Your output should be 100% based on the provided content. DO NOT output fake information.\n- If there is no specific date or status, just omit those keys.\n\n### Context\nThe text below is from the clinical note:\n\"{{input}}\"\n</code></pre></p>"},{"location":"quick_start/#design-prompting-algorithm-for-information-extraction","title":"Design prompting algorithm for information extraction","text":"<p>Instead of prompting LLMs with the entire document (which, by our experiments, has worse performance), we divide the input document into units (e.g., sentences, text lines, paragraphs). LLM only focus on one unit at a time, before moving to the next unit. This is achieved by the <code>UnitChunker</code> classes. In this demo, we use <code>SentenceUnitChunker</code> for sentence-by-sentence prompting. LLM only focus on one sentence at a time. We supply a context, in this case, a slide window of 2 sentences as context. This provides LLM with additional information. This is achieved by the <code>SlideWindowContextChunker</code> class. To learn more about prompting algorithm, see Extractors.</p> <pre><code># Load synthesized medical note\nwith open(\"./demo/document/synthesized_note.txt\", 'r') as f:\n    note_text = f.read()\n\n# Define unit chunker. Prompts sentences-by-sentence.\nunit_chunker = SentenceUnitChunker()\n# Define context chunker. Provides context for units.\ncontext_chunker = SlideWindowContextChunker(window_size=2)\n# Define extractor\nextractor = DirectFrameExtractor(inference_engine=llm, \n                                 unit_chunker=unit_chunker,\n                                 context_chunker=context_chunker,\n                                 prompt_template=prompt_template)\n</code></pre> <p>To run the frame extraction, use <code>extract_frames</code> method. A list of entities with attributes (\"frames\") will be returned. Concurrent processing is supported by setting <code>concurrent=True</code>. <pre><code># To stream the extraction process, use concurrent=False, stream=True:\nframes =  extractor.extract_frames(note_text, concurrent=False, verbose=True)\n# For faster extraction, use concurrent=True to enable asynchronous prompting\n# frames =  extractor.extract_frames(note_text, concurrent=True)\n\n# Check extractions\nfor frame in frames:\n    print(frame.to_dict())\n</code></pre> The output is a list of frames. Each frame has a <code>entity_text</code>, <code>start</code>, <code>end</code>, and a dictionary of <code>attr</code>. </p> <pre><code>{'frame_id': '0', 'start': 537, 'end': 549, 'entity_text': 'hypertension', 'attr': {'Status': ''}}\n{'frame_id': '1', 'start': 551, 'end': 565, 'entity_text': 'hyperlipidemia', 'attr': {'Status': ''}}\n{'frame_id': '2', 'start': 571, 'end': 595, 'entity_text': 'Type 2 diabetes mellitus', 'attr': {'Status': ''}}\n{'frame_id': '3', 'start': 991, 'end': 1003, 'entity_text': 'Hypertension', 'attr': {'Date': '2010', 'Status': None}}\n{'frame_id': '4', 'start': 1026, 'end': 1040, 'entity_text': 'Hyperlipidemia', 'attr': {'Date': '2015', 'Status': None}}\n{'frame_id': '5', 'start': 1063, 'end': 1087, 'entity_text': 'Type 2 Diabetes Mellitus', 'attr': {'Date': '2018', 'Status': None}}\n{'frame_id': '6', 'start': 1646, 'end': 1682, 'entity_text': 'Jugular venous pressure not elevated', 'attr': {}}\n{'frame_id': '7', 'start': 1703, 'end': 1767, 'entity_text': 'Clear to auscultation bilaterally, no wheezes, rales, or rhonchi', 'attr': {}}\n{'frame_id': '8', 'start': 1802, 'end': 1823, 'entity_text': 'no hepatosplenomegaly', 'attr': {}}\n{'frame_id': '9', 'start': 1926, 'end': 1962, 'entity_text': 'ST-segment depression in leads V4-V6', 'attr': {}}\n{'frame_id': '10', 'start': 1982, 'end': 2004, 'entity_text': 'Elevated at 0.15 ng/mL', 'attr': {'Date': '', 'Status': ''}}\n{'frame_id': '11', 'start': 2046, 'end': 2066, 'entity_text': 'No acute infiltrates', 'attr': {}}\n{'frame_id': '12', 'start': 2068, 'end': 2093, 'entity_text': 'normal cardiac silhouette', 'attr': {}}\n{'frame_id': '13', 'start': 2117, 'end': 2150, 'entity_text': 'Mild left ventricular hypertrophy', 'attr': {'Date': '', 'Status': ''}}\n{'frame_id': '14', 'start': 2321, 'end': 2338, 'entity_text': 'Glucose 180 mg/dL', 'attr': {}}\n{'frame_id': '15', 'start': 2340, 'end': 2350, 'entity_text': 'HbA1c 7.8%', 'attr': {}}\n{'frame_id': '16', 'start': 2402, 'end': 2431, 'entity_text': 'acute coronary syndrome (ACS)', 'attr': {'Date': None, 'Status': None}}\n{'frame_id': '17', 'start': 3025, 'end': 3033, 'entity_text': 'Diabetes', 'attr': {}}\n{'frame_id': '18', 'start': 3925, 'end': 3935, 'entity_text': 'chest pain', 'attr': {'Date': '', 'Status': ''}}\n</code></pre>"},{"location":"quick_start/#data-management-and-visualization","title":"Data management and visualization","text":"<p>We can save the frames to a document object for better management. The document holds <code>text</code> and <code>frames</code>. The <code>add_frame()</code> method performs validation and (if passed) adds a frame to the document. The <code>valid_mode</code> controls how frame validation should be performed. For example, the <code>valid_mode = \"span\"</code> will prevent new frames from being added if the frame spans (<code>start</code>, <code>end</code>) has already exist. The <code>create_id = True</code> allows the document to assign unique frame IDs.  </p> <pre><code>from llm_ie.data_types import LLMInformationExtractionDocument\n\n# Define document\ndoc = LLMInformationExtractionDocument(doc_id=\"Synthesized medical note\",\n                                       text=note_text)\n# Add frames to a document\ndoc.add_frames(frames, create_id=True)\n\n# Save document to file (.llmie)\ndoc.save(\"&lt;your filename&gt;.llmie\")\n</code></pre> <p>To visualize the extracted frames, we use the <code>viz_serve()</code> method.  <pre><code>doc.viz_serve()\n</code></pre> A Flask App starts at port 5000 (default). To learn more about visualization, see Visualization.</p> <p></p>"},{"location":"visualization/","title":"Visualization","text":"<p>The <code>LLMInformationExtractionDocument</code> class supports named entity, entity attributes, and relation visualization. The implementation is through our plug-in package ie-viz. </p> <pre><code>pip install ie-viz\n</code></pre> <p>The <code>viz_serve()</code> method starts a Flask App on localhost port 5000 by default.  <pre><code>from llm_ie.data_types import LLMInformationExtractionDocument\n\n# Define document\ndoc = LLMInformationExtractionDocument(doc_id=\"Medical note\",\n                                       text=note_text)\n# Add extracted frames and relations to document\ndoc.add_frames(frames)\ndoc.add_relations(relations)\n# Visualize the document\ndoc.viz_serve()\n</code></pre></p> <p></p> <p>Alternatively, the <code>viz_render()</code> method returns a self-contained (HTML + JS + CSS) string. Save it to file and open with a browser. <pre><code>html = doc.viz_render()\n\nwith open(\"Medical note.html\", \"w\") as f:\n    f.write(html)\n</code></pre></p> <p>To customize colors for different entities, use <code>color_attr_key</code> (simple) or <code>color_map_func</code> (advanced). </p> <p>The <code>color_attr_key</code> automatically assign colors based on the specified attribute key. For example, \"EntityType\". <pre><code>doc.viz_serve(color_attr_key=\"EntityType\")\n</code></pre></p> <p>The <code>color_map_func</code> allow users to define a custom entity-color mapping function. For example, <pre><code>def color_map_func(entity) -&gt; str:\n    if entity['attr']['&lt;attribute key&gt;'] == \"&lt;a certain value&gt;\":\n        return \"#7f7f7f\"\n    else:\n        return \"#03A9F4\"\n\ndoc.viz_serve(color_map_func=color_map_func)\n</code></pre></p>"},{"location":"web_application/","title":"Web Application","text":"<p>We provide a drag-and-drop web Application for no-code access to the LLM-IE. The web Application streamlines the workflow:</p> <ol> <li>Prompt engineering with LLM agent: Prompt Editor Tab</li> <li>Prompting algorithm design: Frame extraction Tab</li> <li>Visualization &amp; Validation: Result viewer Tab</li> <li>Repeat step #1-#3 until achieves high accuracy</li> </ol>"},{"location":"web_application/#installation","title":"Installation","text":"<p>The image is available on Docker Hub. Use the command below to pull and run locally: <pre><code>docker pull daviden1013/llm-ie-web-app:latest\ndocker run -p 5000:5000 daviden1013/llm-ie-web-app:latest\n</code></pre></p>"},{"location":"web_application/#prompt-editor-tab","title":"Prompt Editor Tab","text":""},{"location":"web_application/#select-an-inference-api","title":"Select an inference API","text":"<p>Select from the dropdown (e.g., OpenAI, Azure, Ollama, Huggingface Hub) and specify an LLM. It is recomanded to select larger/smarter LLMs for better performance. Supply the API key, base URL, and deployment (for Azure) if needed. </p>"},{"location":"web_application/#start-chatting","title":"Start chatting","text":"<p>Describe your task and the Prompt Editor LLM agent behind the scene will help construct a structured prompt template. Feel free to ask the Prompt Editor questions, request it to revise, or ask it to add examples. </p>"},{"location":"web_application/#frame-extraction-tab","title":"Frame Extraction Tab","text":"<ul> <li> <p>Unit: a text snippet that LLM extrator will process at a time. It could be a sentence, a line of text, or a paragraph. </p> </li> <li>Context: the context around the unit. For exapmle, a slidewindow of 2 sentences before and after. Context is optional. </li> </ul> <p>See more details in Extractors</p> <ul> <li> </li> <li> </li> </ul> <p></p>"},{"location":"web_application/#select-an-inference-api_1","title":"Select an inference API","text":"<p>Select an inferencing API and specify an LLM. This model will perform the frame extraction task, so keep the balance of cost and performance in mind. If you have a lot of document to process, the API calls/cost can be high! </p>"},{"location":"web_application/#paste-your-document-and-prompt-temlpate","title":"Paste your document and prompt temlpate","text":"<p>In the \"Input Text\" textbox, paste the document text that you want to process. In the \"Prompt Template\" textbox, paste the prompt template you obtained from the previous step Prompt Editor Tab.</p>"},{"location":"web_application/#select-a-chunking-method","title":"Select a chunking method","text":"<p>Frame extraction adopts an unit-context schema. The purpose is to avoid having LLM to process long context and suffer from needle in the haystack challenge. We split an input document into multiple units. LLM only process a unit of text at a time. </p>"},{"location":"web_application/#watch-the-extraction-process","title":"Watch the extraction process","text":"<p>Click on the \"Start Extraction\" button and watch LLM processes unit-by-unit on the right panel. It is recommanded to monitor the process and look for errors.</p>"},{"location":"web_application/#download-results","title":"Download results","text":"<p>Use the download button on the top-right to download results. The resulting JSON will be saved as a <code>.llmie</code> file to your download folder. </p>"},{"location":"web_application/#result-viewer-tab","title":"Result Viewer Tab","text":""},{"location":"web_application/#upload-the-result-llmie-file","title":"Upload the result <code>.llmie</code> file.","text":"<p>Drop the result file from the previous step Frame extraction Tab to upload. </p>"},{"location":"web_application/#select-color-key-optional","title":"Select color key (Optional)","text":"<p>Optionally, select the attribute key in the dropdown for color coding.</p>"},{"location":"api/chunkers/","title":"Chunkers API","text":"<p>This module provides classes for splitting documents into manageable units for processing by LLMs and for providing context to those units.</p>"},{"location":"api/chunkers/#unit-chunkers","title":"Unit Chunkers","text":"<p>Unit chunkers determine how a document is divided into smaller pieces for frame extraction. Each piece is a <code>FrameExtractionUnit</code>.</p>"},{"location":"api/chunkers/#llm_ie.chunkers.UnitChunker","title":"llm_ie.chunkers.UnitChunker","text":"<pre><code>UnitChunker()\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>This is the abstract class for frame extraction unit chunker. It chunks a document into units (e.g., sentences). LLMs process unit by unit.</p> Source code in <code>package/llm-ie/src/llm_ie/chunkers.py</code> <pre><code>def __init__(self):\n    \"\"\"\n    This is the abstract class for frame extraction unit chunker.\n    It chunks a document into units (e.g., sentences). LLMs process unit by unit. \n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/chunkers/#llm_ie.chunkers.UnitChunker.chunk","title":"chunk","text":"<pre><code>chunk(text: str) -&gt; List[FrameExtractionUnit]\n</code></pre> Parameters: <p>text : str     The document text.</p> Source code in <code>package/llm-ie/src/llm_ie/chunkers.py</code> <pre><code>def chunk(self, text:str) -&gt; List[FrameExtractionUnit]:\n    \"\"\"\n    Parameters:\n    ----------\n    text : str\n        The document text.\n    \"\"\"\n    return NotImplemented\n</code></pre>"},{"location":"api/chunkers/#llm_ie.chunkers.WholeDocumentUnitChunker","title":"llm_ie.chunkers.WholeDocumentUnitChunker","text":"<pre><code>WholeDocumentUnitChunker()\n</code></pre> <p>               Bases: <code>UnitChunker</code></p> <p>This class chunks the whole document into a single unit (no chunking).</p> Source code in <code>package/llm-ie/src/llm_ie/chunkers.py</code> <pre><code>def __init__(self):\n    \"\"\"\n    This class chunks the whole document into a single unit (no chunking).\n    \"\"\"\n    super().__init__()\n</code></pre>"},{"location":"api/chunkers/#llm_ie.chunkers.WholeDocumentUnitChunker.chunk","title":"chunk","text":"<pre><code>chunk(text: str) -&gt; List[FrameExtractionUnit]\n</code></pre> Parameters: <p>text : str     The document text.</p> Source code in <code>package/llm-ie/src/llm_ie/chunkers.py</code> <pre><code>def chunk(self, text:str) -&gt; List[FrameExtractionUnit]:\n    \"\"\"\n    Parameters:\n    ----------\n    text : str\n        The document text.\n    \"\"\"\n    return [FrameExtractionUnit(\n        start=0,\n        end=len(text),\n        text=text\n    )]\n</code></pre>"},{"location":"api/chunkers/#llm_ie.chunkers.SentenceUnitChunker","title":"llm_ie.chunkers.SentenceUnitChunker","text":"<pre><code>SentenceUnitChunker()\n</code></pre> <p>               Bases: <code>UnitChunker</code></p> <p>This class uses the NLTK PunktSentenceTokenizer to chunk a document into sentences.</p> Source code in <code>package/llm-ie/src/llm_ie/chunkers.py</code> <pre><code>def __init__(self):\n    \"\"\"\n    This class uses the NLTK PunktSentenceTokenizer to chunk a document into sentences.\n    \"\"\"\n    super().__init__()\n</code></pre>"},{"location":"api/chunkers/#llm_ie.chunkers.SentenceUnitChunker.chunk","title":"chunk","text":"<pre><code>chunk(text: str) -&gt; List[FrameExtractionUnit]\n</code></pre> Parameters: <p>text : str     The document text.</p> Source code in <code>package/llm-ie/src/llm_ie/chunkers.py</code> <pre><code>def chunk(self, text:str) -&gt; List[FrameExtractionUnit]:\n    \"\"\"\n    Parameters:\n    ----------\n    text : str\n        The document text.\n    \"\"\"\n    sentences = []\n    for start, end in self.PunktSentenceTokenizer().span_tokenize(text):\n        sentences.append(FrameExtractionUnit(\n            start=start,\n            end=end,\n            text=text[start:end]\n        ))    \n    return sentences\n</code></pre>"},{"location":"api/chunkers/#llm_ie.chunkers.TextLineUnitChunker","title":"llm_ie.chunkers.TextLineUnitChunker","text":"<pre><code>TextLineUnitChunker()\n</code></pre> <p>               Bases: <code>UnitChunker</code></p> <p>This class chunks a document into lines.</p> Source code in <code>package/llm-ie/src/llm_ie/chunkers.py</code> <pre><code>def __init__(self):\n    \"\"\"\n    This class chunks a document into lines.\n    \"\"\"\n    super().__init__()\n</code></pre>"},{"location":"api/chunkers/#llm_ie.chunkers.TextLineUnitChunker.chunk","title":"chunk","text":"<pre><code>chunk(text: str) -&gt; List[FrameExtractionUnit]\n</code></pre> Parameters: <p>text : str     The document text.</p> Source code in <code>package/llm-ie/src/llm_ie/chunkers.py</code> <pre><code>def chunk(self, text:str) -&gt; List[FrameExtractionUnit]:\n    \"\"\"\n    Parameters:\n    ----------\n    text : str\n        The document text.\n    \"\"\"\n    lines = text.split('\\n')\n    line_units = []\n    start = 0\n    for line in lines:\n        end = start + len(line)\n        line_units.append(FrameExtractionUnit(\n            start=start,\n            end=end,\n            text=line\n        ))\n        start = end + 1 \n    return line_units\n</code></pre>"},{"location":"api/chunkers/#context-chunkers","title":"Context Chunkers","text":"<p>Context chunkers determine what contextual information is provided to the LLM alongside a specific <code>FrameExtractionUnit</code>.</p>"},{"location":"api/chunkers/#llm_ie.chunkers.ContextChunker","title":"llm_ie.chunkers.ContextChunker","text":"<pre><code>ContextChunker()\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>This is the abstract class for context chunker. Given a frame extraction unit, it returns the context for it.</p> Source code in <code>package/llm-ie/src/llm_ie/chunkers.py</code> <pre><code>def __init__(self):\n    \"\"\"\n    This is the abstract class for context chunker. Given a frame extraction unit,\n    it returns the context for it.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/chunkers/#llm_ie.chunkers.ContextChunker.chunk","title":"chunk","text":"<pre><code>chunk(unit: FrameExtractionUnit) -&gt; str\n</code></pre> Parameters: <p>unit : FrameExtractionUnit     The frame extraction unit.</p> <p>Return : str      The context for the frame extraction unit.</p> Source code in <code>package/llm-ie/src/llm_ie/chunkers.py</code> <pre><code>def chunk(self, unit:FrameExtractionUnit) -&gt; str:\n    \"\"\"\n    Parameters:\n    ----------\n    unit : FrameExtractionUnit\n        The frame extraction unit.\n\n    Return : str \n        The context for the frame extraction unit.\n    \"\"\"\n    return NotImplemented\n</code></pre>"},{"location":"api/chunkers/#llm_ie.chunkers.NoContextChunker","title":"llm_ie.chunkers.NoContextChunker","text":"<pre><code>NoContextChunker()\n</code></pre> <p>               Bases: <code>ContextChunker</code></p> <p>This class does not provide any context.</p> Source code in <code>package/llm-ie/src/llm_ie/chunkers.py</code> <pre><code>def __init__(self):\n    \"\"\"\n    This class does not provide any context.\n    \"\"\"\n    super().__init__()\n</code></pre>"},{"location":"api/chunkers/#llm_ie.chunkers.NoContextChunker.fit","title":"fit","text":"<pre><code>fit(text: str, units: List[FrameExtractionUnit])\n</code></pre> Parameters: <p>text : str     The document text.</p> Source code in <code>package/llm-ie/src/llm_ie/chunkers.py</code> <pre><code>def fit(self, text:str, units:List[FrameExtractionUnit]):\n    \"\"\"\n    Parameters:\n    ----------\n    text : str\n        The document text.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/chunkers/#llm_ie.chunkers.WholeDocumentContextChunker","title":"llm_ie.chunkers.WholeDocumentContextChunker","text":"<pre><code>WholeDocumentContextChunker()\n</code></pre> <p>               Bases: <code>ContextChunker</code></p> <p>This class provides the whole document as context.</p> Source code in <code>package/llm-ie/src/llm_ie/chunkers.py</code> <pre><code>def __init__(self):\n    \"\"\"\n    This class provides the whole document as context.\n    \"\"\"\n    super().__init__()\n    self.text = None\n</code></pre>"},{"location":"api/chunkers/#llm_ie.chunkers.WholeDocumentContextChunker.fit","title":"fit","text":"<pre><code>fit(text: str, units: List[FrameExtractionUnit])\n</code></pre> Parameters: <p>text : str     The document text.</p> Source code in <code>package/llm-ie/src/llm_ie/chunkers.py</code> <pre><code>def fit(self, text:str, units:List[FrameExtractionUnit]):\n    \"\"\"\n    Parameters:\n    ----------\n    text : str\n        The document text.\n    \"\"\"\n    self.text = text\n</code></pre>"},{"location":"api/chunkers/#llm_ie.chunkers.SlideWindowContextChunker","title":"llm_ie.chunkers.SlideWindowContextChunker","text":"<pre><code>SlideWindowContextChunker(window_size: int)\n</code></pre> <p>               Bases: <code>ContextChunker</code></p> <p>This class provides a sliding window context. For example, +-2 sentences around a unit sentence.</p> Source code in <code>package/llm-ie/src/llm_ie/chunkers.py</code> <pre><code>def __init__(self, window_size:int):\n    \"\"\"\n    This class provides a sliding window context. For example, +-2 sentences around a unit sentence. \n    \"\"\"\n    super().__init__()\n    self.window_size = window_size\n    self.units = None\n</code></pre>"},{"location":"api/chunkers/#llm_ie.chunkers.SlideWindowContextChunker.fit","title":"fit","text":"<pre><code>fit(text: str, units: List[FrameExtractionUnit])\n</code></pre> Parameters: <p>units : List[FrameExtractionUnit]     The list of frame extraction units.</p> Source code in <code>package/llm-ie/src/llm_ie/chunkers.py</code> <pre><code>def fit(self, text:str, units:List[FrameExtractionUnit]):\n    \"\"\"\n    Parameters:\n    ----------\n    units : List[FrameExtractionUnit]\n        The list of frame extraction units.\n    \"\"\"\n    self.units = sorted(units)\n</code></pre>"},{"location":"api/engines/","title":"Engines API","text":""},{"location":"api/engines/#llm_ie.engines.InferenceEngine","title":"llm_ie.engines.InferenceEngine","text":"<pre><code>InferenceEngine()\n</code></pre> <p>This is an abstract class to provide interfaces for LLM inference engines.  Children classes that inherts this class can be used in extrators. Must implement chat() method.</p> Source code in <code>package/llm-ie/src/llm_ie/engines.py</code> <pre><code>@abc.abstractmethod\ndef __init__(self):\n    \"\"\"\n    This is an abstract class to provide interfaces for LLM inference engines. \n    Children classes that inherts this class can be used in extrators. Must implement chat() method.\n    \"\"\"\n    return NotImplemented\n</code></pre>"},{"location":"api/engines/#llm_ie.engines.InferenceEngine.chat","title":"chat  <code>abstractmethod</code>","text":"<pre><code>chat(\n    messages: List[Dict[str, str]],\n    max_new_tokens: int = 2048,\n    temperature: float = 0.0,\n    verbose: bool = False,\n    stream: bool = False,\n    **kwrs\n) -&gt; Union[str, Generator[str, None, None]]\n</code></pre> <p>This method inputs chat messages and outputs LLM generated text.</p> Parameters: <p>messages : List[Dict[str,str]]     a list of dict with role and content. role must be one of {\"system\", \"user\", \"assistant\"} max_new_tokens : str, Optional     the max number of new tokens LLM can generate.  temperature : float, Optional     the temperature for token sampling.  verbose : bool, Optional     if True, LLM generated text will be printed in terminal in real-time. stream : bool, Optional     if True, returns a generator that yields the output in real-time.</p> Source code in <code>package/llm-ie/src/llm_ie/engines.py</code> <pre><code>@abc.abstractmethod\ndef chat(self, messages:List[Dict[str,str]], max_new_tokens:int=2048, temperature:float=0.0, \n         verbose:bool=False, stream:bool=False, **kwrs) -&gt; Union[str, Generator[str, None, None]]:\n    \"\"\"\n    This method inputs chat messages and outputs LLM generated text.\n\n    Parameters:\n    ----------\n    messages : List[Dict[str,str]]\n        a list of dict with role and content. role must be one of {\"system\", \"user\", \"assistant\"}\n    max_new_tokens : str, Optional\n        the max number of new tokens LLM can generate. \n    temperature : float, Optional\n        the temperature for token sampling. \n    verbose : bool, Optional\n        if True, LLM generated text will be printed in terminal in real-time.\n    stream : bool, Optional\n        if True, returns a generator that yields the output in real-time.  \n    \"\"\"\n    return NotImplemented\n</code></pre>"},{"location":"api/engines/#llm_ie.engines.OllamaInferenceEngine","title":"llm_ie.engines.OllamaInferenceEngine","text":"<pre><code>OllamaInferenceEngine(\n    model_name: str,\n    num_ctx: int = 4096,\n    keep_alive: int = 300,\n    **kwrs\n)\n</code></pre> <p>               Bases: <code>InferenceEngine</code></p> <p>The Ollama inference engine.</p> Parameters: <p>model_name : str     the model name exactly as shown in &gt;&gt; ollama ls num_ctx : int, Optional     context length that LLM will evaluate. keep_alive : int, Optional     seconds to hold the LLM after the last API call.</p> Source code in <code>package/llm-ie/src/llm_ie/engines.py</code> <pre><code>def __init__(self, model_name:str, num_ctx:int=4096, keep_alive:int=300, **kwrs):\n    \"\"\"\n    The Ollama inference engine.\n\n    Parameters:\n    ----------\n    model_name : str\n        the model name exactly as shown in &gt;&gt; ollama ls\n    num_ctx : int, Optional\n        context length that LLM will evaluate.\n    keep_alive : int, Optional\n        seconds to hold the LLM after the last API call.\n    \"\"\"\n    if importlib.util.find_spec(\"ollama\") is None:\n        raise ImportError(\"ollama-python not found. Please install ollama-python (```pip install ollama```).\")\n\n    from ollama import Client, AsyncClient\n    self.client = Client(**kwrs)\n    self.async_client = AsyncClient(**kwrs)\n    self.model_name = model_name\n    self.num_ctx = num_ctx\n    self.keep_alive = keep_alive\n</code></pre>"},{"location":"api/engines/#llm_ie.engines.OllamaInferenceEngine.chat","title":"chat","text":"<pre><code>chat(\n    messages: List[Dict[str, str]],\n    max_new_tokens: int = 2048,\n    temperature: float = 0.0,\n    verbose: bool = False,\n    stream: bool = False,\n    **kwrs\n) -&gt; Union[str, Generator[str, None, None]]\n</code></pre> <p>This method inputs chat messages and outputs VLM generated text.</p> Parameters: <p>messages : List[Dict[str,str]]     a list of dict with role and content. role must be one of {\"system\", \"user\", \"assistant\"} max_new_tokens : str, Optional     the max number of new tokens VLM can generate.  temperature : float, Optional     the temperature for token sampling.  verbose : bool, Optional     if True, VLM generated text will be printed in terminal in real-time. stream : bool, Optional     if True, returns a generator that yields the output in real-time.</p> Source code in <code>package/llm-ie/src/llm_ie/engines.py</code> <pre><code>def chat(self, messages:List[Dict[str,str]], max_new_tokens:int=2048, temperature:float=0.0, \n         verbose:bool=False, stream:bool=False, **kwrs) -&gt; Union[str, Generator[str, None, None]]:\n    \"\"\"\n    This method inputs chat messages and outputs VLM generated text.\n\n    Parameters:\n    ----------\n    messages : List[Dict[str,str]]\n        a list of dict with role and content. role must be one of {\"system\", \"user\", \"assistant\"}\n    max_new_tokens : str, Optional\n        the max number of new tokens VLM can generate. \n    temperature : float, Optional\n        the temperature for token sampling. \n    verbose : bool, Optional\n        if True, VLM generated text will be printed in terminal in real-time.\n    stream : bool, Optional\n        if True, returns a generator that yields the output in real-time.\n    \"\"\"\n    options={'temperature':temperature, 'num_ctx': self.num_ctx, 'num_predict': max_new_tokens, **kwrs}\n    if stream:\n        def _stream_generator():\n            response_stream = self.client.chat(\n                model=self.model_name, \n                messages=messages, \n                options=options,\n                stream=True, \n                keep_alive=self.keep_alive\n            )\n            for chunk in response_stream:\n                content_chunk = chunk.get('message', {}).get('content')\n                if content_chunk:\n                    yield content_chunk\n\n        return _stream_generator()\n\n    elif verbose:\n        response = self.client.chat(\n                        model=self.model_name, \n                        messages=messages, \n                        options=options,\n                        stream=True,\n                        keep_alive=self.keep_alive\n                    )\n\n        res = ''\n        for chunk in response:\n            content_chunk = chunk.get('message', {}).get('content')\n            print(content_chunk, end='', flush=True)\n            res += content_chunk\n        print('\\n')\n        return res\n\n    else:\n        response = self.client.chat(\n                            model=self.model_name, \n                            messages=messages, \n                            options=options,\n                            stream=False,\n                            keep_alive=self.keep_alive\n                        )\n        return response.get('message', {}).get('content')\n</code></pre>"},{"location":"api/engines/#llm_ie.engines.OllamaInferenceEngine.chat_async","title":"chat_async  <code>async</code>","text":"<pre><code>chat_async(\n    messages: List[Dict[str, str]],\n    max_new_tokens: int = 2048,\n    temperature: float = 0.0,\n    **kwrs\n) -&gt; str\n</code></pre> <p>Async version of chat method. Streaming is not supported.</p> Source code in <code>package/llm-ie/src/llm_ie/engines.py</code> <pre><code>async def chat_async(self, messages:List[Dict[str,str]], max_new_tokens:int=2048, temperature:float=0.0, **kwrs) -&gt; str:\n    \"\"\"\n    Async version of chat method. Streaming is not supported.\n    \"\"\"\n    response = await self.async_client.chat(\n                        model=self.model_name, \n                        messages=messages, \n                        options={'temperature':temperature, 'num_ctx': self.num_ctx, 'num_predict': max_new_tokens, **kwrs},\n                        stream=False,\n                        keep_alive=self.keep_alive\n                    )\n\n    return response['message']['content']\n</code></pre>"},{"location":"api/engines/#llm_ie.engines.OpenAIInferenceEngine","title":"llm_ie.engines.OpenAIInferenceEngine","text":"<pre><code>OpenAIInferenceEngine(\n    model: str, reasoning_model: bool = False, **kwrs\n)\n</code></pre> <p>               Bases: <code>InferenceEngine</code></p> <p>The OpenAI API inference engine. Supports OpenAI models and OpenAI compatible servers: - vLLM OpenAI compatible server (https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html) - Llama.cpp OpenAI compatible server (https://llama-cpp-python.readthedocs.io/en/latest/server/)</p> <p>For parameters and documentation, refer to https://platform.openai.com/docs/api-reference/introduction</p> Parameters: <p>model_name : str     model name as described in https://platform.openai.com/docs/models reasoning_model : bool, Optional     indicator for OpenAI reasoning models (\"o\" series).</p> Source code in <code>package/llm-ie/src/llm_ie/engines.py</code> <pre><code>def __init__(self, model:str, reasoning_model:bool=False, **kwrs):\n    \"\"\"\n    The OpenAI API inference engine. Supports OpenAI models and OpenAI compatible servers:\n    - vLLM OpenAI compatible server (https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html)\n    - Llama.cpp OpenAI compatible server (https://llama-cpp-python.readthedocs.io/en/latest/server/)\n\n    For parameters and documentation, refer to https://platform.openai.com/docs/api-reference/introduction\n\n    Parameters:\n    ----------\n    model_name : str\n        model name as described in https://platform.openai.com/docs/models\n    reasoning_model : bool, Optional\n        indicator for OpenAI reasoning models (\"o\" series).\n    \"\"\"\n    if importlib.util.find_spec(\"openai\") is None:\n        raise ImportError(\"OpenAI Python API library not found. Please install OpanAI (```pip install openai```).\")\n\n    from openai import OpenAI, AsyncOpenAI\n    self.client = OpenAI(**kwrs)\n    self.async_client = AsyncOpenAI(**kwrs)\n    self.model = model\n    self.reasoning_model = reasoning_model\n</code></pre>"},{"location":"api/engines/#llm_ie.engines.OpenAIInferenceEngine.chat","title":"chat","text":"<pre><code>chat(\n    messages: List[Dict[str, str]],\n    max_new_tokens: int = 2048,\n    temperature: float = 0.0,\n    verbose: bool = False,\n    stream: bool = False,\n    **kwrs\n) -&gt; Union[str, Generator[str, None, None]]\n</code></pre> <p>This method inputs chat messages and outputs LLM generated text.</p> Parameters: <p>messages : List[Dict[str,str]]     a list of dict with role and content. role must be one of {\"system\", \"user\", \"assistant\"} max_new_tokens : str, Optional     the max number of new tokens LLM can generate.  temperature : float, Optional     the temperature for token sampling.  verbose : bool, Optional     if True, VLM generated text will be printed in terminal in real-time. stream : bool, Optional     if True, returns a generator that yields the output in real-time.</p> Source code in <code>package/llm-ie/src/llm_ie/engines.py</code> <pre><code>def chat(self, messages:List[Dict[str,str]], max_new_tokens:int=2048, temperature:float=0.0, \n         verbose:bool=False, stream:bool=False, **kwrs) -&gt; Union[str, Generator[str, None, None]]:\n    \"\"\"\n    This method inputs chat messages and outputs LLM generated text.\n\n    Parameters:\n    ----------\n    messages : List[Dict[str,str]]\n        a list of dict with role and content. role must be one of {\"system\", \"user\", \"assistant\"}\n    max_new_tokens : str, Optional\n        the max number of new tokens LLM can generate. \n    temperature : float, Optional\n        the temperature for token sampling. \n    verbose : bool, Optional\n        if True, VLM generated text will be printed in terminal in real-time.\n    stream : bool, Optional\n        if True, returns a generator that yields the output in real-time.\n    \"\"\"\n    # For reasoning models\n    if self.reasoning_model:\n        # Reasoning models do not support temperature parameter\n        if temperature != 0.0:\n            warnings.warn(\"Reasoning models do not support temperature parameter. Will be ignored.\", UserWarning)\n\n        # Reasoning models do not support system prompts\n        if any(msg['role'] == 'system' for msg in messages):\n            warnings.warn(\"Reasoning models do not support system prompts. Will be ignored.\", UserWarning)\n            messages = [msg for msg in messages if msg['role'] != 'system']\n\n\n        if stream:\n            def _stream_generator():\n                response_stream = self.client.chat.completions.create(\n                                        model=self.model,\n                                        messages=messages,\n                                        max_completion_tokens=max_new_tokens,\n                                        stream=True,\n                                        **kwrs\n                                    )\n                for chunk in response_stream:\n                    if len(chunk.choices) &gt; 0:\n                        if chunk.choices[0].delta.content is not None:\n                            yield chunk.choices[0].delta.content\n                        if chunk.choices[0].finish_reason == \"length\":\n                            warnings.warn(\"Model stopped generating due to context length limit.\", RuntimeWarning)\n                            if self.reasoning_model:\n                                warnings.warn(\"max_new_tokens includes reasoning tokens and output tokens.\", UserWarning)\n            return _stream_generator()\n\n        elif verbose:\n            response = self.client.chat.completions.create(\n                model=self.model,\n                messages=messages,\n                max_completion_tokens=max_new_tokens,\n                stream=True,\n                **kwrs\n            )\n            res = ''\n            for chunk in response:\n                if len(chunk.choices) &gt; 0:\n                    if chunk.choices[0].delta.content is not None:\n                        res += chunk.choices[0].delta.content\n                        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n                    if chunk.choices[0].finish_reason == \"length\":\n                        warnings.warn(\"Model stopped generating due to context length limit.\", RuntimeWarning)\n                        if self.reasoning_model:\n                            warnings.warn(\"max_new_tokens includes reasoning tokens and output tokens.\", UserWarning)\n\n            print('\\n')\n            return res\n        else:\n            response = self.client.chat.completions.create(\n                model=self.model,\n                messages=messages,\n                max_completion_tokens=max_new_tokens,\n                stream=False,\n                **kwrs\n            )\n            return response.choices[0].message.content\n\n    # For non-reasoning models\n    else:\n        if stream:\n            def _stream_generator():\n                response_stream = self.client.chat.completions.create(\n                    model=self.model,\n                    messages=messages,\n                    max_tokens=max_new_tokens,\n                    temperature=temperature,\n                    stream=True,\n                    **kwrs\n                )\n                for chunk in response_stream:\n                    if len(chunk.choices) &gt; 0:\n                        if chunk.choices[0].delta.content is not None:\n                            yield chunk.choices[0].delta.content\n                        if chunk.choices[0].finish_reason == \"length\":\n                            warnings.warn(\"Model stopped generating due to context length limit.\", RuntimeWarning)\n                            if self.reasoning_model:\n                                warnings.warn(\"max_new_tokens includes reasoning tokens and output tokens.\", UserWarning)\n            return _stream_generator()\n\n        elif verbose:\n            response = self.client.chat.completions.create(\n                model=self.model,\n                messages=messages,\n                max_tokens=max_new_tokens,\n                temperature=temperature,\n                stream=True,\n                **kwrs\n            )\n            res = ''\n            for chunk in response:\n                if len(chunk.choices) &gt; 0:\n                    if chunk.choices[0].delta.content is not None:\n                        res += chunk.choices[0].delta.content\n                        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n                    if chunk.choices[0].finish_reason == \"length\":\n                        warnings.warn(\"Model stopped generating due to context length limit.\", RuntimeWarning)\n                        if self.reasoning_model:\n                            warnings.warn(\"max_new_tokens includes reasoning tokens and output tokens.\", UserWarning)\n\n            print('\\n')\n            return res\n\n        else:\n            response = self.client.chat.completions.create(\n                model=self.model,\n                messages=messages,\n                max_tokens=max_new_tokens,\n                temperature=temperature,\n                stream=False,\n                **kwrs\n            )\n\n        return response.choices[0].message.content\n</code></pre>"},{"location":"api/engines/#llm_ie.engines.OpenAIInferenceEngine.chat_async","title":"chat_async  <code>async</code>","text":"<pre><code>chat_async(\n    messages: List[Dict[str, str]],\n    max_new_tokens: int = 4096,\n    temperature: float = 0.0,\n    **kwrs\n) -&gt; str\n</code></pre> <p>Async version of chat method. Streaming is not supported.</p> Source code in <code>package/llm-ie/src/llm_ie/engines.py</code> <pre><code>async def chat_async(self, messages:List[Dict[str,str]], max_new_tokens:int=4096, temperature:float=0.0, **kwrs) -&gt; str:\n    \"\"\"\n    Async version of chat method. Streaming is not supported.\n    \"\"\"\n    if self.reasoning_model:\n        # Reasoning models do not support temperature parameter\n        if temperature != 0.0:\n            warnings.warn(\"Reasoning models do not support temperature parameter. Will be ignored.\", UserWarning)\n\n        # Reasoning models do not support system prompts\n        if any(msg['role'] == 'system' for msg in messages):\n            warnings.warn(\"Reasoning models do not support system prompts. Will be ignored.\", UserWarning)\n            messages = [msg for msg in messages if msg['role'] != 'system']\n\n        response = await self.async_client.chat.completions.create(\n            model=self.model,\n            messages=messages,\n            max_completion_tokens=max_new_tokens,\n            stream=False,\n            **kwrs\n        )\n\n    else:\n        response = await self.async_client.chat.completions.create(\n            model=self.model,\n            messages=messages,\n            max_tokens=max_new_tokens,\n            temperature=temperature,\n            stream=False,\n            **kwrs\n        )\n\n    if response.choices[0].finish_reason == \"length\":\n        warnings.warn(\"Model stopped generating due to context length limit.\", RuntimeWarning)\n        if self.reasoning_model:\n            warnings.warn(\"max_new_tokens includes reasoning tokens and output tokens.\", UserWarning)\n\n    return response.choices[0].message.content\n</code></pre>"},{"location":"api/engines/#llm_ie.engines.AzureOpenAIInferenceEngine","title":"llm_ie.engines.AzureOpenAIInferenceEngine","text":"<pre><code>AzureOpenAIInferenceEngine(\n    model: str,\n    api_version: str,\n    reasoning_model: bool = False,\n    **kwrs\n)\n</code></pre> <p>               Bases: <code>OpenAIInferenceEngine</code></p> <p>The Azure OpenAI API inference engine. For parameters and documentation, refer to  - https://azure.microsoft.com/en-us/products/ai-services/openai-service - https://learn.microsoft.com/en-us/azure/ai-services/openai/quickstart</p> Parameters: <p>model : str     model name as described in https://platform.openai.com/docs/models api_version : str     the Azure OpenAI API version reasoning_model : bool, Optional     indicator for OpenAI reasoning models (\"o\" series).</p> Source code in <code>package/llm-ie/src/llm_ie/engines.py</code> <pre><code>def __init__(self, model:str, api_version:str, reasoning_model:bool=False, **kwrs):\n    \"\"\"\n    The Azure OpenAI API inference engine.\n    For parameters and documentation, refer to \n    - https://azure.microsoft.com/en-us/products/ai-services/openai-service\n    - https://learn.microsoft.com/en-us/azure/ai-services/openai/quickstart\n\n    Parameters:\n    ----------\n    model : str\n        model name as described in https://platform.openai.com/docs/models\n    api_version : str\n        the Azure OpenAI API version\n    reasoning_model : bool, Optional\n        indicator for OpenAI reasoning models (\"o\" series).\n    \"\"\"\n    if importlib.util.find_spec(\"openai\") is None:\n        raise ImportError(\"OpenAI Python API library not found. Please install OpanAI (```pip install openai```).\")\n\n    from openai import AzureOpenAI, AsyncAzureOpenAI\n    self.model = model\n    self.api_version = api_version\n    self.client = AzureOpenAI(api_version=self.api_version, \n                              **kwrs)\n    self.async_client = AsyncAzureOpenAI(api_version=self.api_version, \n                                         **kwrs)\n    self.reasoning_model = reasoning_model\n</code></pre>"},{"location":"api/engines/#llm_ie.engines.HuggingFaceHubInferenceEngine","title":"llm_ie.engines.HuggingFaceHubInferenceEngine","text":"<pre><code>HuggingFaceHubInferenceEngine(\n    model: str = None,\n    token: Union[str, bool] = None,\n    base_url: str = None,\n    api_key: str = None,\n    **kwrs\n)\n</code></pre> <p>               Bases: <code>InferenceEngine</code></p> <p>The Huggingface_hub InferenceClient inference engine. For parameters and documentation, refer to https://huggingface.co/docs/huggingface_hub/en/package_reference/inference_client</p> Source code in <code>package/llm-ie/src/llm_ie/engines.py</code> <pre><code>def __init__(self, model:str=None, token:Union[str, bool]=None, base_url:str=None, api_key:str=None, **kwrs):\n    \"\"\"\n    The Huggingface_hub InferenceClient inference engine.\n    For parameters and documentation, refer to https://huggingface.co/docs/huggingface_hub/en/package_reference/inference_client\n    \"\"\"\n    if importlib.util.find_spec(\"huggingface_hub\") is None:\n        raise ImportError(\"huggingface-hub not found. Please install huggingface-hub (```pip install huggingface-hub```).\")\n\n    from huggingface_hub import InferenceClient, AsyncInferenceClient\n    self.client = InferenceClient(model=model, token=token, base_url=base_url, api_key=api_key, **kwrs)\n    self.client_async = AsyncInferenceClient(model=model, token=token, base_url=base_url, api_key=api_key, **kwrs)\n</code></pre>"},{"location":"api/engines/#llm_ie.engines.HuggingFaceHubInferenceEngine.chat","title":"chat","text":"<pre><code>chat(\n    messages: List[Dict[str, str]],\n    max_new_tokens: int = 2048,\n    temperature: float = 0.0,\n    verbose: bool = False,\n    stream: bool = False,\n    **kwrs\n) -&gt; Union[str, Generator[str, None, None]]\n</code></pre> <p>This method inputs chat messages and outputs LLM generated text.</p> Parameters: <p>messages : List[Dict[str,str]]     a list of dict with role and content. role must be one of {\"system\", \"user\", \"assistant\"} max_new_tokens : str, Optional     the max number of new tokens LLM can generate.  temperature : float, Optional     the temperature for token sampling.  verbose : bool, Optional     if True, VLM generated text will be printed in terminal in real-time. stream : bool, Optional     if True, returns a generator that yields the output in real-time.</p> Source code in <code>package/llm-ie/src/llm_ie/engines.py</code> <pre><code>def chat(self, messages:List[Dict[str,str]], max_new_tokens:int=2048, temperature:float=0.0, \n         verbose:bool=False, stream:bool=False, **kwrs) -&gt; Union[str, Generator[str, None, None]]:\n    \"\"\"\n    This method inputs chat messages and outputs LLM generated text.\n\n    Parameters:\n    ----------\n    messages : List[Dict[str,str]]\n        a list of dict with role and content. role must be one of {\"system\", \"user\", \"assistant\"}\n    max_new_tokens : str, Optional\n        the max number of new tokens LLM can generate. \n    temperature : float, Optional\n        the temperature for token sampling. \n    verbose : bool, Optional\n        if True, VLM generated text will be printed in terminal in real-time.\n    stream : bool, Optional\n        if True, returns a generator that yields the output in real-time.\n    \"\"\"\n    if stream:\n        def _stream_generator():\n            response_stream = self.client.chat.completions.create(\n                                messages=messages,\n                                max_tokens=max_new_tokens,\n                                temperature=temperature,\n                                stream=True,\n                                **kwrs\n                            )\n            for chunk in response_stream:\n                content_chunk = chunk.get('choices')[0].get('delta').get('content')\n                if content_chunk:\n                    yield content_chunk\n\n        return _stream_generator()\n\n    elif verbose:\n        response = self.client.chat.completions.create(\n                        messages=messages,\n                        max_tokens=max_new_tokens,\n                        temperature=temperature,\n                        stream=True,\n                        **kwrs\n                    )\n\n        res = ''\n        for chunk in response:\n            content_chunk = chunk.get('choices')[0].get('delta').get('content')\n            if content_chunk:\n                res += content_chunk\n                print(content_chunk, end='', flush=True)\n        return res\n\n    else:\n        response = self.client.chat.completions.create(\n                            messages=messages,\n                            max_tokens=max_new_tokens,\n                            temperature=temperature,\n                            stream=False,\n                            **kwrs\n                        )\n        return response.choices[0].message.content\n</code></pre>"},{"location":"api/engines/#llm_ie.engines.HuggingFaceHubInferenceEngine.chat_async","title":"chat_async  <code>async</code>","text":"<pre><code>chat_async(\n    messages: List[Dict[str, str]],\n    max_new_tokens: int = 2048,\n    temperature: float = 0.0,\n    **kwrs\n) -&gt; str\n</code></pre> <p>Async version of chat method. Streaming is not supported.</p> Source code in <code>package/llm-ie/src/llm_ie/engines.py</code> <pre><code>async def chat_async(self, messages:List[Dict[str,str]], max_new_tokens:int=2048, temperature:float=0.0, **kwrs) -&gt; str:\n    \"\"\"\n    Async version of chat method. Streaming is not supported.\n    \"\"\"\n    response = await self.client_async.chat.completions.create(\n                messages=messages,\n                max_tokens=max_new_tokens,\n                temperature=temperature,\n                stream=False,\n                **kwrs\n            )\n\n    return response.choices[0].message.content\n</code></pre>"},{"location":"api/engines/#llm_ie.engines.LiteLLMInferenceEngine","title":"llm_ie.engines.LiteLLMInferenceEngine","text":"<pre><code>LiteLLMInferenceEngine(\n    model: str = None,\n    base_url: str = None,\n    api_key: str = None,\n)\n</code></pre> <p>               Bases: <code>InferenceEngine</code></p> <p>The LiteLLM inference engine.  For parameters and documentation, refer to https://github.com/BerriAI/litellm?tab=readme-ov-file</p> Parameters: <p>model : str     the model name base_url : str, Optional     the base url for the LLM server api_key : str, Optional     the API key for the LLM server</p> Source code in <code>package/llm-ie/src/llm_ie/engines.py</code> <pre><code>def __init__(self, model:str=None, base_url:str=None, api_key:str=None):\n    \"\"\"\n    The LiteLLM inference engine. \n    For parameters and documentation, refer to https://github.com/BerriAI/litellm?tab=readme-ov-file\n\n    Parameters:\n    ----------\n    model : str\n        the model name\n    base_url : str, Optional\n        the base url for the LLM server\n    api_key : str, Optional\n        the API key for the LLM server\n    \"\"\"\n    if importlib.util.find_spec(\"litellm\") is None:\n        raise ImportError(\"litellm not found. Please install litellm (```pip install litellm```).\")\n\n    import litellm \n    self.litellm = litellm\n    self.model = model\n    self.base_url = base_url\n    self.api_key = api_key\n</code></pre>"},{"location":"api/engines/#llm_ie.engines.LiteLLMInferenceEngine.chat","title":"chat","text":"<pre><code>chat(\n    messages: List[Dict[str, str]],\n    max_new_tokens: int = 2048,\n    temperature: float = 0.0,\n    verbose: bool = False,\n    stream: bool = False,\n    **kwrs\n) -&gt; Union[str, Generator[str, None, None]]\n</code></pre> <p>This method inputs chat messages and outputs LLM generated text.</p> Parameters: <p>messages : List[Dict[str,str]]     a list of dict with role and content. role must be one of {\"system\", \"user\", \"assistant\"} max_new_tokens : str, Optional     the max number of new tokens LLM can generate.  temperature : float, Optional     the temperature for token sampling.  verbose : bool, Optional     if True, VLM generated text will be printed in terminal in real-time. stream : bool, Optional     if True, returns a generator that yields the output in real-time.</p> Source code in <code>package/llm-ie/src/llm_ie/engines.py</code> <pre><code>def chat(self, messages:List[Dict[str,str]], max_new_tokens:int=2048, temperature:float=0.0, \n         verbose:bool=False, stream:bool=False, **kwrs) -&gt; Union[str, Generator[str, None, None]]:\n    \"\"\"\n    This method inputs chat messages and outputs LLM generated text.\n\n    Parameters:\n    ----------\n    messages : List[Dict[str,str]]\n        a list of dict with role and content. role must be one of {\"system\", \"user\", \"assistant\"}\n    max_new_tokens : str, Optional\n        the max number of new tokens LLM can generate. \n    temperature : float, Optional\n        the temperature for token sampling. \n    verbose : bool, Optional\n        if True, VLM generated text will be printed in terminal in real-time.\n    stream : bool, Optional\n        if True, returns a generator that yields the output in real-time.\n    \"\"\"\n    if stream:\n        def _stream_generator():\n            response_stream = self.litellm.completion(\n                model=self.model,\n                messages=messages,\n                max_tokens=max_new_tokens,\n                temperature=temperature,\n                stream=True,\n                base_url=self.base_url,\n                api_key=self.api_key,\n                **kwrs\n            )\n\n            for chunk in response_stream:\n                chunk_content = chunk.get('choices')[0].get('delta').get('content')\n                if chunk_content:\n                    yield chunk_content\n\n        return _stream_generator()\n\n    elif verbose:\n        response = self.litellm.completion(\n            model=self.model,\n            messages=messages,\n            max_tokens=max_new_tokens,\n            temperature=temperature,\n            stream=True,\n            base_url=self.base_url,\n            api_key=self.api_key,\n            **kwrs\n        )\n\n        res = ''\n        for chunk in response:\n            chunk_content = chunk.get('choices')[0].get('delta').get('content')\n            if chunk_content:\n                res += chunk_content\n                print(chunk_content, end='', flush=True)\n\n        return res\n\n    else:\n        response = self.litellm.completion(\n                model=self.model,\n                messages=messages,\n                max_tokens=max_new_tokens,\n                temperature=temperature,\n                stream=False,\n                base_url=self.base_url,\n                api_key=self.api_key,\n                **kwrs\n            )\n        return response.choices[0].message.content\n</code></pre>"},{"location":"api/engines/#llm_ie.engines.LiteLLMInferenceEngine.chat_async","title":"chat_async  <code>async</code>","text":"<pre><code>chat_async(\n    messages: List[Dict[str, str]],\n    max_new_tokens: int = 2048,\n    temperature: float = 0.0,\n    **kwrs\n) -&gt; str\n</code></pre> <p>Async version of chat method. Streaming is not supported.</p> Source code in <code>package/llm-ie/src/llm_ie/engines.py</code> <pre><code>async def chat_async(self, messages:List[Dict[str,str]], max_new_tokens:int=2048, temperature:float=0.0, **kwrs) -&gt; str:\n    \"\"\"\n    Async version of chat method. Streaming is not supported.\n    \"\"\"\n    response = await self.litellm.acompletion(\n        model=self.model,\n        messages=messages,\n        max_tokens=max_new_tokens,\n        temperature=temperature,\n        stream=False,\n        base_url=self.base_url,\n        api_key=self.api_key,\n        **kwrs\n    )\n\n    return response.get('choices')[0].get('message').get('content')\n</code></pre>"},{"location":"api/extractors/","title":"Extractors API","text":""},{"location":"api/extractors/#frame-extractors","title":"Frame Extractors","text":""},{"location":"api/extractors/#llm_ie.extractors.DirectFrameExtractor","title":"llm_ie.extractors.DirectFrameExtractor","text":"<pre><code>DirectFrameExtractor(\n    inference_engine: InferenceEngine,\n    unit_chunker: UnitChunker,\n    prompt_template: str,\n    system_prompt: str = None,\n    context_chunker: ContextChunker = None,\n    **kwrs\n)\n</code></pre> <p>               Bases: <code>FrameExtractor</code></p> <p>This class is for general unit-context frame extraction. Input LLM inference engine, system prompt (optional), prompt template (with instruction, few-shot examples).</p> Parameters: <p>inference_engine : InferenceEngine     the LLM inferencing engine object. Must implements the chat() method. unit_chunker : UnitChunker     the unit chunker object that determines how to chunk the document text into units. prompt_template : str     prompt template with \"{{}}\" placeholder. system_prompt : str, Optional     system prompt. context_chunker : ContextChunker     the context chunker object that determines how to get context for each unit. Source code in <code>package/llm-ie/src/llm_ie/extractors.py</code> <pre><code>def __init__(self, inference_engine:InferenceEngine, unit_chunker:UnitChunker, \n             prompt_template:str, system_prompt:str=None, context_chunker:ContextChunker=None, **kwrs):\n    \"\"\"\n    This class is for general unit-context frame extraction.\n    Input LLM inference engine, system prompt (optional), prompt template (with instruction, few-shot examples).\n\n    Parameters:\n    ----------\n    inference_engine : InferenceEngine\n        the LLM inferencing engine object. Must implements the chat() method.\n    unit_chunker : UnitChunker\n        the unit chunker object that determines how to chunk the document text into units.\n    prompt_template : str\n        prompt template with \"{{&lt;placeholder name&gt;}}\" placeholder.\n    system_prompt : str, Optional\n        system prompt.\n    context_chunker : ContextChunker\n        the context chunker object that determines how to get context for each unit.\n    \"\"\"\n    super().__init__(inference_engine=inference_engine,\n                     unit_chunker=unit_chunker,\n                     prompt_template=prompt_template,\n                     system_prompt=system_prompt,\n                     context_chunker=context_chunker,\n                     **kwrs)\n</code></pre>"},{"location":"api/extractors/#llm_ie.extractors.DirectFrameExtractor.extract","title":"extract","text":"<pre><code>extract(\n    text_content: Union[str, Dict[str, str]],\n    max_new_tokens: int = 2048,\n    document_key: str = None,\n    temperature: float = 0.0,\n    verbose: bool = False,\n    return_messages_log: bool = False,\n    **kwrs\n) -&gt; List[FrameExtractionUnitResult]\n</code></pre> <p>This method inputs a text and outputs a list of outputs per unit.</p> Parameters: <p>text_content : Union[str, Dict[str,str]]     the input text content to put in prompt template.      If str, the prompt template must has only 1 placeholder {{}}, regardless of placeholder name.     If dict, all the keys must be included in the prompt template placeholder {{}}. max_new_tokens : int, Optional     the max number of new tokens LLM should generate.  document_key : str, Optional     specify the key in text_content where document text is.      If text_content is str, this parameter will be ignored. temperature : float, Optional     the temperature for token sampling. verbose : bool, Optional     if True, LLM generated text will be printed in terminal in real-time.  return_messages_log : bool, Optional     if True, a list of messages will be returned. <p>Return : List[FrameExtractionUnitResult]     the output from LLM for each unit. Contains the start, end, text, and generated text.</p> Source code in <code>package/llm-ie/src/llm_ie/extractors.py</code> <pre><code>def extract(self, text_content:Union[str, Dict[str,str]], max_new_tokens:int=2048, \n            document_key:str=None, temperature:float=0.0, verbose:bool=False, return_messages_log:bool=False, **kwrs) -&gt; List[FrameExtractionUnitResult]:\n    \"\"\"\n    This method inputs a text and outputs a list of outputs per unit.\n\n    Parameters:\n    ----------\n    text_content : Union[str, Dict[str,str]]\n        the input text content to put in prompt template. \n        If str, the prompt template must has only 1 placeholder {{&lt;placeholder name&gt;}}, regardless of placeholder name.\n        If dict, all the keys must be included in the prompt template placeholder {{&lt;placeholder name&gt;}}.\n    max_new_tokens : int, Optional\n        the max number of new tokens LLM should generate. \n    document_key : str, Optional\n        specify the key in text_content where document text is. \n        If text_content is str, this parameter will be ignored.\n    temperature : float, Optional\n        the temperature for token sampling.\n    verbose : bool, Optional\n        if True, LLM generated text will be printed in terminal in real-time. \n    return_messages_log : bool, Optional\n        if True, a list of messages will be returned.\n\n    Return : List[FrameExtractionUnitResult]\n        the output from LLM for each unit. Contains the start, end, text, and generated text.\n    \"\"\"\n    # define output\n    output = []\n    # unit chunking\n    if isinstance(text_content, str):\n        doc_text = text_content\n\n    elif isinstance(text_content, dict):\n        if document_key is None:\n            raise ValueError(\"document_key must be provided when text_content is dict.\")\n        doc_text = text_content[document_key]\n\n    units = self.unit_chunker.chunk(doc_text)\n    # context chunker init\n    self.context_chunker.fit(doc_text, units)\n    # messages log\n    if return_messages_log:\n        messages_log = []\n\n    # generate unit by unit\n    for i, unit in enumerate(units):\n        # construct chat messages\n        messages = []\n        if self.system_prompt:\n            messages.append({'role': 'system', 'content': self.system_prompt})\n\n        context = self.context_chunker.chunk(unit)\n\n        if context == \"\":\n            # no context, just place unit in user prompt\n            if isinstance(text_content, str):\n                messages.append({'role': 'user', 'content': self._get_user_prompt(unit.text)})\n            else:\n                unit_content = text_content.copy()\n                unit_content[document_key] = unit.text\n                messages.append({'role': 'user', 'content': self._get_user_prompt(unit_content)})\n        else:\n            # insert context to user prompt\n            if isinstance(text_content, str):\n                messages.append({'role': 'user', 'content': self._get_user_prompt(context)})\n            else:\n                context_content = text_content.copy()\n                context_content[document_key] = context\n                messages.append({'role': 'user', 'content': self._get_user_prompt(context_content)})\n            # simulate conversation where assistant confirms\n            messages.append({'role': 'assistant', 'content': 'Sure, please provide the unit text (e.g., sentence, line, chunk) of interest.'})\n            # place unit of interest\n            messages.append({'role': 'user', 'content': unit.text})\n\n        if verbose:\n            print(f\"\\n\\n{Fore.GREEN}Unit {i}:{Style.RESET_ALL}\\n{unit.text}\\n\")\n            if context != \"\":\n                print(f\"{Fore.YELLOW}Context:{Style.RESET_ALL}\\n{context}\\n\")\n\n            print(f\"{Fore.BLUE}Extraction:{Style.RESET_ALL}\")\n\n            response_stream = self.inference_engine.chat(\n                            messages=messages, \n                            max_new_tokens=max_new_tokens, \n                            temperature=temperature,\n                            stream=True,\n                            **kwrs\n                        )\n\n            gen_text = \"\"\n            for chunk in response_stream:\n                gen_text += chunk\n                print(chunk, end='', flush=True)\n\n        else:\n            gen_text = self.inference_engine.chat(\n                            messages=messages, \n                            max_new_tokens=max_new_tokens, \n                            temperature=temperature,\n                            stream=False,\n                            **kwrs\n                        )\n\n        if return_messages_log:\n            messages.append({\"role\": \"assistant\", \"content\": gen_text})\n            messages_log.append(messages)\n\n        # add to output\n        result = FrameExtractionUnitResult(\n                        start=unit.start,\n                        end=unit.end,\n                        text=unit.text,\n                        gen_text=gen_text)\n        output.append(result)\n\n    if return_messages_log:\n        return output, messages_log\n\n    return output\n</code></pre>"},{"location":"api/extractors/#llm_ie.extractors.DirectFrameExtractor.stream","title":"stream","text":"<pre><code>stream(\n    text_content: Union[str, Dict[str, str]],\n    max_new_tokens: int = 2048,\n    document_key: str = None,\n    temperature: float = 0.0,\n    **kwrs\n) -&gt; Generator[\n    Dict[str, Any], None, List[FrameExtractionUnitResult]\n]\n</code></pre> <p>Streams LLM responses per unit with structured event types, and returns collected data for post-processing.</p> Yields: <p>Dict[str, Any]: (type, data)     - {\"type\": \"info\", \"data\": str_message}: General informational messages.     - {\"type\": \"unit\", \"data\": dict_unit_info}: Signals start of a new unit. dict_unit_info contains {'id', 'text', 'start', 'end'}     - {\"type\": \"context\", \"data\": str_context}: Context string for the current unit.     - {\"type\": \"llm_chunk\", \"data\": str_chunk}: A raw chunk from the LLM.</p> Returns: <p>List[FrameExtractionUnitResult]:     A list of FrameExtractionUnitResult objects, each containing the     original unit details and the fully accumulated 'gen_text' from the LLM.</p> Source code in <code>package/llm-ie/src/llm_ie/extractors.py</code> <pre><code>def stream(self, text_content: Union[str, Dict[str, str]], max_new_tokens: int = 2048, document_key: str = None, \n           temperature: float = 0.0, **kwrs) -&gt; Generator[Dict[str, Any], None, List[FrameExtractionUnitResult]]:\n    \"\"\"\n    Streams LLM responses per unit with structured event types,\n    and returns collected data for post-processing.\n\n    Yields:\n    -------\n    Dict[str, Any]: (type, data)\n        - {\"type\": \"info\", \"data\": str_message}: General informational messages.\n        - {\"type\": \"unit\", \"data\": dict_unit_info}: Signals start of a new unit. dict_unit_info contains {'id', 'text', 'start', 'end'}\n        - {\"type\": \"context\", \"data\": str_context}: Context string for the current unit.\n        - {\"type\": \"llm_chunk\", \"data\": str_chunk}: A raw chunk from the LLM.\n\n    Returns:\n    --------\n    List[FrameExtractionUnitResult]:\n        A list of FrameExtractionUnitResult objects, each containing the\n        original unit details and the fully accumulated 'gen_text' from the LLM.\n    \"\"\"\n    collected_results: List[FrameExtractionUnitResult] = []\n\n    if isinstance(text_content, str):\n        doc_text = text_content\n    elif isinstance(text_content, dict):\n        if document_key is None:\n            raise ValueError(\"document_key must be provided when text_content is dict.\")\n        if document_key not in text_content:\n            raise ValueError(f\"document_key '{document_key}' not found in text_content.\")\n        doc_text = text_content[document_key]\n    else:\n        raise TypeError(\"text_content must be a string or a dictionary.\")\n\n    units: List[FrameExtractionUnit] = self.unit_chunker.chunk(doc_text)\n    self.context_chunker.fit(doc_text, units)\n\n    yield {\"type\": \"info\", \"data\": f\"Starting LLM processing for {len(units)} units.\"}\n\n    for i, unit in enumerate(units):\n        unit_info_payload = {\"id\": i, \"text\": unit.text, \"start\": unit.start, \"end\": unit.end}\n        yield {\"type\": \"unit\", \"data\": unit_info_payload}\n\n        messages = []\n        if self.system_prompt:\n            messages.append({'role': 'system', 'content': self.system_prompt})\n\n        context_str = self.context_chunker.chunk(unit)\n\n        # Construct prompt input based on whether text_content was str or dict\n        if context_str:\n            yield {\"type\": \"context\", \"data\": context_str}\n            prompt_input_for_context = context_str\n            if isinstance(text_content, dict):\n                context_content_dict = text_content.copy()\n                context_content_dict[document_key] = context_str\n                prompt_input_for_context = context_content_dict\n            messages.append({'role': 'user', 'content': self._get_user_prompt(prompt_input_for_context)})\n            messages.append({'role': 'assistant', 'content': 'Sure, please provide the unit text (e.g., sentence, line, chunk) of interest.'})\n            messages.append({'role': 'user', 'content': unit.text})\n        else: # No context\n            prompt_input_for_unit = unit.text\n            if isinstance(text_content, dict):\n                unit_content_dict = text_content.copy()\n                unit_content_dict[document_key] = unit.text\n                prompt_input_for_unit = unit_content_dict\n            messages.append({'role': 'user', 'content': self._get_user_prompt(prompt_input_for_unit)})\n\n        current_gen_text = \"\"\n\n        response_stream = self.inference_engine.chat(\n            messages=messages,\n            max_new_tokens=max_new_tokens,\n            temperature=temperature,\n            stream=True,\n            **kwrs\n        )\n        for chunk in response_stream:\n            yield {\"type\": \"llm_chunk\", \"data\": chunk}\n            current_gen_text += chunk\n\n        # Store the result for this unit\n        result_for_unit = FrameExtractionUnitResult(\n            start=unit.start,\n            end=unit.end,\n            text=unit.text,\n            gen_text=current_gen_text\n        )\n        collected_results.append(result_for_unit)\n\n    yield {\"type\": \"info\", \"data\": \"All units processed by LLM.\"}\n    return collected_results\n</code></pre>"},{"location":"api/extractors/#llm_ie.extractors.DirectFrameExtractor.extract_async","title":"extract_async  <code>async</code>","text":"<pre><code>extract_async(\n    text_content: Union[str, Dict[str, str]],\n    max_new_tokens: int = 2048,\n    document_key: str = None,\n    temperature: float = 0.0,\n    concurrent_batch_size: int = 32,\n    return_messages_log: bool = False,\n    **kwrs\n) -&gt; List[FrameExtractionUnitResult]\n</code></pre> <p>This is the asynchronous version of the extract() method.</p> Parameters: <p>text_content : Union[str, Dict[str,str]]     the input text content to put in prompt template.      If str, the prompt template must has only 1 placeholder {{}}, regardless of placeholder name.     If dict, all the keys must be included in the prompt template placeholder {{}}. max_new_tokens : int, Optional     the max number of new tokens LLM should generate.  document_key : str, Optional     specify the key in text_content where document text is.      If text_content is str, this parameter will be ignored. temperature : float, Optional     the temperature for token sampling. concurrent_batch_size : int, Optional     the batch size for concurrent processing.  return_messages_log : bool, Optional     if True, a list of messages will be returned. <p>Return : List[FrameExtractionUnitResult]     the output from LLM for each unit. Contains the start, end, text, and generated text.</p> Source code in <code>package/llm-ie/src/llm_ie/extractors.py</code> <pre><code>async def extract_async(self, text_content:Union[str, Dict[str,str]], max_new_tokens:int=2048, document_key:str=None, temperature:float=0.0, \n                        concurrent_batch_size:int=32, return_messages_log:bool=False, **kwrs) -&gt; List[FrameExtractionUnitResult]:\n    \"\"\"\n    This is the asynchronous version of the extract() method.\n\n    Parameters:\n    ----------\n    text_content : Union[str, Dict[str,str]]\n        the input text content to put in prompt template. \n        If str, the prompt template must has only 1 placeholder {{&lt;placeholder name&gt;}}, regardless of placeholder name.\n        If dict, all the keys must be included in the prompt template placeholder {{&lt;placeholder name&gt;}}.\n    max_new_tokens : int, Optional\n        the max number of new tokens LLM should generate. \n    document_key : str, Optional\n        specify the key in text_content where document text is. \n        If text_content is str, this parameter will be ignored.\n    temperature : float, Optional\n        the temperature for token sampling.\n    concurrent_batch_size : int, Optional\n        the batch size for concurrent processing. \n    return_messages_log : bool, Optional\n        if True, a list of messages will be returned.\n\n    Return : List[FrameExtractionUnitResult]\n        the output from LLM for each unit. Contains the start, end, text, and generated text.\n    \"\"\"\n    if isinstance(text_content, str):\n        doc_text = text_content\n    elif isinstance(text_content, dict):\n        if document_key is None:\n            raise ValueError(\"document_key must be provided when text_content is dict.\")\n        if document_key not in text_content:\n             raise ValueError(f\"document_key '{document_key}' not found in text_content dictionary.\")\n        doc_text = text_content[document_key]\n    else:\n        raise TypeError(\"text_content must be a string or a dictionary.\")\n\n    units = self.unit_chunker.chunk(doc_text)\n\n    # context chunker init \n    self.context_chunker.fit(doc_text, units)\n\n    # Prepare inputs for all units first\n    tasks_input = []\n    for i, unit in enumerate(units):\n        # construct chat messages\n        messages = []\n        if self.system_prompt:\n            messages.append({'role': 'system', 'content': self.system_prompt})\n\n        context = self.context_chunker.chunk(unit)\n\n        if context == \"\":\n             # no context, just place unit in user prompt\n            if isinstance(text_content, str):\n                messages.append({'role': 'user', 'content': self._get_user_prompt(unit.text)})\n            else:\n                unit_content = text_content.copy()\n                unit_content[document_key] = unit.text\n                messages.append({'role': 'user', 'content': self._get_user_prompt(unit_content)})\n        else:\n            # insert context to user prompt\n            if isinstance(text_content, str):\n                messages.append({'role': 'user', 'content': self._get_user_prompt(context)})\n            else:\n                context_content = text_content.copy()\n                context_content[document_key] = context\n                messages.append({'role': 'user', 'content': self._get_user_prompt(context_content)})\n            # simulate conversation where assistant confirms\n            messages.append({'role': 'assistant', 'content': 'Sure, please provide the unit text (e.g., sentence, line, chunk) of interest.'})\n            # place unit of interest\n            messages.append({'role': 'user', 'content': unit.text})\n\n        # Store unit and messages together for the task\n        tasks_input.append({\"unit\": unit, \"messages\": messages, \"original_index\": i})\n\n    # Process units concurrently with asyncio.Semaphore\n    semaphore = asyncio.Semaphore(concurrent_batch_size)\n\n    async def semaphore_helper(task_data: Dict, max_new_tokens: int, temperature: float, **kwrs):\n        unit = task_data[\"unit\"]\n        messages = task_data[\"messages\"]\n        original_index = task_data[\"original_index\"]\n\n        async with semaphore:\n            gen_text = await self.inference_engine.chat_async(\n                messages=messages,\n                max_new_tokens=max_new_tokens,\n                temperature=temperature,\n                **kwrs\n            )\n        return {\"original_index\": original_index, \"unit\": unit, \"gen_text\": gen_text, \"messages\": messages}\n\n    # Create and gather tasks\n    tasks = []\n    for task_inp in tasks_input:\n        task = asyncio.create_task(semaphore_helper(\n            task_inp,\n            max_new_tokens=max_new_tokens,\n            temperature=temperature,\n            **kwrs\n        ))\n        tasks.append(task)\n\n    results_raw = await asyncio.gather(*tasks)\n\n    # Sort results back into original order using the index stored\n    results_raw.sort(key=lambda x: x[\"original_index\"])\n\n    # Restructure the results\n    output: List[FrameExtractionUnitResult] = []\n    messages_log: Optional[List[List[Dict[str, str]]]] = [] if return_messages_log else None\n\n    for result_data in results_raw:\n        unit = result_data[\"unit\"]\n        gen_text = result_data[\"gen_text\"]\n\n        # Create result object\n        result = FrameExtractionUnitResult(\n            start=unit.start,\n            end=unit.end,\n            text=unit.text,\n            gen_text=gen_text\n        )\n        output.append(result)\n\n        # Append to messages log if requested\n        if return_messages_log:\n            final_messages = result_data[\"messages\"] + [{\"role\": \"assistant\", \"content\": gen_text}]\n            messages_log.append(final_messages)\n\n    if return_messages_log:\n        return output, messages_log\n    else:\n        return output\n</code></pre>"},{"location":"api/extractors/#llm_ie.extractors.DirectFrameExtractor.extract_frames","title":"extract_frames","text":"<pre><code>extract_frames(\n    text_content: Union[str, Dict[str, str]],\n    max_new_tokens: int = 512,\n    document_key: str = None,\n    temperature: float = 0.0,\n    verbose: bool = False,\n    concurrent: bool = False,\n    concurrent_batch_size: int = 32,\n    case_sensitive: bool = False,\n    fuzzy_match: bool = True,\n    fuzzy_buffer_size: float = 0.2,\n    fuzzy_score_cutoff: float = 0.8,\n    allow_overlap_entities: bool = False,\n    return_messages_log: bool = False,\n    **kwrs\n) -&gt; List[LLMInformationExtractionFrame]\n</code></pre> <p>This method inputs a text and outputs a list of LLMInformationExtractionFrame It use the extract() method and post-process outputs into frames.</p> Parameters: <p>text_content : Union[str, Dict[str,str]]     the input text content to put in prompt template.      If str, the prompt template must has only 1 placeholder {{}}, regardless of placeholder name.     If dict, all the keys must be included in the prompt template placeholder {{}}. max_new_tokens : str, Optional     the max number of new tokens LLM should generate.  document_key : str, Optional     specify the key in text_content where document text is.      If text_content is str, this parameter will be ignored. temperature : float, Optional     the temperature for token sampling. verbose : bool, Optional     if True, LLM generated text will be printed in terminal in real-time.  concurrent : bool, Optional     if True, the sentences will be extracted in concurrent. concurrent_batch_size : int, Optional     the number of sentences to process in concurrent. Only used when <code>concurrent</code> is True. case_sensitive : bool, Optional     if True, entity text matching will be case-sensitive. fuzzy_match : bool, Optional     if True, fuzzy matching will be applied to find entity text. fuzzy_buffer_size : float, Optional     the buffer size for fuzzy matching. Default is 20% of entity text length. fuzzy_score_cutoff : float, Optional     the Jaccard score cutoff for fuzzy matching.      Matched entity text must have a score higher than this value or a None will be returned. allow_overlap_entities : bool, Optional     if True, entities can overlap in the text.      Note that this can cause multiple frames to be generated on the same entity span if they have same entity text. return_messages_log : bool, Optional     if True, a list of messages will be returned. <p>Return : str     a list of frames.</p> Source code in <code>package/llm-ie/src/llm_ie/extractors.py</code> <pre><code>def extract_frames(self, text_content:Union[str, Dict[str,str]], max_new_tokens:int=512, \n                    document_key:str=None, temperature:float=0.0, verbose:bool=False, \n                    concurrent:bool=False, concurrent_batch_size:int=32,\n                    case_sensitive:bool=False, fuzzy_match:bool=True, fuzzy_buffer_size:float=0.2, fuzzy_score_cutoff:float=0.8,\n                    allow_overlap_entities:bool=False, return_messages_log:bool=False, **kwrs) -&gt; List[LLMInformationExtractionFrame]:\n    \"\"\"\n    This method inputs a text and outputs a list of LLMInformationExtractionFrame\n    It use the extract() method and post-process outputs into frames.\n\n    Parameters:\n    ----------\n    text_content : Union[str, Dict[str,str]]\n        the input text content to put in prompt template. \n        If str, the prompt template must has only 1 placeholder {{&lt;placeholder name&gt;}}, regardless of placeholder name.\n        If dict, all the keys must be included in the prompt template placeholder {{&lt;placeholder name&gt;}}.\n    max_new_tokens : str, Optional\n        the max number of new tokens LLM should generate. \n    document_key : str, Optional\n        specify the key in text_content where document text is. \n        If text_content is str, this parameter will be ignored.\n    temperature : float, Optional\n        the temperature for token sampling.\n    verbose : bool, Optional\n        if True, LLM generated text will be printed in terminal in real-time. \n    concurrent : bool, Optional\n        if True, the sentences will be extracted in concurrent.\n    concurrent_batch_size : int, Optional\n        the number of sentences to process in concurrent. Only used when `concurrent` is True.\n    case_sensitive : bool, Optional\n        if True, entity text matching will be case-sensitive.\n    fuzzy_match : bool, Optional\n        if True, fuzzy matching will be applied to find entity text.\n    fuzzy_buffer_size : float, Optional\n        the buffer size for fuzzy matching. Default is 20% of entity text length.\n    fuzzy_score_cutoff : float, Optional\n        the Jaccard score cutoff for fuzzy matching. \n        Matched entity text must have a score higher than this value or a None will be returned.\n    allow_overlap_entities : bool, Optional\n        if True, entities can overlap in the text. \n        Note that this can cause multiple frames to be generated on the same entity span if they have same entity text.\n    return_messages_log : bool, Optional\n        if True, a list of messages will be returned.\n\n    Return : str\n        a list of frames.\n    \"\"\"\n    ENTITY_KEY = \"entity_text\"\n    if concurrent:\n        if verbose:\n            warnings.warn(\"verbose=True is not supported in concurrent mode.\", RuntimeWarning)\n\n        nest_asyncio.apply() # For Jupyter notebook. Terminal does not need this.\n        extraction_results = asyncio.run(self.extract_async(text_content=text_content, \n                                            max_new_tokens=max_new_tokens, \n                                            document_key=document_key,\n                                            temperature=temperature, \n                                            concurrent_batch_size=concurrent_batch_size,\n                                            return_messages_log=return_messages_log,\n                                            **kwrs)\n                                        )\n    else:\n        extraction_results = self.extract(text_content=text_content, \n                                            max_new_tokens=max_new_tokens, \n                                            document_key=document_key,\n                                            temperature=temperature, \n                                            verbose=verbose,\n                                            return_messages_log=return_messages_log,\n                                            **kwrs)\n\n    llm_output_results, messages_log = extraction_results if return_messages_log else (extraction_results, None)\n\n    frame_list = []\n    for res in llm_output_results:\n        entity_json = []\n        for entity in self._extract_json(gen_text=res.gen_text):\n            if ENTITY_KEY in entity:\n                entity_json.append(entity)\n            else:\n                warnings.warn(f'Extractor output \"{entity}\" does not have entity_key (\"{ENTITY_KEY}\"). This frame will be dropped.', RuntimeWarning)\n\n        spans = self._find_entity_spans(text=res.text, \n                                        entities=[e[ENTITY_KEY] for e in entity_json], \n                                        case_sensitive=case_sensitive,\n                                        fuzzy_match=fuzzy_match,\n                                        fuzzy_buffer_size=fuzzy_buffer_size,\n                                        fuzzy_score_cutoff=fuzzy_score_cutoff,\n                                        allow_overlap_entities=allow_overlap_entities)\n        for ent, span in zip(entity_json, spans):\n            if span is not None:\n                start, end = span\n                entity_text = res.text[start:end]\n                start += res.start\n                end += res.start\n                attr = {}\n                if \"attr\" in ent and ent[\"attr\"] is not None:\n                    attr = ent[\"attr\"]\n\n                frame = LLMInformationExtractionFrame(frame_id=f\"{len(frame_list)}\", \n                            start=start,\n                            end=end,\n                            entity_text=entity_text,\n                            attr=attr)\n                frame_list.append(frame)\n\n    if return_messages_log:\n        return frame_list, messages_log\n    return frame_list\n</code></pre>"},{"location":"api/extractors/#llm_ie.extractors.ReviewFrameExtractor","title":"llm_ie.extractors.ReviewFrameExtractor","text":"<pre><code>ReviewFrameExtractor(\n    unit_chunker: UnitChunker,\n    context_chunker: ContextChunker,\n    inference_engine: InferenceEngine,\n    prompt_template: str,\n    review_mode: str,\n    review_prompt: str = None,\n    system_prompt: str = None,\n    **kwrs\n)\n</code></pre> <p>               Bases: <code>DirectFrameExtractor</code></p> <p>This class add a review step after the DirectFrameExtractor. The Review process asks LLM to review its output and:     1. add more frames while keep current. This is efficient for boosting recall.      2. or, regenerate frames (add new and delete existing).  Use the review_mode parameter to specify. Note that the review_prompt should instruct LLM accordingly.</p> Parameters: <p>unit_chunker : UnitChunker     the unit chunker object that determines how to chunk the document text into units. context_chunker : ContextChunker     the context chunker object that determines how to get context for each unit. inference_engine : InferenceEngine     the LLM inferencing engine object. Must implements the chat() method. prompt_template : str     prompt template with \"{{}}\" placeholder. review_prompt : str: Optional     the prompt text that ask LLM to review. Specify addition or revision in the instruction.     if not provided, a default review prompt will be used.  review_mode : str     review mode. Must be one of {\"addition\", \"revision\"}     addition mode only ask LLM to add new frames, while revision mode ask LLM to regenerate. system_prompt : str, Optional     system prompt. Source code in <code>package/llm-ie/src/llm_ie/extractors.py</code> <pre><code>def __init__(self, unit_chunker:UnitChunker, context_chunker:ContextChunker, \n             inference_engine:InferenceEngine, prompt_template:str, review_mode:str, review_prompt:str=None, system_prompt:str=None, **kwrs):\n    \"\"\"\n    This class add a review step after the DirectFrameExtractor.\n    The Review process asks LLM to review its output and:\n        1. add more frames while keep current. This is efficient for boosting recall. \n        2. or, regenerate frames (add new and delete existing). \n    Use the review_mode parameter to specify. Note that the review_prompt should instruct LLM accordingly.\n\n    Parameters:\n    ----------\n    unit_chunker : UnitChunker\n        the unit chunker object that determines how to chunk the document text into units.\n    context_chunker : ContextChunker\n        the context chunker object that determines how to get context for each unit.\n    inference_engine : InferenceEngine\n        the LLM inferencing engine object. Must implements the chat() method.\n    prompt_template : str\n        prompt template with \"{{&lt;placeholder name&gt;}}\" placeholder.\n    review_prompt : str: Optional\n        the prompt text that ask LLM to review. Specify addition or revision in the instruction.\n        if not provided, a default review prompt will be used. \n    review_mode : str\n        review mode. Must be one of {\"addition\", \"revision\"}\n        addition mode only ask LLM to add new frames, while revision mode ask LLM to regenerate.\n    system_prompt : str, Optional\n        system prompt.\n    \"\"\"\n    super().__init__(inference_engine=inference_engine, \n                     unit_chunker=unit_chunker, \n                     prompt_template=prompt_template, \n                     system_prompt=system_prompt, \n                     context_chunker=context_chunker,\n                     **kwrs)\n    # check review mode\n    if review_mode not in {\"addition\", \"revision\"}: \n        raise ValueError('review_mode must be one of {\"addition\", \"revision\"}.')\n    self.review_mode = review_mode\n    # assign review prompt\n    if review_prompt:\n        self.review_prompt = review_prompt\n    else:\n        self.review_prompt = None\n        original_class_name = self.__class__.__name__\n\n        current_class_name = original_class_name\n        for current_class_in_mro in self.__class__.__mro__:\n            if current_class_in_mro is object: \n                continue\n\n            current_class_name = current_class_in_mro.__name__\n            try:\n                file_path = importlib.resources.files('llm_ie.asset.default_prompts').\\\n                    joinpath(f\"{self.__class__.__name__}_{self.review_mode}_review_prompt.txt\")\n                with open(file_path, 'r', encoding=\"utf-8\") as f:\n                    self.review_prompt = f.read()\n            except FileNotFoundError:\n                pass\n\n            except Exception as e:\n                warnings.warn(\n                    f\"Error attempting to read default review prompt for '{current_class_name}' \"\n                    f\"from '{str(file_path)}': {e}. Trying next in MRO.\",\n                    UserWarning\n                )\n                continue \n\n    if self.review_prompt is None:\n        raise ValueError(f\"Cannot find review prompt for {self.__class__.__name__} in the package. Please provide a review_prompt.\")\n</code></pre>"},{"location":"api/extractors/#llm_ie.extractors.ReviewFrameExtractor.extract","title":"extract","text":"<pre><code>extract(\n    text_content: Union[str, Dict[str, str]],\n    max_new_tokens: int = 2048,\n    document_key: str = None,\n    temperature: float = 0.0,\n    verbose: bool = False,\n    return_messages_log: bool = False,\n    **kwrs\n) -&gt; List[FrameExtractionUnitResult]\n</code></pre> <p>This method inputs a text and outputs a list of outputs per unit.</p> Parameters: <p>text_content : Union[str, Dict[str,str]]     the input text content to put in prompt template.      If str, the prompt template must has only 1 placeholder {{}}, regardless of placeholder name.     If dict, all the keys must be included in the prompt template placeholder {{}}. max_new_tokens : int, Optional     the max number of new tokens LLM should generate.  document_key : str, Optional     specify the key in text_content where document text is.      If text_content is str, this parameter will be ignored. temperature : float, Optional     the temperature for token sampling. verbose : bool, Optional     if True, LLM generated text will be printed in terminal in real-time.  return_messages_log : bool, Optional     if True, a list of messages will be returned. <p>Return : List[FrameExtractionUnitResult]     the output from LLM for each unit. Contains the start, end, text, and generated text.</p> Source code in <code>package/llm-ie/src/llm_ie/extractors.py</code> <pre><code>def extract(self, text_content:Union[str, Dict[str,str]], max_new_tokens:int=2048, document_key:str=None, \n            temperature:float=0.0, verbose:bool=False, return_messages_log:bool=False, **kwrs) -&gt; List[FrameExtractionUnitResult]:\n    \"\"\"\n    This method inputs a text and outputs a list of outputs per unit.\n\n    Parameters:\n    ----------\n    text_content : Union[str, Dict[str,str]]\n        the input text content to put in prompt template. \n        If str, the prompt template must has only 1 placeholder {{&lt;placeholder name&gt;}}, regardless of placeholder name.\n        If dict, all the keys must be included in the prompt template placeholder {{&lt;placeholder name&gt;}}.\n    max_new_tokens : int, Optional\n        the max number of new tokens LLM should generate. \n    document_key : str, Optional\n        specify the key in text_content where document text is. \n        If text_content is str, this parameter will be ignored.\n    temperature : float, Optional\n        the temperature for token sampling.\n    verbose : bool, Optional\n        if True, LLM generated text will be printed in terminal in real-time. \n    return_messages_log : bool, Optional\n        if True, a list of messages will be returned.\n\n    Return : List[FrameExtractionUnitResult]\n        the output from LLM for each unit. Contains the start, end, text, and generated text.\n    \"\"\"\n    # define output\n    output = []\n    # unit chunking\n    if isinstance(text_content, str):\n        doc_text = text_content\n\n    elif isinstance(text_content, dict):\n        if document_key is None:\n            raise ValueError(\"document_key must be provided when text_content is dict.\")\n        doc_text = text_content[document_key]\n\n    units = self.unit_chunker.chunk(doc_text)\n    # context chunker init\n    self.context_chunker.fit(doc_text, units)\n    # messages log\n    if return_messages_log:\n        messages_log = []\n\n    # generate unit by unit\n    for i, unit in enumerate(units):\n        # &lt;--- Initial generation step ---&gt;\n        # construct chat messages\n        messages = []\n        if self.system_prompt:\n            messages.append({'role': 'system', 'content': self.system_prompt})\n\n        context = self.context_chunker.chunk(unit)\n\n        if context == \"\":\n            # no context, just place unit in user prompt\n            if isinstance(text_content, str):\n                messages.append({'role': 'user', 'content': self._get_user_prompt(unit.text)})\n            else:\n                unit_content = text_content.copy()\n                unit_content[document_key] = unit.text\n                messages.append({'role': 'user', 'content': self._get_user_prompt(unit_content)})\n        else:\n            # insert context to user prompt\n            if isinstance(text_content, str):\n                messages.append({'role': 'user', 'content': self._get_user_prompt(context)})\n            else:\n                context_content = text_content.copy()\n                context_content[document_key] = context\n                messages.append({'role': 'user', 'content': self._get_user_prompt(context_content)})\n            # simulate conversation where assistant confirms\n            messages.append({'role': 'assistant', 'content': 'Sure, please provide the unit text (e.g., sentence, line, chunk) of interest.'})\n            # place unit of interest\n            messages.append({'role': 'user', 'content': unit.text})\n\n        if verbose:\n            print(f\"\\n\\n{Fore.GREEN}Unit {i}:{Style.RESET_ALL}\\n{unit.text}\\n\")\n            if context != \"\":\n                print(f\"{Fore.YELLOW}Context:{Style.RESET_ALL}\\n{context}\\n\")\n\n            print(f\"{Fore.BLUE}Extraction:{Style.RESET_ALL}\")\n\n            response_stream = self.inference_engine.chat(\n                            messages=messages, \n                            max_new_tokens=max_new_tokens, \n                            temperature=temperature,\n                            stream=True,\n                            **kwrs\n                        )\n\n            initial = \"\"\n            for chunk in response_stream:\n                initial += chunk\n                print(chunk, end='', flush=True)\n\n        else:\n            initial = self.inference_engine.chat(\n                            messages=messages, \n                            max_new_tokens=max_new_tokens, \n                            temperature=temperature,\n                            stream=False,\n                            **kwrs\n                        )\n\n        if return_messages_log:\n            messages.append({\"role\": \"assistant\", \"content\": initial})\n            messages_log.append(messages)\n\n        # &lt;--- Review step ---&gt;\n        if verbose:\n            print(f\"\\n{Fore.YELLOW}Review:{Style.RESET_ALL}\")\n\n        messages.append({'role': 'assistant', 'content': initial})\n        messages.append({'role': 'user', 'content': self.review_prompt})\n\n        if verbose:\n            response_stream = self.inference_engine.chat(\n                            messages=messages, \n                            max_new_tokens=max_new_tokens, \n                            temperature=temperature,\n                            stream=True,\n                            **kwrs\n                        )\n\n            review = \"\"\n            for chunk in response_stream:\n                review += chunk\n                print(chunk, end='', flush=True)\n\n        else:\n            review = self.inference_engine.chat(\n                            messages=messages, \n                            max_new_tokens=max_new_tokens, \n                            temperature=temperature,\n                            stream=False,\n                            **kwrs\n                        )\n\n        # Output\n        if self.review_mode == \"revision\":\n            gen_text = review\n        elif self.review_mode == \"addition\":\n            gen_text = initial + '\\n' + review\n\n        if return_messages_log:\n            messages.append({\"role\": \"assistant\", \"content\": review})\n            messages_log.append(messages)\n\n        # add to output\n        result = FrameExtractionUnitResult(\n                        start=unit.start,\n                        end=unit.end,\n                        text=unit.text,\n                        gen_text=gen_text)\n        output.append(result)\n\n    if return_messages_log:\n        return output, messages_log\n\n    return output\n</code></pre>"},{"location":"api/extractors/#llm_ie.extractors.ReviewFrameExtractor.stream","title":"stream","text":"<pre><code>stream(\n    text_content: Union[str, Dict[str, str]],\n    max_new_tokens: int = 2048,\n    document_key: str = None,\n    temperature: float = 0.0,\n    **kwrs\n) -&gt; Generator[str, None, None]\n</code></pre> <p>This method inputs a text and outputs a list of outputs per unit.</p> Parameters: <p>text_content : Union[str, Dict[str,str]]     the input text content to put in prompt template.      If str, the prompt template must has only 1 placeholder {{}}, regardless of placeholder name.     If dict, all the keys must be included in the prompt template placeholder {{}}. max_new_tokens : int, Optional     the max number of new tokens LLM should generate.  document_key : str, Optional     specify the key in text_content where document text is.      If text_content is str, this parameter will be ignored. temperature : float, Optional     the temperature for token sampling. <p>Return : List[FrameExtractionUnitResult]     the output from LLM for each unit. Contains the start, end, text, and generated text.</p> Source code in <code>package/llm-ie/src/llm_ie/extractors.py</code> <pre><code>def stream(self, text_content:Union[str, Dict[str,str]], max_new_tokens:int=2048, \n           document_key:str=None, temperature:float=0.0, **kwrs) -&gt; Generator[str, None, None]:\n    \"\"\"\n    This method inputs a text and outputs a list of outputs per unit.\n\n    Parameters:\n    ----------\n    text_content : Union[str, Dict[str,str]]\n        the input text content to put in prompt template. \n        If str, the prompt template must has only 1 placeholder {{&lt;placeholder name&gt;}}, regardless of placeholder name.\n        If dict, all the keys must be included in the prompt template placeholder {{&lt;placeholder name&gt;}}.\n    max_new_tokens : int, Optional\n        the max number of new tokens LLM should generate. \n    document_key : str, Optional\n        specify the key in text_content where document text is. \n        If text_content is str, this parameter will be ignored.\n    temperature : float, Optional\n        the temperature for token sampling.\n\n    Return : List[FrameExtractionUnitResult]\n        the output from LLM for each unit. Contains the start, end, text, and generated text.\n    \"\"\"\n    # unit chunking\n    if isinstance(text_content, str):\n        doc_text = text_content\n\n    elif isinstance(text_content, dict):\n        if document_key is None:\n            raise ValueError(\"document_key must be provided when text_content is dict.\")\n        doc_text = text_content[document_key]\n\n    units = self.unit_chunker.chunk(doc_text)\n    # context chunker init\n    self.context_chunker.fit(doc_text, units)\n\n    # generate unit by unit\n    for i, unit in enumerate(units):\n        # &lt;--- Initial generation step ---&gt;\n        # construct chat messages\n        messages = []\n        if self.system_prompt:\n            messages.append({'role': 'system', 'content': self.system_prompt})\n\n        context = self.context_chunker.chunk(unit)\n\n        if context == \"\":\n            # no context, just place unit in user prompt\n            if isinstance(text_content, str):\n                messages.append({'role': 'user', 'content': self._get_user_prompt(unit.text)})\n            else:\n                unit_content = text_content.copy()\n                unit_content[document_key] = unit.text\n                messages.append({'role': 'user', 'content': self._get_user_prompt(unit_content)})\n        else:\n            # insert context to user prompt\n            if isinstance(text_content, str):\n                messages.append({'role': 'user', 'content': self._get_user_prompt(context)})\n            else:\n                context_content = text_content.copy()\n                context_content[document_key] = context\n                messages.append({'role': 'user', 'content': self._get_user_prompt(context_content)})\n            # simulate conversation where assistant confirms\n            messages.append({'role': 'assistant', 'content': 'Sure, please provide the unit text (e.g., sentence, line, chunk) of interest.'})\n            # place unit of interest\n            messages.append({'role': 'user', 'content': unit.text})\n\n\n        yield f\"\\n\\n{Fore.GREEN}Unit {i}:{Style.RESET_ALL}\\n{unit.text}\\n\"\n        if context != \"\":\n            yield f\"{Fore.YELLOW}Context:{Style.RESET_ALL}\\n{context}\\n\"\n\n        yield f\"{Fore.BLUE}Extraction:{Style.RESET_ALL}\\n\"\n\n        response_stream = self.inference_engine.chat(\n                        messages=messages, \n                        max_new_tokens=max_new_tokens, \n                        temperature=temperature,\n                        stream=True,\n                        **kwrs\n                    )\n\n        initial = \"\"\n        for chunk in response_stream:\n            initial += chunk\n            yield chunk\n\n        # &lt;--- Review step ---&gt;\n        yield f\"\\n{Fore.YELLOW}Review:{Style.RESET_ALL}\"\n\n        messages.append({'role': 'assistant', 'content': initial})\n        messages.append({'role': 'user', 'content': self.review_prompt})\n\n        response_stream = self.inference_engine.chat(\n                        messages=messages, \n                        max_new_tokens=max_new_tokens, \n                        temperature=temperature,\n                        stream=True,\n                        **kwrs\n                    )\n\n        for chunk in response_stream:\n            yield chunk\n</code></pre>"},{"location":"api/extractors/#llm_ie.extractors.ReviewFrameExtractor.extract_async","title":"extract_async  <code>async</code>","text":"<pre><code>extract_async(\n    text_content: Union[str, Dict[str, str]],\n    max_new_tokens: int = 2048,\n    document_key: str = None,\n    temperature: float = 0.0,\n    concurrent_batch_size: int = 32,\n    return_messages_log: bool = False,\n    **kwrs\n) -&gt; List[FrameExtractionUnitResult]\n</code></pre> <p>This is the asynchronous version of the extract() method with the review step.</p> Parameters: <p>text_content : Union[str, Dict[str,str]]     the input text content to put in prompt template.     If str, the prompt template must has only 1 placeholder {{}}, regardless of placeholder name.     If dict, all the keys must be included in the prompt template placeholder {{}}. max_new_tokens : int, Optional     the max number of new tokens LLM should generate. document_key : str, Optional     specify the key in text_content where document text is.     If text_content is str, this parameter will be ignored. temperature : float, Optional     the temperature for token sampling. concurrent_batch_size : int, Optional     the batch size for concurrent processing. return_messages_log : bool, Optional     if True, a list of messages will be returned, including review steps. <p>Return : List[FrameExtractionUnitResult]     the output from LLM for each unit after review. Contains the start, end, text, and generated text.</p> Source code in <code>package/llm-ie/src/llm_ie/extractors.py</code> <pre><code>async def extract_async(self, text_content:Union[str, Dict[str,str]], max_new_tokens:int=2048, document_key:str=None, temperature:float=0.0,\n                        concurrent_batch_size:int=32, return_messages_log:bool=False, **kwrs) -&gt; List[FrameExtractionUnitResult]:\n    \"\"\"\n    This is the asynchronous version of the extract() method with the review step.\n\n    Parameters:\n    ----------\n    text_content : Union[str, Dict[str,str]]\n        the input text content to put in prompt template.\n        If str, the prompt template must has only 1 placeholder {{&lt;placeholder name&gt;}}, regardless of placeholder name.\n        If dict, all the keys must be included in the prompt template placeholder {{&lt;placeholder name&gt;}}.\n    max_new_tokens : int, Optional\n        the max number of new tokens LLM should generate.\n    document_key : str, Optional\n        specify the key in text_content where document text is.\n        If text_content is str, this parameter will be ignored.\n    temperature : float, Optional\n        the temperature for token sampling.\n    concurrent_batch_size : int, Optional\n        the batch size for concurrent processing.\n    return_messages_log : bool, Optional\n        if True, a list of messages will be returned, including review steps.\n\n    Return : List[FrameExtractionUnitResult]\n        the output from LLM for each unit after review. Contains the start, end, text, and generated text.\n    \"\"\"\n    if isinstance(text_content, str):\n        doc_text = text_content\n    elif isinstance(text_content, dict):\n        if document_key is None:\n            raise ValueError(\"document_key must be provided when text_content is dict.\")\n        if document_key not in text_content:\n             raise ValueError(f\"document_key '{document_key}' not found in text_content dictionary.\")\n        doc_text = text_content[document_key]\n    else:\n        raise TypeError(\"text_content must be a string or a dictionary.\")\n\n    units = self.unit_chunker.chunk(doc_text)\n\n    # context chunker init\n    self.context_chunker.fit(doc_text, units)\n\n    # &lt;--- Initial generation step ---&gt;\n    initial_tasks_input = []\n    for i, unit in enumerate(units):\n        # construct chat messages for initial generation\n        messages = []\n        if self.system_prompt:\n            messages.append({'role': 'system', 'content': self.system_prompt})\n\n        context = self.context_chunker.chunk(unit)\n\n        if context == \"\":\n             # no context, just place unit in user prompt\n            if isinstance(text_content, str):\n                messages.append({'role': 'user', 'content': self._get_user_prompt(unit.text)})\n            else:\n                unit_content = text_content.copy()\n                unit_content[document_key] = unit.text\n                messages.append({'role': 'user', 'content': self._get_user_prompt(unit_content)})\n        else:\n            # insert context to user prompt\n            if isinstance(text_content, str):\n                messages.append({'role': 'user', 'content': self._get_user_prompt(context)})\n            else:\n                context_content = text_content.copy()\n                context_content[document_key] = context\n                messages.append({'role': 'user', 'content': self._get_user_prompt(context_content)})\n            # simulate conversation where assistant confirms\n            messages.append({'role': 'assistant', 'content': 'Sure, please provide the unit text (e.g., sentence, line, chunk) of interest.'})\n            # place unit of interest\n            messages.append({'role': 'user', 'content': unit.text})\n\n        # Store unit and messages together for the initial task\n        initial_tasks_input.append({\"unit\": unit, \"messages\": messages, \"original_index\": i})\n\n    semaphore = asyncio.Semaphore(concurrent_batch_size)\n\n    async def initial_semaphore_helper(task_data: Dict, max_new_tokens: int, temperature: float, **kwrs):\n        unit = task_data[\"unit\"]\n        messages = task_data[\"messages\"]\n        original_index = task_data[\"original_index\"]\n\n        async with semaphore:\n            gen_text = await self.inference_engine.chat_async(\n                messages=messages,\n                max_new_tokens=max_new_tokens,\n                temperature=temperature,\n                **kwrs\n            )\n        # Return initial generation result along with the messages used and the unit\n        return {\"original_index\": original_index, \"unit\": unit, \"initial_gen_text\": gen_text, \"initial_messages\": messages}\n\n    # Create and gather initial generation tasks\n    initial_tasks = [\n        asyncio.create_task(initial_semaphore_helper(\n            task_inp,\n            max_new_tokens=max_new_tokens,\n            temperature=temperature,\n            **kwrs\n        ))\n        for task_inp in initial_tasks_input\n    ]\n\n    initial_results_raw = await asyncio.gather(*initial_tasks)\n\n    # Sort initial results back into original order\n    initial_results_raw.sort(key=lambda x: x[\"original_index\"])\n\n    # &lt;--- Review step ---&gt;\n    review_tasks_input = []\n    for result_data in initial_results_raw:\n        # Prepare messages for the review step\n        initial_messages = result_data[\"initial_messages\"]\n        initial_gen_text = result_data[\"initial_gen_text\"]\n        review_messages = initial_messages + [\n            {'role': 'assistant', 'content': initial_gen_text},\n            {'role': 'user', 'content': self.review_prompt}\n        ]\n        # Store data needed for review task\n        review_tasks_input.append({\n            \"unit\": result_data[\"unit\"],\n            \"initial_gen_text\": initial_gen_text,\n            \"messages\": review_messages, \n            \"original_index\": result_data[\"original_index\"],\n            \"full_initial_log\": initial_messages + [{'role': 'assistant', 'content': initial_gen_text}] if return_messages_log else None # Log up to initial generation\n        })\n\n\n    async def review_semaphore_helper(task_data: Dict, max_new_tokens: int, temperature: float, **kwrs):\n        messages = task_data[\"messages\"] \n        original_index = task_data[\"original_index\"]\n\n        async with semaphore:\n            review_gen_text = await self.inference_engine.chat_async(\n                messages=messages,\n                max_new_tokens=max_new_tokens,\n                temperature=temperature,\n                **kwrs\n            )\n        # Combine initial and review results\n        task_data[\"review_gen_text\"] = review_gen_text\n        if return_messages_log:\n            # Log for the review call itself\n             task_data[\"full_review_log\"] = messages + [{'role': 'assistant', 'content': review_gen_text}]\n        return task_data # Return the augmented dictionary\n\n    # Create and gather review tasks\n    review_tasks = [\n         asyncio.create_task(review_semaphore_helper(\n            task_inp,\n            max_new_tokens=max_new_tokens,\n            temperature=temperature,\n            **kwrs\n        ))\n       for task_inp in review_tasks_input\n    ]\n\n    final_results_raw = await asyncio.gather(*review_tasks)\n\n    # Sort final results back into original order (although gather might preserve order for tasks added sequentially)\n    final_results_raw.sort(key=lambda x: x[\"original_index\"])\n\n    # &lt;--- Process final results ---&gt;\n    output: List[FrameExtractionUnitResult] = []\n    messages_log: Optional[List[List[Dict[str, str]]]] = [] if return_messages_log else None\n\n    for result_data in final_results_raw:\n        unit = result_data[\"unit\"]\n        initial_gen = result_data[\"initial_gen_text\"]\n        review_gen = result_data[\"review_gen_text\"]\n\n        # Combine based on review mode\n        if self.review_mode == \"revision\":\n            final_gen_text = review_gen\n        elif self.review_mode == \"addition\":\n            final_gen_text = initial_gen + '\\n' + review_gen\n        else: # Should not happen due to init check\n            final_gen_text = review_gen # Default to revision if mode is somehow invalid\n\n        # Create final result object\n        result = FrameExtractionUnitResult(\n            start=unit.start,\n            end=unit.end,\n            text=unit.text,\n            gen_text=final_gen_text # Use the combined/reviewed text\n        )\n        output.append(result)\n\n        # Append full conversation log if requested\n        if return_messages_log:\n            full_log_for_unit = result_data.get(\"full_initial_log\", []) + [{'role': 'user', 'content': self.review_prompt}] + [{'role': 'assistant', 'content': review_gen}]\n            messages_log.append(full_log_for_unit)\n\n    if return_messages_log:\n        return output, messages_log\n    else:\n        return output\n</code></pre>"},{"location":"api/extractors/#convenience-frame-extractors","title":"Convenience Frame Extractors","text":""},{"location":"api/extractors/#llm_ie.extractors.BasicFrameExtractor","title":"llm_ie.extractors.BasicFrameExtractor","text":"<pre><code>BasicFrameExtractor(\n    inference_engine: InferenceEngine,\n    prompt_template: str,\n    system_prompt: str = None,\n    **kwrs\n)\n</code></pre> <p>               Bases: <code>DirectFrameExtractor</code></p> <p>This class diretly prompt LLM for frame extraction. Input system prompt (optional), prompt template (with instruction, few-shot examples),  and specify a LLM.</p> Parameters: <p>inference_engine : InferenceEngine     the LLM inferencing engine object. Must implements the chat() method. prompt_template : str     prompt template with \"{{}}\" placeholder. system_prompt : str, Optional     system prompt. Source code in <code>package/llm-ie/src/llm_ie/extractors.py</code> <pre><code>def __init__(self, inference_engine:InferenceEngine, prompt_template:str, system_prompt:str=None, **kwrs):\n    \"\"\"\n    This class diretly prompt LLM for frame extraction.\n    Input system prompt (optional), prompt template (with instruction, few-shot examples), \n    and specify a LLM.\n\n    Parameters:\n    ----------\n    inference_engine : InferenceEngine\n        the LLM inferencing engine object. Must implements the chat() method.\n    prompt_template : str\n        prompt template with \"{{&lt;placeholder name&gt;}}\" placeholder.\n    system_prompt : str, Optional\n        system prompt.\n    \"\"\"\n    super().__init__(inference_engine=inference_engine, \n                     unit_chunker=WholeDocumentUnitChunker(),\n                     prompt_template=prompt_template, \n                     system_prompt=system_prompt, \n                     context_chunker=NoContextChunker(),\n                     **kwrs)\n</code></pre>"},{"location":"api/extractors/#llm_ie.extractors.SentenceFrameExtractor","title":"llm_ie.extractors.SentenceFrameExtractor","text":"<pre><code>SentenceFrameExtractor(\n    inference_engine: InferenceEngine,\n    prompt_template: str,\n    system_prompt: str = None,\n    context_sentences: Union[str, int] = \"all\",\n    **kwrs\n)\n</code></pre> <p>               Bases: <code>DirectFrameExtractor</code></p> <p>This class performs sentence-by-sentence information extraction. The process is as follows:     1. system prompt (optional)     2. user prompt with instructions (schema, background, full text, few-shot example...)     3. feed a sentence (start with first sentence)     4. LLM extract entities and attributes from the sentence     5. iterate to the next sentence and repeat steps 3-4 until all sentences are processed.</p> <p>Input system prompt (optional), prompt template (with user instructions),  and specify a LLM.</p> Parameters: <p>inference_engine : InferenceEngine     the LLM inferencing engine object. Must implements the chat() method. prompt_template : str     prompt template with \"{{}}\" placeholder. system_prompt : str, Optional     system prompt. context_sentences : Union[str, int], Optional     number of sentences before and after the given sentence to provide additional context.      if \"all\", the full text will be provided in the prompt as context.      if 0, no additional context will be provided.         This is good for tasks that does not require context beyond the given sentence.      if &gt; 0, the number of sentences before and after the given sentence to provide as context.         This is good for tasks that require context beyond the given sentence. Source code in <code>package/llm-ie/src/llm_ie/extractors.py</code> <pre><code>def __init__(self, inference_engine:InferenceEngine, prompt_template:str, system_prompt:str=None,\n             context_sentences:Union[str, int]=\"all\", **kwrs):\n    \"\"\"\n    This class performs sentence-by-sentence information extraction.\n    The process is as follows:\n        1. system prompt (optional)\n        2. user prompt with instructions (schema, background, full text, few-shot example...)\n        3. feed a sentence (start with first sentence)\n        4. LLM extract entities and attributes from the sentence\n        5. iterate to the next sentence and repeat steps 3-4 until all sentences are processed.\n\n    Input system prompt (optional), prompt template (with user instructions), \n    and specify a LLM.\n\n    Parameters:\n    ----------\n    inference_engine : InferenceEngine\n        the LLM inferencing engine object. Must implements the chat() method.\n    prompt_template : str\n        prompt template with \"{{&lt;placeholder name&gt;}}\" placeholder.\n    system_prompt : str, Optional\n        system prompt.\n    context_sentences : Union[str, int], Optional\n        number of sentences before and after the given sentence to provide additional context. \n        if \"all\", the full text will be provided in the prompt as context. \n        if 0, no additional context will be provided.\n            This is good for tasks that does not require context beyond the given sentence. \n        if &gt; 0, the number of sentences before and after the given sentence to provide as context.\n            This is good for tasks that require context beyond the given sentence. \n    \"\"\"\n    if not isinstance(context_sentences, int) and context_sentences != \"all\":\n        raise ValueError('context_sentences must be an integer (&gt;= 0) or \"all\".')\n\n    if isinstance(context_sentences, int) and context_sentences &lt; 0:\n        raise ValueError(\"context_sentences must be a positive integer.\")\n\n    if isinstance(context_sentences, int):\n        context_chunker = SlideWindowContextChunker(window_size=context_sentences)\n    elif context_sentences == \"all\":\n        context_chunker = WholeDocumentContextChunker()\n\n    super().__init__(inference_engine=inference_engine, \n                     unit_chunker=SentenceUnitChunker(),\n                     prompt_template=prompt_template, \n                     system_prompt=system_prompt, \n                     context_chunker=context_chunker,\n                     **kwrs)\n</code></pre>"},{"location":"api/extractors/#llm_ie.extractors.BasicReviewFrameExtractor","title":"llm_ie.extractors.BasicReviewFrameExtractor","text":"<pre><code>BasicReviewFrameExtractor(\n    inference_engine: InferenceEngine,\n    prompt_template: str,\n    review_mode: str,\n    review_prompt: str = None,\n    system_prompt: str = None,\n    **kwrs\n)\n</code></pre> <p>               Bases: <code>ReviewFrameExtractor</code></p> <p>This class add a review step after the BasicFrameExtractor. The Review process asks LLM to review its output and:     1. add more frames while keep current. This is efficient for boosting recall.      2. or, regenerate frames (add new and delete existing).  Use the review_mode parameter to specify. Note that the review_prompt should instruct LLM accordingly.</p> Parameters: <p>inference_engine : InferenceEngine     the LLM inferencing engine object. Must implements the chat() method. prompt_template : str     prompt template with \"{{}}\" placeholder. review_prompt : str: Optional     the prompt text that ask LLM to review. Specify addition or revision in the instruction.     if not provided, a default review prompt will be used.  review_mode : str     review mode. Must be one of {\"addition\", \"revision\"}     addition mode only ask LLM to add new frames, while revision mode ask LLM to regenerate. system_prompt : str, Optional     system prompt. Source code in <code>package/llm-ie/src/llm_ie/extractors.py</code> <pre><code>def __init__(self, inference_engine:InferenceEngine, prompt_template:str, review_mode:str, review_prompt:str=None, system_prompt:str=None, **kwrs):\n    \"\"\"\n    This class add a review step after the BasicFrameExtractor.\n    The Review process asks LLM to review its output and:\n        1. add more frames while keep current. This is efficient for boosting recall. \n        2. or, regenerate frames (add new and delete existing). \n    Use the review_mode parameter to specify. Note that the review_prompt should instruct LLM accordingly.\n\n    Parameters:\n    ----------\n    inference_engine : InferenceEngine\n        the LLM inferencing engine object. Must implements the chat() method.\n    prompt_template : str\n        prompt template with \"{{&lt;placeholder name&gt;}}\" placeholder.\n    review_prompt : str: Optional\n        the prompt text that ask LLM to review. Specify addition or revision in the instruction.\n        if not provided, a default review prompt will be used. \n    review_mode : str\n        review mode. Must be one of {\"addition\", \"revision\"}\n        addition mode only ask LLM to add new frames, while revision mode ask LLM to regenerate.\n    system_prompt : str, Optional\n        system prompt.\n    \"\"\"\n    super().__init__(inference_engine=inference_engine, \n                     unit_chunker=WholeDocumentUnitChunker(),\n                     prompt_template=prompt_template, \n                     review_mode=review_mode,\n                     review_prompt=review_prompt,\n                     system_prompt=system_prompt, \n                     context_chunker=NoContextChunker(),\n                     **kwrs)\n</code></pre>"},{"location":"api/extractors/#llm_ie.extractors.SentenceReviewFrameExtractor","title":"llm_ie.extractors.SentenceReviewFrameExtractor","text":"<pre><code>SentenceReviewFrameExtractor(\n    inference_engine: InferenceEngine,\n    prompt_template: str,\n    review_mode: str,\n    review_prompt: str = None,\n    system_prompt: str = None,\n    context_sentences: Union[str, int] = \"all\",\n    **kwrs\n)\n</code></pre> <p>               Bases: <code>ReviewFrameExtractor</code></p> <p>This class adds a review step after the SentenceFrameExtractor. For each sentence, the review process asks LLM to review its output and:     1. add more frames while keeping current. This is efficient for boosting recall.      2. or, regenerate frames (add new and delete existing).  Use the review_mode parameter to specify. Note that the review_prompt should instruct LLM accordingly.</p> Parameters: <p>inference_engine : InferenceEngine     the LLM inferencing engine object. Must implements the chat() method. prompt_template : str     prompt template with \"{{}}\" placeholder. review_prompt : str: Optional     the prompt text that ask LLM to review. Specify addition or revision in the instruction.     if not provided, a default review prompt will be used.  review_mode : str     review mode. Must be one of {\"addition\", \"revision\"}     addition mode only ask LLM to add new frames, while revision mode ask LLM to regenerate. system_prompt : str, Optional     system prompt. context_sentences : Union[str, int], Optional     number of sentences before and after the given sentence to provide additional context.      if \"all\", the full text will be provided in the prompt as context.      if 0, no additional context will be provided.         This is good for tasks that does not require context beyond the given sentence.      if &gt; 0, the number of sentences before and after the given sentence to provide as context.         This is good for tasks that require context beyond the given sentence. Source code in <code>package/llm-ie/src/llm_ie/extractors.py</code> <pre><code>def __init__(self, inference_engine:InferenceEngine, prompt_template:str,  \n             review_mode:str, review_prompt:str=None, system_prompt:str=None,\n             context_sentences:Union[str, int]=\"all\", **kwrs):\n    \"\"\"\n    This class adds a review step after the SentenceFrameExtractor.\n    For each sentence, the review process asks LLM to review its output and:\n        1. add more frames while keeping current. This is efficient for boosting recall. \n        2. or, regenerate frames (add new and delete existing). \n    Use the review_mode parameter to specify. Note that the review_prompt should instruct LLM accordingly.\n\n    Parameters:\n    ----------\n    inference_engine : InferenceEngine\n        the LLM inferencing engine object. Must implements the chat() method.\n    prompt_template : str\n        prompt template with \"{{&lt;placeholder name&gt;}}\" placeholder.\n    review_prompt : str: Optional\n        the prompt text that ask LLM to review. Specify addition or revision in the instruction.\n        if not provided, a default review prompt will be used. \n    review_mode : str\n        review mode. Must be one of {\"addition\", \"revision\"}\n        addition mode only ask LLM to add new frames, while revision mode ask LLM to regenerate.\n    system_prompt : str, Optional\n        system prompt.\n    context_sentences : Union[str, int], Optional\n        number of sentences before and after the given sentence to provide additional context. \n        if \"all\", the full text will be provided in the prompt as context. \n        if 0, no additional context will be provided.\n            This is good for tasks that does not require context beyond the given sentence. \n        if &gt; 0, the number of sentences before and after the given sentence to provide as context.\n            This is good for tasks that require context beyond the given sentence. \n    \"\"\"\n    if not isinstance(context_sentences, int) and context_sentences != \"all\":\n        raise ValueError('context_sentences must be an integer (&gt;= 0) or \"all\".')\n\n    if isinstance(context_sentences, int) and context_sentences &lt; 0:\n        raise ValueError(\"context_sentences must be a positive integer.\")\n\n    if isinstance(context_sentences, int):\n        context_chunker = SlideWindowContextChunker(window_size=context_sentences)\n    elif context_sentences == \"all\":\n        context_chunker = WholeDocumentContextChunker()\n\n    super().__init__(inference_engine=inference_engine, \n                     unit_chunker=SentenceUnitChunker(),\n                     prompt_template=prompt_template,\n                     review_mode=review_mode,\n                     review_prompt=review_prompt, \n                     system_prompt=system_prompt, \n                     context_chunker=context_chunker,\n                     **kwrs)\n</code></pre>"},{"location":"api/extractors/#relation-extractors","title":"Relation Extractors","text":""},{"location":"api/extractors/#llm_ie.extractors.BinaryRelationExtractor","title":"llm_ie.extractors.BinaryRelationExtractor","text":"<pre><code>BinaryRelationExtractor(\n    inference_engine: InferenceEngine,\n    prompt_template: str,\n    possible_relation_func: Callable,\n    system_prompt: str = None,\n    **kwrs\n)\n</code></pre> <p>               Bases: <code>RelationExtractor</code></p> <p>This class extracts binary (yes/no) relations between two entities. Input LLM inference engine, system prompt (optional), prompt template (with instruction, few-shot examples).</p> <p>Parameters:</p> Name Type Description Default <code>inference_engine</code> <code>InferenceEngine</code> <p>the LLM inferencing engine object. Must implements the chat() method.</p> required <code>prompt_template</code> <code>str</code> <p>prompt template with \"{{}}\" placeholder. required <code>possible_relation_func</code> <code>(Callable, Optional)</code> <p>a function that inputs 2 frames and returns a bool indicating possible relations between them.</p> required <code>system_prompt</code> <code>(str, Optional)</code> <p>system prompt.</p> <code>None</code> Source code in <code>package/llm-ie/src/llm_ie/extractors.py</code> <pre><code>def __init__(self, inference_engine:InferenceEngine, prompt_template:str, possible_relation_func: Callable, \n             system_prompt:str=None, **kwrs):\n    \"\"\"\n    This class extracts binary (yes/no) relations between two entities.\n    Input LLM inference engine, system prompt (optional), prompt template (with instruction, few-shot examples).\n\n    Parameters\n    ----------\n    inference_engine : InferenceEngine\n        the LLM inferencing engine object. Must implements the chat() method.\n    prompt_template : str\n        prompt template with \"{{&lt;placeholder name&gt;}}\" placeholder.\n    possible_relation_func : Callable, Optional\n        a function that inputs 2 frames and returns a bool indicating possible relations between them.\n    system_prompt : str, Optional\n        system prompt.\n    \"\"\"\n    super().__init__(inference_engine=inference_engine,\n                     prompt_template=prompt_template,\n                     system_prompt=system_prompt,\n                     **kwrs)\n\n    if possible_relation_func:\n        # Check if possible_relation_func is a function\n        if not callable(possible_relation_func):\n            raise TypeError(f\"Expect possible_relation_func as a function, received {type(possible_relation_func)} instead.\")\n\n        sig = inspect.signature(possible_relation_func)\n        # Check if frame_1, frame_2 are in input parameters\n        if len(sig.parameters) != 2:\n            raise ValueError(\"The possible_relation_func must have exactly frame_1 and frame_2 as parameters.\")\n        if \"frame_1\" not in sig.parameters.keys():\n            raise ValueError(\"The possible_relation_func is missing frame_1 as a parameter.\")\n        if \"frame_2\" not in sig.parameters.keys():\n            raise ValueError(\"The possible_relation_func is missing frame_2 as a parameter.\")\n        # Check if output is a bool\n        if sig.return_annotation != bool:\n            raise ValueError(f\"Expect possible_relation_func to output a bool, current type hint suggests {sig.return_annotation} instead.\")\n\n        self.possible_relation_func = possible_relation_func\n</code></pre>"},{"location":"api/extractors/#llm_ie.extractors.BinaryRelationExtractor.extract","title":"extract","text":"<pre><code>extract(\n    doc: LLMInformationExtractionDocument,\n    buffer_size: int = 100,\n    max_new_tokens: int = 128,\n    temperature: float = 0.0,\n    stream: bool = False,\n    return_messages_log: bool = False,\n    **kwrs\n) -&gt; List[Dict]\n</code></pre> <p>This method considers all combinations of two frames. Use the possible_relation_func to filter impossible pairs. Outputs pairs that are related.</p> Parameters: <p>doc : LLMInformationExtractionDocument     a document with frames. buffer_size : int, Optional     the number of characters before and after the two frames in the ROI text. max_new_tokens : str, Optional     the max number of new tokens LLM should generate.  temperature : float, Optional     the temperature for token sampling. stream : bool, Optional     if True, LLM generated text will be printed in terminal in real-time.  return_messages_log : bool, Optional     if True, a list of messages will be returned.</p> <p>Return : List[Dict]     a list of dict with {\"frame_1_id\", \"frame_2_id\"}.</p> Source code in <code>package/llm-ie/src/llm_ie/extractors.py</code> <pre><code>def extract(self, doc:LLMInformationExtractionDocument, buffer_size:int=100, max_new_tokens:int=128, \n            temperature:float=0.0, stream:bool=False, return_messages_log:bool=False, **kwrs) -&gt; List[Dict]:\n    \"\"\"\n    This method considers all combinations of two frames. Use the possible_relation_func to filter impossible pairs.\n    Outputs pairs that are related.\n\n    Parameters:\n    -----------\n    doc : LLMInformationExtractionDocument\n        a document with frames.\n    buffer_size : int, Optional\n        the number of characters before and after the two frames in the ROI text.\n    max_new_tokens : str, Optional\n        the max number of new tokens LLM should generate. \n    temperature : float, Optional\n        the temperature for token sampling.\n    stream : bool, Optional\n        if True, LLM generated text will be printed in terminal in real-time. \n    return_messages_log : bool, Optional\n        if True, a list of messages will be returned.\n\n    Return : List[Dict]\n        a list of dict with {\"frame_1_id\", \"frame_2_id\"}.\n    \"\"\"\n    pairs = itertools.combinations(doc.frames, 2)\n\n    if return_messages_log:\n        messages_log = []\n\n    output = []\n    for frame_1, frame_2 in pairs:\n        pos_rel = self.possible_relation_func(frame_1, frame_2)\n\n        if pos_rel:\n            roi_text = self._get_ROI(frame_1, frame_2, doc.text, buffer_size=buffer_size)\n            if stream:\n                print(f\"\\n\\n{Fore.GREEN}ROI text:{Style.RESET_ALL} \\n{roi_text}\\n\")\n                print(f\"{Fore.BLUE}Extraction:{Style.RESET_ALL}\")\n            messages = []\n            if self.system_prompt:\n                messages.append({'role': 'system', 'content': self.system_prompt})\n\n            messages.append({'role': 'user', 'content': self._get_user_prompt(text_content={\"roi_text\":roi_text, \n                                                                                            \"frame_1\": str(frame_1.to_dict()),\n                                                                                            \"frame_2\": str(frame_2.to_dict())}\n                                                                                            )})\n\n            gen_text = self.inference_engine.chat(\n                            messages=messages, \n                            max_new_tokens=max_new_tokens, \n                            temperature=temperature,\n                            stream=stream,\n                            **kwrs\n                        )\n            rel_json = self._extract_json(gen_text)\n            if self._post_process(rel_json):\n                output.append({'frame_1_id':frame_1.frame_id, 'frame_2_id':frame_2.frame_id})\n\n            if return_messages_log:\n                messages.append({\"role\": \"assistant\", \"content\": gen_text})\n                messages_log.append(messages)\n\n    if return_messages_log:\n        return output, messages_log\n    return output\n</code></pre>"},{"location":"api/extractors/#llm_ie.extractors.BinaryRelationExtractor.extract_async","title":"extract_async  <code>async</code>","text":"<pre><code>extract_async(\n    doc: LLMInformationExtractionDocument,\n    buffer_size: int = 100,\n    max_new_tokens: int = 128,\n    temperature: float = 0.0,\n    concurrent_batch_size: int = 32,\n    return_messages_log: bool = False,\n    **kwrs\n) -&gt; List[Dict]\n</code></pre> <p>This is the asynchronous version of the extract() method.</p> Parameters: <p>doc : LLMInformationExtractionDocument     a document with frames. buffer_size : int, Optional     the number of characters before and after the two frames in the ROI text. max_new_tokens : str, Optional     the max number of new tokens LLM should generate.  temperature : float, Optional     the temperature for token sampling. concurrent_batch_size : int, Optional     the number of frame pairs to process in concurrent. return_messages_log : bool, Optional     if True, a list of messages will be returned.</p> <p>Return : List[Dict]     a list of dict with {\"frame_1\", \"frame_2\"}.</p> Source code in <code>package/llm-ie/src/llm_ie/extractors.py</code> <pre><code>async def extract_async(self, doc:LLMInformationExtractionDocument, buffer_size:int=100, max_new_tokens:int=128, \n                        temperature:float=0.0, concurrent_batch_size:int=32, return_messages_log:bool=False, **kwrs) -&gt; List[Dict]:\n    \"\"\"\n    This is the asynchronous version of the extract() method.\n\n    Parameters:\n    -----------\n    doc : LLMInformationExtractionDocument\n        a document with frames.\n    buffer_size : int, Optional\n        the number of characters before and after the two frames in the ROI text.\n    max_new_tokens : str, Optional\n        the max number of new tokens LLM should generate. \n    temperature : float, Optional\n        the temperature for token sampling.\n    concurrent_batch_size : int, Optional\n        the number of frame pairs to process in concurrent.\n    return_messages_log : bool, Optional\n        if True, a list of messages will be returned.\n\n    Return : List[Dict]\n        a list of dict with {\"frame_1\", \"frame_2\"}. \n    \"\"\"\n    # Check if self.inference_engine.chat_async() is implemented\n    if not hasattr(self.inference_engine, 'chat_async'):\n        raise NotImplementedError(f\"{self.inference_engine.__class__.__name__} does not have chat_async() method.\")\n\n    pairs = itertools.combinations(doc.frames, 2)\n    if return_messages_log:\n        messages_log = []\n\n    n_frames = len(doc.frames)\n    num_pairs = (n_frames * (n_frames-1)) // 2\n    output = []\n    for i in range(0, num_pairs, concurrent_batch_size):\n        rel_pair_list = []\n        tasks = []\n        batch = list(itertools.islice(pairs, concurrent_batch_size))\n        batch_messages = []\n        for frame_1, frame_2 in batch:\n            pos_rel = self.possible_relation_func(frame_1, frame_2)\n\n            if pos_rel:\n                rel_pair_list.append({'frame_1_id':frame_1.frame_id, 'frame_2_id':frame_2.frame_id})\n                roi_text = self._get_ROI(frame_1, frame_2, doc.text, buffer_size=buffer_size)\n                messages = []\n                if self.system_prompt:\n                    messages.append({'role': 'system', 'content': self.system_prompt})\n\n                messages.append({'role': 'user', 'content': self._get_user_prompt(text_content={\"roi_text\":roi_text, \n                                                                                                \"frame_1\": str(frame_1.to_dict()),\n                                                                                                \"frame_2\": str(frame_2.to_dict())}\n                                                                                                )})\n\n                task = asyncio.create_task(\n                    self.inference_engine.chat_async(\n                        messages=messages, \n                        max_new_tokens=max_new_tokens, \n                        temperature=temperature,\n                        **kwrs\n                    )\n                )\n                tasks.append(task)\n                batch_messages.append(messages)\n\n        responses = await asyncio.gather(*tasks)\n\n        for d, response, messages in zip(rel_pair_list, responses, batch_messages):\n            if return_messages_log:\n                messages.append({\"role\": \"assistant\", \"content\": response})\n                messages_log.append(messages)\n\n            rel_json = self._extract_json(response)\n            if self._post_process(rel_json):\n                output.append(d)\n\n    if return_messages_log:\n        return output, messages_log\n    return output\n</code></pre>"},{"location":"api/extractors/#llm_ie.extractors.BinaryRelationExtractor.extract_relations","title":"extract_relations","text":"<pre><code>extract_relations(\n    doc: LLMInformationExtractionDocument,\n    buffer_size: int = 100,\n    max_new_tokens: int = 128,\n    temperature: float = 0.0,\n    concurrent: bool = False,\n    concurrent_batch_size: int = 32,\n    stream: bool = False,\n    return_messages_log: bool = False,\n    **kwrs\n) -&gt; List[Dict]\n</code></pre> <p>This method considers all combinations of two frames. Use the possible_relation_func to filter impossible pairs.</p> Parameters: <p>doc : LLMInformationExtractionDocument     a document with frames. buffer_size : int, Optional     the number of characters before and after the two frames in the ROI text. max_new_tokens : str, Optional     the max number of new tokens LLM should generate.  temperature : float, Optional     the temperature for token sampling. concurrent: bool, Optional     if True, the extraction will be done in concurrent. concurrent_batch_size : int, Optional     the number of frame pairs to process in concurrent. stream : bool, Optional     if True, LLM generated text will be printed in terminal in real-time.  return_messages_log : bool, Optional     if True, a list of messages will be returned.</p> <p>Return : List[Dict]     a list of dict with {\"frame_1\", \"frame_2\"} for all relations.</p> Source code in <code>package/llm-ie/src/llm_ie/extractors.py</code> <pre><code>def extract_relations(self, doc:LLMInformationExtractionDocument, buffer_size:int=100, max_new_tokens:int=128, \n                     temperature:float=0.0, concurrent:bool=False, concurrent_batch_size:int=32, \n                     stream:bool=False, return_messages_log:bool=False, **kwrs) -&gt; List[Dict]:\n    \"\"\"\n    This method considers all combinations of two frames. Use the possible_relation_func to filter impossible pairs.\n\n    Parameters:\n    -----------\n    doc : LLMInformationExtractionDocument\n        a document with frames.\n    buffer_size : int, Optional\n        the number of characters before and after the two frames in the ROI text.\n    max_new_tokens : str, Optional\n        the max number of new tokens LLM should generate. \n    temperature : float, Optional\n        the temperature for token sampling.\n    concurrent: bool, Optional\n        if True, the extraction will be done in concurrent.\n    concurrent_batch_size : int, Optional\n        the number of frame pairs to process in concurrent.\n    stream : bool, Optional\n        if True, LLM generated text will be printed in terminal in real-time. \n    return_messages_log : bool, Optional\n        if True, a list of messages will be returned.\n\n    Return : List[Dict]\n        a list of dict with {\"frame_1\", \"frame_2\"} for all relations.\n    \"\"\"\n    if not doc.has_frame():\n        raise ValueError(\"Input document must have frames.\")\n\n    if doc.has_duplicate_frame_ids():\n        raise ValueError(\"All frame_ids in the input document must be unique.\")\n\n    if concurrent:\n        if stream:\n            warnings.warn(\"stream=True is not supported in concurrent mode.\", RuntimeWarning)\n\n        nest_asyncio.apply() # For Jupyter notebook. Terminal does not need this.\n        return asyncio.run(self.extract_async(doc=doc, \n                                              buffer_size=buffer_size, \n                                              max_new_tokens=max_new_tokens,\n                                              temperature=temperature,\n                                              concurrent_batch_size=concurrent_batch_size, \n                                              return_messages_log=return_messages_log,\n                                              **kwrs)\n                            )\n    else:\n        return self.extract(doc=doc, \n                            buffer_size=buffer_size, \n                            max_new_tokens=max_new_tokens,\n                            temperature=temperature,\n                            stream=stream,\n                            return_messages_log=return_messages_log,\n                            **kwrs)\n</code></pre>"},{"location":"api/extractors/#llm_ie.extractors.MultiClassRelationExtractor","title":"llm_ie.extractors.MultiClassRelationExtractor","text":"<pre><code>MultiClassRelationExtractor(\n    inference_engine: InferenceEngine,\n    prompt_template: str,\n    possible_relation_types_func: Callable,\n    system_prompt: str = None,\n    **kwrs\n)\n</code></pre> <p>               Bases: <code>RelationExtractor</code></p> <p>This class extracts relations with relation types. Input LLM inference engine, system prompt (optional), prompt template (with instruction, few-shot examples).</p> <p>Parameters:</p> Name Type Description Default <code>inference_engine</code> <code>InferenceEngine</code> <p>the LLM inferencing engine object. Must implements the chat() method.</p> required <code>prompt_template</code> <code>str</code> <p>prompt template with \"{{}}\" placeholder. required <code>possible_relation_types_func</code> <code>Callable</code> <p>a function that inputs 2 frames and returns a List of possible relation types between them.  If the two frames must not have relations, this function should return an empty list [].</p> required <code>system_prompt</code> <code>(str, Optional)</code> <p>system prompt.</p> <code>None</code> Source code in <code>package/llm-ie/src/llm_ie/extractors.py</code> <pre><code>def __init__(self, inference_engine:InferenceEngine, prompt_template:str, possible_relation_types_func: Callable, \n             system_prompt:str=None, **kwrs):\n    \"\"\"\n    This class extracts relations with relation types.\n    Input LLM inference engine, system prompt (optional), prompt template (with instruction, few-shot examples).\n\n    Parameters\n    ----------\n    inference_engine : InferenceEngine\n        the LLM inferencing engine object. Must implements the chat() method.\n    prompt_template : str\n        prompt template with \"{{&lt;placeholder name&gt;}}\" placeholder.\n    possible_relation_types_func : Callable\n        a function that inputs 2 frames and returns a List of possible relation types between them. \n        If the two frames must not have relations, this function should return an empty list [].\n    system_prompt : str, Optional\n        system prompt.\n    \"\"\"\n    super().__init__(inference_engine=inference_engine,\n                     prompt_template=prompt_template,\n                     system_prompt=system_prompt,\n                     **kwrs)\n\n    if possible_relation_types_func:\n        # Check if possible_relation_types_func is a function\n        if not callable(possible_relation_types_func):\n            raise TypeError(f\"Expect possible_relation_types_func as a function, received {type(possible_relation_types_func)} instead.\")\n\n        sig = inspect.signature(possible_relation_types_func)\n        # Check if frame_1, frame_2 are in input parameters\n        if len(sig.parameters) != 2:\n            raise ValueError(\"The possible_relation_types_func must have exactly frame_1 and frame_2 as parameters.\")\n        if \"frame_1\" not in sig.parameters.keys():\n            raise ValueError(\"The possible_relation_types_func is missing frame_1 as a parameter.\")\n        if \"frame_2\" not in sig.parameters.keys():\n            raise ValueError(\"The possible_relation_types_func is missing frame_2 as a parameter.\")\n        # Check if output is a List\n        if sig.return_annotation not in {inspect._empty, List, List[str]}:\n            raise ValueError(f\"Expect possible_relation_types_func to output a List of string, current type hint suggests {sig.return_annotation} instead.\")\n\n        self.possible_relation_types_func = possible_relation_types_func\n</code></pre>"},{"location":"api/extractors/#llm_ie.extractors.MultiClassRelationExtractor.extract","title":"extract","text":"<pre><code>extract(\n    doc: LLMInformationExtractionDocument,\n    buffer_size: int = 100,\n    max_new_tokens: int = 128,\n    temperature: float = 0.0,\n    stream: bool = False,\n    return_messages_log: bool = False,\n    **kwrs\n) -&gt; List[Dict]\n</code></pre> <p>This method considers all combinations of two frames. Use the possible_relation_types_func to filter impossible pairs.</p> Parameters: <p>doc : LLMInformationExtractionDocument     a document with frames. buffer_size : int, Optional     the number of characters before and after the two frames in the ROI text. max_new_tokens : str, Optional     the max number of new tokens LLM should generate.  temperature : float, Optional     the temperature for token sampling. stream : bool, Optional     if True, LLM generated text will be printed in terminal in real-time.  return_messages_log : bool, Optional     if True, a list of messages will be returned.</p> <p>Return : List[Dict]     a list of dict with {\"frame_1_id\", \"frame_2_id\", \"relation\"} for all frame pairs.</p> Source code in <code>package/llm-ie/src/llm_ie/extractors.py</code> <pre><code>def extract(self, doc:LLMInformationExtractionDocument, buffer_size:int=100, max_new_tokens:int=128, \n            temperature:float=0.0, stream:bool=False, return_messages_log:bool=False, **kwrs) -&gt; List[Dict]:\n    \"\"\"\n    This method considers all combinations of two frames. Use the possible_relation_types_func to filter impossible pairs.\n\n    Parameters:\n    -----------\n    doc : LLMInformationExtractionDocument\n        a document with frames.\n    buffer_size : int, Optional\n        the number of characters before and after the two frames in the ROI text.\n    max_new_tokens : str, Optional\n        the max number of new tokens LLM should generate. \n    temperature : float, Optional\n        the temperature for token sampling.\n    stream : bool, Optional\n        if True, LLM generated text will be printed in terminal in real-time. \n    return_messages_log : bool, Optional\n        if True, a list of messages will be returned.\n\n    Return : List[Dict]\n        a list of dict with {\"frame_1_id\", \"frame_2_id\", \"relation\"} for all frame pairs.\n    \"\"\"\n    pairs = itertools.combinations(doc.frames, 2)\n\n    if return_messages_log:\n        messages_log = []\n\n    output = []\n    for frame_1, frame_2 in pairs:\n        pos_rel_types = self.possible_relation_types_func(frame_1, frame_2)\n\n        if pos_rel_types:\n            roi_text = self._get_ROI(frame_1, frame_2, doc.text, buffer_size=buffer_size)\n            if stream:\n                print(f\"\\n\\n{Fore.GREEN}ROI text:{Style.RESET_ALL} \\n{roi_text}\\n\")\n                print(f\"{Fore.BLUE}Extraction:{Style.RESET_ALL}\")\n            messages = []\n            if self.system_prompt:\n                messages.append({'role': 'system', 'content': self.system_prompt})\n\n            messages.append({'role': 'user', 'content': self._get_user_prompt(text_content={\"roi_text\":roi_text, \n                                                                                            \"frame_1\": str(frame_1.to_dict()),\n                                                                                            \"frame_2\": str(frame_2.to_dict()),\n                                                                                            \"pos_rel_types\":str(pos_rel_types)}\n                                                                                            )})\n\n            gen_text = self.inference_engine.chat(\n                            messages=messages, \n                            max_new_tokens=max_new_tokens, \n                            temperature=temperature,\n                            stream=stream,\n                            **kwrs\n                        )\n\n            if return_messages_log:\n                messages.append({\"role\": \"assistant\", \"content\": gen_text})\n                messages_log.append(messages)\n\n            rel_json = self._extract_json(gen_text)\n            rel = self._post_process(rel_json, pos_rel_types)\n            if rel:\n                output.append({'frame_1_id':frame_1.frame_id, 'frame_2_id':frame_2.frame_id, 'relation':rel})\n\n    if return_messages_log:\n        return output, messages_log\n    return output   \n</code></pre>"},{"location":"api/extractors/#llm_ie.extractors.MultiClassRelationExtractor.extract_async","title":"extract_async  <code>async</code>","text":"<pre><code>extract_async(\n    doc: LLMInformationExtractionDocument,\n    buffer_size: int = 100,\n    max_new_tokens: int = 128,\n    temperature: float = 0.0,\n    concurrent_batch_size: int = 32,\n    return_messages_log: bool = False,\n    **kwrs\n) -&gt; List[Dict]\n</code></pre> <p>This is the asynchronous version of the extract() method.</p> Parameters: <p>doc : LLMInformationExtractionDocument     a document with frames. buffer_size : int, Optional     the number of characters before and after the two frames in the ROI text. max_new_tokens : str, Optional     the max number of new tokens LLM should generate.  temperature : float, Optional     the temperature for token sampling. concurrent_batch_size : int, Optional     the number of frame pairs to process in concurrent. return_messages_log : bool, Optional     if True, a list of messages will be returned.</p> <p>Return : List[Dict]     a list of dict with {\"frame_1_id\", \"frame_2_id\", \"relation\"} for all frame pairs.</p> Source code in <code>package/llm-ie/src/llm_ie/extractors.py</code> <pre><code>async def extract_async(self, doc:LLMInformationExtractionDocument, buffer_size:int=100, max_new_tokens:int=128, \n                        temperature:float=0.0, concurrent_batch_size:int=32, return_messages_log:bool=False, **kwrs) -&gt; List[Dict]:\n    \"\"\"\n    This is the asynchronous version of the extract() method.\n\n    Parameters:\n    -----------\n    doc : LLMInformationExtractionDocument\n        a document with frames.\n    buffer_size : int, Optional\n        the number of characters before and after the two frames in the ROI text.\n    max_new_tokens : str, Optional\n        the max number of new tokens LLM should generate. \n    temperature : float, Optional\n        the temperature for token sampling.\n    concurrent_batch_size : int, Optional\n        the number of frame pairs to process in concurrent.\n    return_messages_log : bool, Optional\n        if True, a list of messages will be returned.\n\n    Return : List[Dict]\n        a list of dict with {\"frame_1_id\", \"frame_2_id\", \"relation\"} for all frame pairs. \n    \"\"\"\n    # Check if self.inference_engine.chat_async() is implemented\n    if not hasattr(self.inference_engine, 'chat_async'):\n        raise NotImplementedError(f\"{self.inference_engine.__class__.__name__} does not have chat_async() method.\")\n\n    pairs = itertools.combinations(doc.frames, 2)\n    if return_messages_log:\n        messages_log = []\n\n    n_frames = len(doc.frames)\n    num_pairs = (n_frames * (n_frames-1)) // 2\n    output = []\n    for i in range(0, num_pairs, concurrent_batch_size):\n        rel_pair_list = []\n        tasks = []\n        batch = list(itertools.islice(pairs, concurrent_batch_size))\n        batch_messages = []\n        for frame_1, frame_2 in batch:\n            pos_rel_types = self.possible_relation_types_func(frame_1, frame_2)\n\n            if pos_rel_types:\n                rel_pair_list.append({'frame_1':frame_1.frame_id, 'frame_2':frame_2.frame_id, 'pos_rel_types':pos_rel_types})\n                roi_text = self._get_ROI(frame_1, frame_2, doc.text, buffer_size=buffer_size)\n                messages = []\n                if self.system_prompt:\n                    messages.append({'role': 'system', 'content': self.system_prompt})\n\n                messages.append({'role': 'user', 'content': self._get_user_prompt(text_content={\"roi_text\":roi_text, \n                                                                                                \"frame_1\": str(frame_1.to_dict()),\n                                                                                                \"frame_2\": str(frame_2.to_dict()),\n                                                                                                \"pos_rel_types\":str(pos_rel_types)}\n                                                                                                )})\n                task = asyncio.create_task(\n                    self.inference_engine.chat_async(\n                        messages=messages, \n                        max_new_tokens=max_new_tokens, \n                        temperature=temperature,\n                        **kwrs\n                    )\n                )\n                tasks.append(task)\n                batch_messages.append(messages)\n\n        responses = await asyncio.gather(*tasks)\n\n        for d, response, messages in zip(rel_pair_list, responses, batch_messages):\n            if return_messages_log:\n                messages.append({\"role\": \"assistant\", \"content\": response})\n                messages_log.append(messages)\n\n            rel_json = self._extract_json(response)\n            rel = self._post_process(rel_json, d['pos_rel_types'])\n            if rel:\n                output.append({'frame_1_id':d['frame_1'], 'frame_2_id':d['frame_2'], 'relation':rel})\n\n    if return_messages_log:\n        return output, messages_log\n    return output\n</code></pre>"},{"location":"api/extractors/#llm_ie.extractors.MultiClassRelationExtractor.extract_relations","title":"extract_relations","text":"<pre><code>extract_relations(\n    doc: LLMInformationExtractionDocument,\n    buffer_size: int = 100,\n    max_new_tokens: int = 128,\n    temperature: float = 0.0,\n    concurrent: bool = False,\n    concurrent_batch_size: int = 32,\n    stream: bool = False,\n    return_messages_log: bool = False,\n    **kwrs\n) -&gt; List[Dict]\n</code></pre> <p>This method considers all combinations of two frames. Use the possible_relation_types_func to filter impossible pairs.</p> Parameters: <p>doc : LLMInformationExtractionDocument     a document with frames. buffer_size : int, Optional     the number of characters before and after the two frames in the ROI text. max_new_tokens : str, Optional     the max number of new tokens LLM should generate.  temperature : float, Optional     the temperature for token sampling. concurrent: bool, Optional     if True, the extraction will be done in concurrent. concurrent_batch_size : int, Optional     the number of frame pairs to process in concurrent. stream : bool, Optional     if True, LLM generated text will be printed in terminal in real-time.  return_messages_log : bool, Optional     if True, a list of messages will be returned.</p> <p>Return : List[Dict]     a list of dict with {\"frame_1\", \"frame_2\", \"relation\"} for all relations.</p> Source code in <code>package/llm-ie/src/llm_ie/extractors.py</code> <pre><code>def extract_relations(self, doc:LLMInformationExtractionDocument, buffer_size:int=100, max_new_tokens:int=128, \n                     temperature:float=0.0, concurrent:bool=False, concurrent_batch_size:int=32, \n                     stream:bool=False, return_messages_log:bool=False, **kwrs) -&gt; List[Dict]:\n    \"\"\"\n    This method considers all combinations of two frames. Use the possible_relation_types_func to filter impossible pairs.\n\n    Parameters:\n    -----------\n    doc : LLMInformationExtractionDocument\n        a document with frames.\n    buffer_size : int, Optional\n        the number of characters before and after the two frames in the ROI text.\n    max_new_tokens : str, Optional\n        the max number of new tokens LLM should generate. \n    temperature : float, Optional\n        the temperature for token sampling.\n    concurrent: bool, Optional\n        if True, the extraction will be done in concurrent.\n    concurrent_batch_size : int, Optional\n        the number of frame pairs to process in concurrent.\n    stream : bool, Optional\n        if True, LLM generated text will be printed in terminal in real-time. \n    return_messages_log : bool, Optional\n        if True, a list of messages will be returned.\n\n    Return : List[Dict]\n        a list of dict with {\"frame_1\", \"frame_2\", \"relation\"} for all relations.\n    \"\"\"\n    if not doc.has_frame():\n        raise ValueError(\"Input document must have frames.\")\n\n    if doc.has_duplicate_frame_ids():\n        raise ValueError(\"All frame_ids in the input document must be unique.\")\n\n    if concurrent:\n        if stream:\n            warnings.warn(\"stream=True is not supported in concurrent mode.\", RuntimeWarning)\n\n        nest_asyncio.apply() # For Jupyter notebook. Terminal does not need this.\n        return asyncio.run(self.extract_async(doc=doc, \n                                              buffer_size=buffer_size, \n                                              max_new_tokens=max_new_tokens,\n                                              temperature=temperature,\n                                              concurrent_batch_size=concurrent_batch_size, \n                                              return_messages_log=return_messages_log,\n                                              **kwrs)\n                            )\n    else:\n        return self.extract(doc=doc, \n                            buffer_size=buffer_size, \n                            max_new_tokens=max_new_tokens,\n                            temperature=temperature,\n                            stream=stream,\n                            return_messages_log=return_messages_log,\n                            **kwrs)\n</code></pre>"},{"location":"api/prompt_editor/","title":"Prompt Edirot API","text":""},{"location":"api/prompt_editor/#llm_ie.prompt_editor.PromptEditor","title":"llm_ie.prompt_editor.PromptEditor","text":"<pre><code>PromptEditor(\n    inference_engine: InferenceEngine,\n    extractor: FrameExtractor,\n    prompt_guide: str = None,\n)\n</code></pre> <p>This class is a LLM agent that rewrite or comment a prompt draft based on the prompt guide of an extractor.</p> <p>Parameters:</p> Name Type Description Default <code>inference_engine</code> <code>InferenceEngine</code> <p>the LLM inferencing engine object. Must implements the chat() method.</p> required <code>extractor</code> <code>FrameExtractor</code> <p>a FrameExtractor.</p> required <code>prompt_guide</code> <code>str</code> <p>the prompt guide for the extractor.  All built-in extractors have a prompt guide in the asset folder. Passing values to this parameter  will override the built-in prompt guide which is not recommended. For custom extractors, this parameter must be provided.</p> <code>None</code> Source code in <code>package/llm-ie/src/llm_ie/prompt_editor.py</code> <pre><code>def __init__(self, inference_engine:InferenceEngine, extractor:FrameExtractor, prompt_guide:str=None):\n    \"\"\"\n    This class is a LLM agent that rewrite or comment a prompt draft based on the prompt guide of an extractor.\n\n    Parameters\n    ----------\n    inference_engine : InferenceEngine\n        the LLM inferencing engine object. Must implements the chat() method.\n    extractor : FrameExtractor\n        a FrameExtractor. \n    prompt_guide : str, optional\n        the prompt guide for the extractor. \n        All built-in extractors have a prompt guide in the asset folder. Passing values to this parameter \n        will override the built-in prompt guide which is not recommended.\n        For custom extractors, this parameter must be provided.\n    \"\"\"\n    self.inference_engine = inference_engine\n\n    # if prompt_guide is provided, use it anyways\n    if prompt_guide:\n        self.prompt_guide = prompt_guide\n    # if prompt_guide is not provided, get it from the extractor\n    else:\n        self.prompt_guide = extractor.get_prompt_guide()\n        # when extractor does not have a prompt guide (e.g. custom extractor), ValueError\n        if self.prompt_guide is None:\n            raise ValueError(f\"Prompt guide for {extractor.__class__.__name__} is not available. Use `prompt_guide` parameter to provide a prompt guide.\")\n\n    # get system prompt\n    file_path = importlib.resources.files('llm_ie.asset.PromptEditor_prompts').joinpath('system.txt')\n    with open(file_path, 'r') as f:\n        self.system_prompt =  f.read()\n</code></pre>"},{"location":"api/prompt_editor/#llm_ie.prompt_editor.PromptEditor.rewrite","title":"rewrite","text":"<pre><code>rewrite(draft: str, **kwrs) -&gt; str\n</code></pre> <p>This method inputs a prompt draft and rewrites it following the extractor's guideline.</p> Source code in <code>package/llm-ie/src/llm_ie/prompt_editor.py</code> <pre><code>def rewrite(self, draft:str, **kwrs) -&gt; str:\n    \"\"\"\n    This method inputs a prompt draft and rewrites it following the extractor's guideline.\n    \"\"\"\n    file_path = importlib.resources.files('llm_ie.asset.PromptEditor_prompts').joinpath('rewrite.txt')\n    with open(file_path, 'r') as f:\n        rewrite_prompt_template = f.read()\n\n    prompt = self._apply_prompt_template(text_content={\"draft\": draft, \"prompt_guideline\": self.prompt_guide}, \n                                         prompt_template=rewrite_prompt_template)\n    messages = [{\"role\": \"system\", \"content\": self.system_prompt},\n                {\"role\": \"user\", \"content\": prompt}]\n    res = self.inference_engine.chat(messages, verbose=True, **kwrs)\n    return res\n</code></pre>"},{"location":"api/prompt_editor/#llm_ie.prompt_editor.PromptEditor.comment","title":"comment","text":"<pre><code>comment(draft: str, **kwrs) -&gt; str\n</code></pre> <p>This method inputs a prompt draft and comment following the extractor's guideline.</p> Source code in <code>package/llm-ie/src/llm_ie/prompt_editor.py</code> <pre><code>def comment(self, draft:str, **kwrs) -&gt; str:\n    \"\"\"\n    This method inputs a prompt draft and comment following the extractor's guideline.\n    \"\"\"\n    file_path = importlib.resources.files('llm_ie.asset.PromptEditor_prompts').joinpath('comment.txt')\n    with open(file_path, 'r') as f:\n        comment_prompt_template = f.read()\n\n    prompt = self._apply_prompt_template(text_content={\"draft\": draft, \"prompt_guideline\": self.prompt_guide}, \n                                         prompt_template=comment_prompt_template)\n    messages = [{\"role\": \"system\", \"content\": self.system_prompt},\n                {\"role\": \"user\", \"content\": prompt}]\n    res = self.inference_engine.chat(messages, verbose=True, **kwrs)\n    return res\n</code></pre>"},{"location":"api/prompt_editor/#llm_ie.prompt_editor.PromptEditor.chat","title":"chat","text":"<pre><code>chat(**kwrs)\n</code></pre> <p>External method that detects the environment and calls the appropriate chat method.</p> Source code in <code>package/llm-ie/src/llm_ie/prompt_editor.py</code> <pre><code>def chat(self, **kwrs):\n    \"\"\"\n    External method that detects the environment and calls the appropriate chat method.\n    \"\"\"\n    if 'ipykernel' in sys.modules:\n        self._IPython_chat(**kwrs)\n    else:\n        self._terminal_chat(**kwrs)\n</code></pre>"},{"location":"api/prompt_editor/#llm_ie.prompt_editor.PromptEditor.chat_stream","title":"chat_stream","text":"<pre><code>chat_stream(\n    messages: List[Dict[str, str]], **kwrs\n) -&gt; Generator[str, None, None]\n</code></pre> <p>This method processes messages and yields response chunks from the inference engine. This is for frontend App.</p> Parameters: <p>messages : List[Dict[str, str]]     List of message dictionaries (e.g., [{\"role\": \"user\", \"content\": \"Hi\"}]).</p> Yields: <pre><code>Chunks of the assistant's response.\n</code></pre> Source code in <code>package/llm-ie/src/llm_ie/prompt_editor.py</code> <pre><code>def chat_stream(self, messages: List[Dict[str, str]], **kwrs) -&gt; Generator[str, None, None]:\n    \"\"\"\n    This method processes messages and yields response chunks from the inference engine.\n    This is for frontend App.\n\n    Parameters:\n    ----------\n    messages : List[Dict[str, str]]\n        List of message dictionaries (e.g., [{\"role\": \"user\", \"content\": \"Hi\"}]).\n\n    Yields:\n    -------\n        Chunks of the assistant's response.\n    \"\"\"\n    # Validate messages\n    if not isinstance(messages, list) or not all(isinstance(m, dict) and 'role' in m and 'content' in m for m in messages):\n         raise ValueError(\"Messages must be a list of dictionaries with 'role' and 'content' keys.\")\n\n    # Always append system prompt and initial user message\n    file_path = importlib.resources.files('llm_ie.asset.PromptEditor_prompts').joinpath('chat.txt')\n    with open(file_path, 'r') as f:\n        chat_prompt_template = f.read()\n\n    prompt = self._apply_prompt_template(text_content={\"prompt_guideline\": self.prompt_guide}, \n                                        prompt_template=chat_prompt_template)\n\n    messages = [{\"role\": \"system\", \"content\": self.system_prompt},\n                {\"role\": \"user\", \"content\": prompt}] + messages\n\n\n    stream_generator = self.inference_engine.chat(messages, stream=True, **kwrs)\n    yield from stream_generator\n</code></pre>"},{"location":"release_notes/v1.0.0/","title":"V1.0.0","text":""},{"location":"release_notes/v1.0.0/#highlights","title":"\u26a1Highlights","text":""},{"location":"release_notes/v1.0.0/#web-application","title":"Web Application","text":"<p>We provide a drag-and-drop web Application for no-code access to the LLM-IE. The web Application streamlines the workflow:</p> <ol> <li>Prompt engineering with LLM agent</li> <li>Prompting algorithm design</li> <li>Visualization &amp; Validation</li> <li>Repeat step #1-#3 until achieves high accuracy</li> </ol>"},{"location":"release_notes/v1.0.0/#prompt-editor-tab","title":"Prompt Editor Tab","text":"<p>Select an LLM API (e.g., OpenAI, Azure, Ollama, Huggingface Hub) and LLM. Describe your task and the Prompt Editor LLM agent behind the scene will help construct a structured prompt template.</p> <p></p>"},{"location":"release_notes/v1.0.0/#frame-extraction-tab","title":"Frame Extraction Tab","text":"<p>Select an inferencing API and specify an LLM. In the \"Input Text\" textbox, paste the document text that you want to process. In the \"Prompt Template\" textbox, paste the prompt template you obtained from the previous step. Click on the \"Start Extraction\" button and watch LLM processes unit-by-unit on the right panel. </p> <p></p>"},{"location":"release_notes/v1.0.0/#result-viewer-tab","title":"Result Viewer Tab","text":"<p>Drop the result file from the previous step. Optionally, select the attribute key in the dropdown for color coding.</p> <p></p>"},{"location":"release_notes/v1.0.0/#refactored-frame-extractor","title":"Refactored Frame Extractor","text":"<p>The <code>FrameExtractor</code> has been developed over time. As new methods been added, we kept adding new classes (e.g., Sentence Review) which is exhausive. The new design separates chunking methods (e.g., while document, sentence, paragraph) and prompting method (e.g., direct and review). Chunking is now defined in <code>UnitChunker</code> and <code>ContextChunker</code>, while <code>FrameExtractor</code> defines prompting method. </p> <p>For example, what used to be  <code>SentenceReviewFrameExtractor</code> for sentence-level extraction and review.</p> <pre><code>from llm_ie import SentenceReviewFrameExtractor\n\nextractor = SentenceReviewFrameExtractor(inference_engine, prompt_temp, context_sentences=2, review_mode=\"revision\")\n</code></pre> <p>Is now implemented in a more flexible modular design. Users can specify using sentence as unit (extract from one sentence at a time), using a slide window of 2 sentences as context (additional context for each unit), and use review as prompting method. </p> <pre><code>from llm_ie import ReviewFrameExtractor, SentenceUnitChunker, SlideWindowContextChunker\n\nunit_chunker = SentenceUnitChunker()\ncontext_chunker = SlideWindowContextChunker(window_size=2)\nextractor = ReviewFrameExtractor(inference_engine=llm, \n                                 unit_chunker=unit_chunker,\n                                 context_chunker=context_chunker,\n                                 prompt_template=prompt_template,\n                                 review_mode=\"revision\")\n</code></pre> <p>Note that <code>BasicFrameExtractor</code>, <code>BasicReviewFrameExtractor</code> (previously <code>ReviewFrameExtractor</code>), <code>SentenceFrameExtractor</code>, and <code>SentenceReviewFrameExtractor</code> are still available as pre-packaged \"Convenience Frame Extractors\".</p>"},{"location":"release_notes/v1.0.0/#features-and-changes","title":"\ud83d\udce3Features and Changes","text":""},{"location":"release_notes/v1.0.0/#optimized-concurrent-processing","title":"Optimized concurrent processing","text":"<p>We now reimplemented concurrent processing in FrameExtractor with Semaphore. This allows dynamic allocation of available computation resources. Instead of waiting for a batch inferencing (e.g., sentences) to complete, the next units can start when a slot becomes available. This optimization increases thoughput while maintaning batch size. </p>"},{"location":"release_notes/v1.0.0/#simplified-frameextractor-schema","title":"Simplified FrameExtractor schema","text":"<p>\u26a0\ufe0f<code>entity_key</code> parameter in <code>extract_frames</code> method is now deprecated!</p> <p>We now regulate the <code>FrameExtractor</code> prompt template and post-processor to use <code>entity_text</code> for entity text and <code>attr</code> for attributes. This avoids the confusing <code>entity_key</code> parameter in <code>extract_frames</code> method and further strengthen accuracy of post-processing. </p> <p>The new LLM output schema is:</p> <pre><code>[\n    {\n        \"entity_text\": \"&lt;entity text&gt;\", \n        \"attr\": {\n                    \"&lt;attribute 1&gt;\": \"&lt;value 1&gt;\", \n                    \"&lt;attribute 2&gt;\": \"&lt;value 2&gt;\"\n                }\n    },\n    ...\n]\n</code></pre> <p>After post-processing, it becomes:</p> <pre><code>[\n    {\n        \"entity_text\": \"&lt;entity text&gt;\", \n        \"start\": &lt;start char&gt;,\n        \"end\": &lt;end char&gt;, \n        \"attr\": {\n                    \"&lt;attribute 1&gt;\": \"&lt;value 1&gt;\", \n                    \"&lt;attribute 2&gt;\": \"&lt;value 2&gt;\"\n                }\n    },\n    ...\n]\n</code></pre>"},{"location":"release_notes/v1.0.0/#added-streaming-method-to-frameextractor","title":"Added streaming method to FrameExtractor","text":"<p>We added streaming method, <code>FrameExtractor.stream</code> that returns a generator to support frontend development. To avoid confusion, the <code>stream</code> parameter in <code>FrameExtractor.extract</code> and <code>FrameExtractor.extract_frames</code> is now renamed to <code>verbose</code>. </p>"},{"location":"release_notes/v1.0.0/#removed-cot-frameextractor","title":"Removed CoT FrameExtractor","text":"<p>As reasoning models become more mature, we decide to reply on them instead. We added support to OpenAI's reasoning models (\"o\" serise) in version (v0.4.5). We will continue adding support to other reasoning models.</p> <p>\u26a0\ufe0f<code>SentenceCoTFrameExtractor</code> is now deprecated.</p>"}]}
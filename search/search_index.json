{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"LLM-IE Documentation","text":"<p>LLM-IE is a toolkit that provides robust information extraction utilities for named entity, entity attributes, and entity relation extraction. The flowchart below demonstrates the workflow:</p> <ol> <li>Prompt engineering with LLM agent: Prompt Editor</li> <li>Prompting algorithm design: Extractors</li> <li>Visualization &amp; Validation: Visualization</li> <li>Repeat step #1-#3 until achieves high accuracy</li> </ol> <p>For more information and benchmarks, please check our paper: <pre><code>@article{hsu2025llm,\n  title={LLM-IE: a python package for biomedical generative information extraction with large language models},\n  author={Hsu, Enshuo and Roberts, Kirk},\n  journal={JAMIA open},\n  volume={8},\n  number={2},\n  pages={ooaf012},\n  year={2025},\n  publisher={Oxford University Press}\n}\n</code></pre></p>"},{"location":"extractors/","title":"Extractors","text":"<p>An extractor implements a prompting algorithm for information extraction. There are three extractor families: <code>FrameExtractor</code>, <code>AttributeExtractor</code> and <code>RelationExtractor</code>.  The <code>FrameExtractor</code> extracts named entities with attributes (\"frames\"). The <code>AttributeExtractor</code> extracts additional attributes and serve as an assistant class for <code>FrameExtractor</code>. The <code>RelationExtractor</code> extracts the relations (and relation types) between frames. Under <code>FrameExtractor</code>, we made pre-packaged extractors that does not require much configuation and are often sufficient for regular use case (Convenience FrameExtractor).</p>"},{"location":"extractors/#frameextractor","title":"FrameExtractor","text":"<p>Frame extractors in general adopts an unit-context schema. The purpose is to avoid having LLM to process long context and suffer from needle in the haystack challenge. We split an input document into multiple units. LLM only process a unit of text at a time. </p> <ul> <li>Unit: a text snippet that LLM extrator will process at a time. It could be a sentence, a line of text, or a paragraph. </li> <li>Context: the context around the unit. For exapmle, a slidewindow of 2 sentences before and after. Context is optional. </li> </ul> <p></p>"},{"location":"extractors/#directframeextractor","title":"DirectFrameExtractor","text":"<p>The <code>DirectFrameExtractor</code> implements the unit-context schema. We start by defining the unit using one of the <code>UnitChunker</code>. The <code>SentenceUnitChunker</code> chunks the input document into sentences. Then, we define how context should be provided by choosing one of the <code>ContextChunker</code>. The <code>SlideWindowContextChunker</code> parse 2 units (sentences in this case) before and after each unit as context. For more options, see Chunkers.</p> <pre><code>from llm_ie import DirectFrameExtractor, SentenceUnitChunker, SlideWindowContextChunker\n\nunit_chunker = SentenceUnitChunker()\ncontext_chunker = SlideWindowContextChunker(window_size=2)\nextractor = DirectFrameExtractor(inference_engine=llm, \n                                 unit_chunker=unit_chunker,\n                                 context_chunker=context_chunker,\n                                 prompt_template=prompt_template)\n\nframes, messages_log =  extractor.extract_frames(note_text, concurrent=True, return_messages_log=True)\n</code></pre>"},{"location":"extractors/#reviewframeextractor","title":"ReviewFrameExtractor","text":"<p>The <code>ReviewFrameExtractor</code> is a child of <code>DirectFrameExtractor</code>. It adds a review step after the initial output. There are two review modes:</p> <ol> <li>Addition mode: add more frames while keeping current. This is efficient for boosting recall. </li> <li>Revision mode: regenerate frames (add new and delete existing). </li> </ol> <p>Under the Addition mode (<code>review_mode=\"addition\"</code>), the <code>review_prompt</code> needs to instruct the LLM not to regenerate existing extractions:</p> <p>... You should ONLY add new diagnoses. DO NOT regenerate the entire answer.</p> <p>Under the Revision mode (<code>review_mode=\"revision\"</code>), the <code>review_prompt</code> needs to instruct the LLM to regenerate:</p> <p>... Regenerate your output.</p> <p>It is recommended to leave the <code>review_prompt=None</code> and use the default, unless there are special needs. </p> <pre><code>from llm_ie import ReviewFrameExtractor, SentenceUnitChunker, SlideWindowContextChunker\n\nunit_chunker = SentenceUnitChunker()\ncontext_chunker = SlideWindowContextChunker(window_size=2)\nextractor = ReviewFrameExtractor(inference_engine=llm, \n                                 unit_chunker=unit_chunker,\n                                 context_chunker=context_chunker,\n                                 prompt_template=prompt_template,\n                                 review_mode=\"addition\")\n</code></pre>"},{"location":"extractors/#post-processing","title":"Post-processing","text":"<p>Since the output entity text from LLMs might not be consistent with the original text due to the limitations of LLMs, we apply JSON repair, case-sensitive, fuzzy search, and entity overlap settings in post-processing to find the accurate entity span. </p>"},{"location":"extractors/#json-repair","title":"JSON repair","text":"<p>Automatically detect and fix broken JSON format with json_repair.</p>"},{"location":"extractors/#case-sensitive","title":"Case sensitive","text":"<p>set <code>case_sensitive=False</code> to allow matching even when LLM generates inconsistent upper/lower cases. </p>"},{"location":"extractors/#fussy-match","title":"Fussy match","text":"<p>In the <code>extract_frames()</code> method, setting parameter <code>fuzzy_match=True</code> applies Jaccard similarity matching. The most likely spans will be returned as entity text.</p>"},{"location":"extractors/#entity-overlap","title":"Entity overlap","text":"<p>Set <code>allow_overlap_entities=True</code> to cpature overlapping entities. Note that this can cause multiple frames to be generated on the same entity span if they have same entity text.</p>"},{"location":"extractors/#concurrent-optimization","title":"Concurrent Optimization","text":"<p>For concurrent extraction (recommended), set <code>concurrent=True</code> in <code>FrameExtractor.extract_frames</code>. The <code>concurrent_batch_size</code> sets the batch size of units to be processed in cocurrent.</p>"},{"location":"extractors/#convenience-frameextractor","title":"Convenience FrameExtractor","text":"<p>The <code>DirectFrameExtractor</code> and <code>ReviewFrameExtractor</code> provide flexible interfaces for all settings. However, in most use cases, simple interface is preferred. We pre-package some common (and high performance) settings for convenience. </p>"},{"location":"extractors/#basicframeextractor","title":"BasicFrameExtractor","text":"<p>The <code>BasicFrameExtractor</code> prompts LLM with the entire document.</p> <pre><code>from llm_ie import BasicFrameExtractor\n\nextractor = BasicFrameExtractor(inference_engine, prompt_template)\n</code></pre> <p>It is equivalent to:</p> <pre><code>from llm_ie import DirectFrameExtractor, WholeDocumentUnitChunker, NoContextChunker\n\nunit_chunker = WholeDocumentUnitChunker()\ncontext_chunker = NoContextChunker()\nextractor = DirectFrameExtractor(inference_engine=llm, \n                                 unit_chunker=unit_chunker,\n                                 context_chunker=context_chunker,\n                                 prompt_template=prompt_template)\n</code></pre>"},{"location":"extractors/#basicreviewframeextractor","title":"BasicReviewFrameExtractor","text":"<p>Similar to the <code>BasicFrameExtractor</code>, but adds a revieww step after the initial outputs.</p> <pre><code>from llm_ie import BasicReviewFrameExtractor\n\nextractor = BasicReviewFrameExtractor(inference_engine, prompt_template, review_mode=\"revision\")\n</code></pre> <p>This is equivalent to:</p> <pre><code>from llm_ie import ReviewFrameExtractor, WholeDocumentUnitChunker, NoContextChunker\n\nunit_chunker = WholeDocumentUnitChunker()\ncontext_chunker = NoContextChunker()\nextractor = ReviewFrameExtractor(inference_engine=llm, \n                                 unit_chunker=unit_chunker,\n                                 context_chunker=context_chunker,\n                                 prompt_template=prompt_template,\n                                 review_mode=\"revision\")\n</code></pre>"},{"location":"extractors/#sentenceframeextractor","title":"SentenceFrameExtractor","text":"<p>The <code>SentenceFrameExtractor</code> prompts LLMs to extract sentence-by-sentence. The <code>context_sentences</code> sets number of sentences before and after the sentence of interest to provide additional context. When <code>context_sentences=2</code>, 2 sentences before and 2 sentences after are included in the user prompt as context. When <code>context_sentences=\"all\"</code>, the entire document is included as context. When <code>context_sentences=0</code>, no context is provided and LLM will only extract based on the current sentence of interest.</p> <pre><code>from llm_ie import SentenceFrameExtractor\n\n# slide window of 2 sentences as context\nextractor = SentenceFrameExtractor(inference_engine, prompt_template, context_sentences=2)\n</code></pre> <p>It is equivalent to:</p> <pre><code>from llm_ie import DirectFrameExtractor, SentenceUnitChunker, SlideWindowContextChunker\n\nunit_chunker = SentenceUnitChunker()\ncontext_chunker = SlideWindowContextChunker(window_size=2)\nextractor = DirectFrameExtractor(inference_engine=llm, \n                                 unit_chunker=unit_chunker,\n                                 context_chunker=context_chunker,\n                                 prompt_template=prompt_template)\n</code></pre>"},{"location":"extractors/#sentencereviewframeextractor","title":"SentenceReviewFrameExtractor","text":"<p>The <code>SentenceReviewFrameExtractor</code> performs sentence-level extraction and review. The example below use no context, revision review mode.</p> <pre><code>from llm_ie import SentenceReviewFrameExtractor\n\nextractor = SentenceReviewFrameExtractor(inference_engine, prompt_temp, context_sentences=0, review_mode=\"revision\")\n</code></pre> <p>It is equivalent to:</p> <pre><code>from llm_ie import ReviewFrameExtractor, SentenceUnitChunker, NoContextChunker\n\nunit_chunker = SentenceUnitChunker()\ncontext_chunker = NoContextChunker()\nextractor = ReviewFrameExtractor(inference_engine=llm, \n                                 unit_chunker=unit_chunker,\n                                 context_chunker=context_chunker,\n                                 prompt_template=prompt_template,\n                                 review_mode=\"revision\")\n</code></pre>"},{"location":"extractors/#attributeextractor","title":"AttributeExtractor","text":"<p>Although the <code>FrameExtractor</code> can extract both named entity and attributes in a single prompt, for some use cases where many attributes are required, offloading some attributes to the <code>AttributeExtractor</code> can improve the accuracy. For examlpe, in clinical notes, we want to extract treatments and diagnoses. For treatment frames, attributes like drug name, strength, dosage, frequency, and route are required; for diagnosis frames, diagnosed date, status, and ICD code are required. In such case, having a frame extractor to handle everything would be challenging. We can divide the treatment frames and diagnosis frames to their own attribute extractors</p> <p>To define an <code>AttributeExtractor</code> <pre><code>from llm_ie import AttributeExtractor\n\nextractor = AttributeExtractor(\n    inference_engine=llm,\n    prompt_template=prompt,\n)\n</code></pre></p> <p>The prompt must include placeholders <code>{{frame}}</code> and <code>{{context}}</code>. For example: <pre><code>### Task description\nThis is an attribute extraction task. Given a diagnosis entity and the context, you need to generate attributes for the entity. \n\n### Schema definition\n    \"Date\" which is the date when the diagnosis was made in MM/DD/YYYY format,\n    \"Status\" which is the current status of the diagnosis (e.g. active, resolved, etc.)\n\n### Output format definition\nYour output should follow the JSON format:\n{\"Date\": \"&lt;MM/DD/YYYY&gt;\", \"Status\": \"&lt;status&gt;\"}\n\nI am only interested in the content between []. Do not explain your answer. \n\n### Hints\n- If the date is not complete, use the first available date in the context. For example, if the date is 01/2023, you should return 01/01/2023.\n- If the status is not available, you should return \"not specified\".\n\n### Entity\nInformation about the entity to extract attributes from:\n{{frame}}\n\n### Context\nContext for the entity. The &lt;Entity&gt; tags are used to mark the entity in the context.\n{{context}}\n</code></pre></p> <p>To extract, input a list of frames. There could be some existing attributes. The <code>AttributeExtractor</code> prompts LLM with a frame and the context to extract attributes. Setting <code>inplace==True</code>, the new attibutes will be added/updated to the input frames, while setting <code>inplace=False</code>, new frames with attributes will be returned.  <pre><code>frame1 = LLMInformationExtractionFrame(frame_id=\"id1\", start=1026, end=1040, entity_text='Hyperlipidemia', attr={\"Date\": \"not available\"})\nframe2 = LLMInformationExtractionFrame(frame_id=\"id2\", start=1063, end=1087, entity_text='Type 2 Diabetes Mellitus')\nframe3 = LLMInformationExtractionFrame(frame_id=\"id3\", start=2046, end=2066, entity_text='No acute infiltrates')\n\n# extract attributes                               \nextractor.extract_attributes([frame1, frame2, frame3], note_text, verbose=True, inplace=True)\n# Use concurrent\n# extractor.extract_attributes([frame1, frame2, frame3], note_text, concurrent=True, inplace=True)\n</code></pre></p> <p>The extraction process: <pre><code>Frame: id1\n{'frame_id': 'id1', 'start': 1026, 'end': 1040, 'entity_text': 'Hyperlipidemia', 'attr': {'Date': '01/01/2015', 'Status': 'not specified'}}\n\nContext:\n...tral chest, radiating to the left arm and jaw. He also experienced dyspnea on exertion and occasional palpitations. The patient denied any recent upper respiratory infection, cough, or fever.\n\n#### Past Medical History\n- Hypertension (diagnosed in 2010)\n- &lt;entity&gt; Hyperlipidemia &lt;/entity&gt; (diagnosed in 2015)\n- Type 2 Diabetes Mellitus (diagnosed in 2018)\n\n#### Social History\n- Former smoker (quit in 2010)\n- Occasional alcohol consumption\n- Works as an accountant\n- Married with two children\n\n#### Family History\n- Father: myocardial infarcti...\n\nExtraction:\n[{\"Date\": \"01/01/2015\", \"Status\": \"not specified\"}]\n\nFrame: id2\n...\n</code></pre></p>"},{"location":"extractors/#relationextractor","title":"RelationExtractor","text":"<p>Relation extractors prompt LLM with combinations of two frames from a document (<code>LLMInformationExtractionDocument</code>) and extract relations. The <code>BinaryRelationExtractor</code> extracts binary relations (yes/no) between two frames. The <code>MultiClassRelationExtractor</code> extracts relations and assign relation types (\"multi-class\"). </p> <p>An important feature of the relation extractors is that users are required to define a <code>possible_relation_func</code> or <code>possible_relation_types_func</code> function for the extractors. The reason is, there are too many possible combinations of two frames (N choose 2 combinations). The <code>possible_relation_func</code> helps rule out impossible combinations and therefore, reduce the LLM inferencing burden.</p>"},{"location":"extractors/#binaryrelationextractor","title":"BinaryRelationExtractor","text":"<p>Use the get_prompt_guide() method to inspect the prompt template guideline for BinaryRelationExtractor. <pre><code>from llm_ie.extractors import BinaryRelationExtractor\n\nprint(BinaryRelationExtractor.get_prompt_guide())\n</code></pre></p> <pre><code>Prompt Template Design:\n\n1. Task description:\n   Provide a detailed description of the task, including the background and the type of task (e.g., binary relation extraction). Mention the region of interest (ROI) text. \n2. Schema definition: \n   List the criterion for relation (True) and for no relation (False).\n\n3. Output format definition:\n   The ouptut must be a dictionary with a key \"Relation\" (i.e., {\"Relation\": \"&lt;True or False&gt;\"}).\n\n4. (optional) Hints:\n   Provide itemized hints for the information extractors to guide the extraction process.\n\n5. (optional) Examples:\n   Include examples in the format:  \n    Input: ...  \n    Output: ...\n\n6. Entity 1 full information:\n   Include a placeholder in the format {{&lt;frame_1&gt;}}\n\n7. Entity 2 full information:\n   Include a placeholder in the format {{&lt;frame_2&gt;}}\n\n8. Input placeholders:\n   The template must include a placeholder \"{{roi_text}}\" for the ROI text.\n\n\nExample:\n\n    # Task description\n    This is a binary relation extraction task. Given a region of interest (ROI) text and two entities from a medical note, indicate the relation existence between the two entities.\n\n    # Schema definition\n        True: if there is a relationship between a medication name (one of the entities) and its strength or frequency (the other entity).\n        False: Otherwise.\n\n    # Output format definition\n    Your output should follow the JSON format:\n    {\"Relation\": \"&lt;True or False&gt;\"}\n\n    I am only interested in the content between []. Do not explain your answer. \n\n    # Hints\n        1. Your input always contains one medication entity and 1) one strength entity or 2) one frequency entity.\n        2. Pay attention to the medication entity and see if the strength or frequency is for it.\n        3. If the strength or frequency is for another medication, output False. \n        4. If the strength or frequency is for the same medication but at a different location (span), output False.\n\n    # Entity 1 full information:\n    {{frame_1}}\n\n    # Entity 2 full information:\n    {{frame_2}}\n\n    # Input placeholders\n    ROI Text with the two entities annotated with &lt;entity_1&gt; and &lt;entity_2&gt;:\n    \"{{roi_text}}\"\n</code></pre> <p>As an example, we define the <code>possible_relation_func</code> function:   - if the two frames are &gt; 500 characters apart, we assume no relation (False)   - if the two frames are \"Medication\" and \"Strength\", or \"Medication\" and \"Frequency\", there could be relations (True)</p> <pre><code>def possible_relation_func(frame_1, frame_2) -&gt; bool:\n    \"\"\"\n    This function pre-process two frames and outputs a bool indicating whether the two frames could be related.\n    \"\"\"\n    # if the distance between the two frames are &gt; 500 characters, assume no relation.\n    if abs(frame_1.start - frame_2.start) &gt; 500:\n        return False\n\n    # if the entity types are \"Medication\" and \"Strength\", there could be relations.\n    if (frame_1.attr[\"entity_type\"] == \"Medication\" and frame_2.attr[\"entity_type\"] == \"Strength\") or \\\n        (frame_2.attr[\"entity_type\"] == \"Medication\" and frame_1.attr[\"entity_type\"] == \"Strength\"):\n        return True\n\n    # if the entity types are \"Medication\" and \"Frequency\", there could be relations.\n    if (frame_1.attr[\"entity_type\"] == \"Medication\" and frame_2.attr[\"entity_type\"] == \"Frequency\") or \\\n        (frame_2.attr[\"entity_type\"] == \"Medication\" and frame_1.attr[\"entity_type\"] == \"Frequency\"):\n        return True\n\n    # Otherwise, no relation.\n    return False\n</code></pre> <p>In the <code>BinaryRelationExtractor</code> constructor, we pass in the prompt template and <code>possible_relation_func</code>.</p> <pre><code>from llm_ie.extractors import BinaryRelationExtractor\n\nextractor = BinaryRelationExtractor(inference_engine, prompt_template=prompt_template, possible_relation_func=possible_relation_func)\n# Extract binary relations with concurrent mode (faster)\nrelations = extractor.extract_relations(doc, concurrent=True)\n\n# To print out the step-by-step, use the `concurrent=False` and `stream=True` options\nrelations = extractor.extract_relations(doc, concurrent=False, stream=True)\n</code></pre>"},{"location":"extractors/#multiclassrelationextractor","title":"MultiClassRelationExtractor","text":"<p>The main difference from <code>BinaryRelationExtractor</code> is that the <code>MultiClassRelationExtractor</code> allows specifying relation types. The prompt template guideline has an additional placeholder for possible relation types <code>{{pos_rel_types}}</code>. </p> <pre><code>print(MultiClassRelationExtractor.get_prompt_guide())\n</code></pre> <pre><code>Prompt Template Design:\n\n1. Task description:\n   Provide a detailed description of the task, including the background and the type of task (e.g., binary relation extraction). Mention the region of interest (ROI) text. \n2. Schema definition: \n   List the criterion for relation (True) and for no relation (False).\n\n3. Output format definition:\n   This section must include a placeholder \"{{pos_rel_types}}\" for the possible relation types.\n   The ouptut must be a dictionary with a key \"RelationType\" (i.e., {\"RelationType\": \"&lt;relation type or No Relation&gt;\"}).\n\n4. (optional) Hints:\n   Provide itemized hints for the information extractors to guide the extraction process.\n\n5. (optional) Examples:\n   Include examples in the format:  \n    Input: ...  \n    Output: ...\n\n6. Entity 1 full information:\n   Include a placeholder in the format {{&lt;frame_1&gt;}}\n\n7. Entity 2 full information:\n   Include a placeholder in the format {{&lt;frame_2&gt;}}\n\n8. Input placeholders:\n   The template must include a placeholder \"{{roi_text}}\" for the ROI text.\n\n\n\nExample:\n\n    # Task description\n    This is a multi-class relation extraction task. Given a region of interest (ROI) text and two frames from a medical note, classify the relation types between the two frames. \n\n    # Schema definition\n        Strength-Drug: this is a relationship between the drug strength and its name. \n        Dosage-Drug: this is a relationship between the drug dosage and its name.\n        Duration-Drug: this is a relationship between a drug duration and its name.\n        Frequency-Drug: this is a relationship between a drug frequency and its name.\n        Form-Drug: this is a relationship between a drug form and its name.\n        Route-Drug: this is a relationship between the route of administration for a drug and its name.\n        Reason-Drug: this is a relationship between the reason for which a drug was administered (e.g., symptoms, diseases, etc.) and a drug name.\n        ADE-Drug: this is a relationship between an adverse drug event (ADE) and a drug name.\n\n    # Output format definition\n    Choose one of the relation types listed below or choose \"No Relation\":\n    {{pos_rel_types}}\n\n    Your output should follow the JSON format:\n    {\"RelationType\": \"&lt;relation type or No Relation&gt;\"}\n\n    I am only interested in the content between []. Do not explain your answer. \n\n    # Hints\n        1. Your input always contains one medication entity and 1) one strength entity or 2) one frequency entity.\n        2. Pay attention to the medication entity and see if the strength or frequency is for it.\n        3. If the strength or frequency is for another medication, output \"No Relation\". \n        4. If the strength or frequency is for the same medication but at a different location (span), output \"No Relation\".\n\n    # Entity 1 full information:\n    {{frame_1}}\n\n    # Entity 2 full information:\n    {{frame_2}}\n\n    # Input placeholders\n    ROI Text with the two entities annotated with &lt;entity_1&gt; and &lt;entity_2&gt;:\n    \"{{roi_text}}\"\n</code></pre> <p>As an example, we define the <code>possible_relation_types_func</code> :   - if the two frames are &gt; 500 characters apart, we assume \"No Relation\" (output [])   - if the two frames are \"Medication\" and \"Strength\", the only possible relation types are \"Strength-Drug\" or \"No Relation\"   - if the two frames are \"Medication\" and \"Frequency\", the only possible relation types are \"Frequency-Drug\" or \"No Relation\"</p> <pre><code>def possible_relation_types_func(frame_1, frame_2) -&gt; List[str]:\n    # If the two frames are &gt; 500 characters apart, we assume \"No Relation\"\n    if abs(frame_1.start - frame_2.start) &gt; 500:\n        return []\n\n    # If the two frames are \"Medication\" and \"Strength\", the only possible relation types are \"Strength-Drug\" or \"No Relation\"\n    if (frame_1.attr[\"entity_type\"] == \"Medication\" and frame_2.attr[\"entity_type\"] == \"Strength\") or \\\n        (frame_2.attr[\"entity_type\"] == \"Medication\" and frame_1.attr[\"entity_type\"] == \"Strength\"):\n        return ['Strength-Drug']\n\n    # If the two frames are \"Medication\" and \"Frequency\", the only possible relation types are \"Frequency-Drug\" or \"No Relation\"\n    if (frame_1.attr[\"entity_type\"] == \"Medication\" and frame_2.attr[\"entity_type\"] == \"Frequency\") or \\\n        (frame_2.attr[\"entity_type\"] == \"Medication\" and frame_1.attr[\"entity_type\"] == \"Frequency\"):\n        return ['Frequency-Drug']\n\n    return []\n</code></pre> <pre><code>from llm_ie.extractors import MultiClassRelationExtractor\n\nextractor = MultiClassRelationExtractor(inference_engine, prompt_template=re_prompt_template,\n                                        possible_relation_types_func=possible_relation_types_func)\n\n# Extract multi-class relations with concurrent mode (faster)\nrelations = extractor.extract_relations(doc, concurrent=True)\n\n# To print out the step-by-step, use the `concurrent=False` and `stream=True` options\nrelations = extractor.extract_relations(doc, concurrent=False, stream=True)\n</code></pre>"},{"location":"extractors/#concurrent-optimization_1","title":"Concurrent Optimization","text":"<p>For concurrent extraction (recommended), the <code>async/await</code> feature is used to speed up inferencing. Set <code>concurrent=True</code> in <code>RelationExtractor.extract_relations</code>. The <code>concurrent_batch_size</code> sets the batch size of frame pairs to be processed in cocurrent.</p>"},{"location":"installation/","title":"Installation","text":"<p>LLM-IE Python package and web application installation guide.</p>"},{"location":"installation/#python-package","title":"Python package","text":"<p>The Python package is available on PyPI. </p> <p><pre><code>pip install llm-ie \n</code></pre> Note that this package does not check LLM inference engine installation nor install them. At least one LLM inference engine is required. There are built-in supports for LiteLLM, Llama-cpp-python, Ollama, Huggingface_hub, OpenAI API, and vLLM. For installation guides, please refer to those projects. Other inference engines can be configured through the <code>InferenceEngine</code> abstract class. See LLM Inference Engine section.</p>"},{"location":"installation/#web-application","title":"Web Application","text":""},{"location":"installation/#docker","title":"Docker","text":"<p>The easiest way to install our web application is \ud83d\udc33Docker. The image is available on Docker Hub. Use the command below to pull and run locally: <pre><code>docker pull daviden1013/llm-ie-web-app:latest\ndocker run -p 5000:5000 daviden1013/llm-ie-web-app:latest\n</code></pre></p> <p>Open your web browser and navigate to: http://localhost:5000</p> <p>If port 5000 is already in use on your machine, you can map it to a different local port. For example, to map it to local port 8080: <pre><code>docker run -p 8080:5000 daviden1013/llm-ie-web-app:latest\n</code></pre></p> <p>Then visit http://localhost:8080</p>"},{"location":"installation/#install-from-source","title":"Install from source","text":"<p>Alternatively, pull the repo and build the required environment locally.</p> <pre><code># Clone source code\ngit clone https://github.com/daviden1013/llm-ie.git\n\n# Install requirements\npip install -r llm-ie/web_app/requirements.txt\n\n# Run Web App\ncd llm-ie/web_app\npython run.py\n</code></pre>"},{"location":"llm_config/","title":"LLM Configuration","text":"<p>In some cases, it is helpful to adjust LLM sampling parameters (e.g., temperature, top-p, top-k, maximum new tokens) or use reasoning models (e.g., OpenAI o-series models, Qwen3) which requires special treatments in system prompt, user prompt, and sampling parameters. For example, OpenAI o-series reasoning models disallow passing a system prompt or setting custom temperature. Another example is Qwen3 hybrid thinking mode. Special tokens \"/think\" and \"/no_think\" should be appended to user prompts to control for the reasoning behavior. </p> Config class LLMs <code>BasicLLMConfig</code> Most non-reasoning LLMs  <li>Llama4 <li>Qwen3-30B-A3B-Instruct-2507 <li>... <code>ReasoningLLMConfig</code> Most reasoning LLMs  <li>Qwen3-30B-A3B-Thinking-2507 <li>gpt-oss-120b <li>... <code>Qwen3LLMConfig</code> Qwen3 hybrid thinking <li>Qwen3-30B-A3B <li>Qwen3-32B <li>... <code>OpenAIReasoningLLMConfig</code> OpenAI API reasoning models  <li>\"o\" series (o1, o3, o4) <li>..."},{"location":"llm_config/#setting-sampling-parameters","title":"Setting sampling parameters","text":"<p>LLM sampling parameters such as temperature, top-p, top-k, and maximum new tokens can be set by passing a <code>LLMConfig</code> class to the <code>InferenceEngine</code> constructor.</p> <pre><code>from llm_ie.engines import OpenAIInferenceEngine, BasicLLMConfig\n\nconfig = BasicLLMConfig(temperature=0.2, max_new_tokens=4096)\ninference_engine = OpenAIInferenceEngine(model=\"gpt-4.1-mini\", config=config)\n</code></pre>"},{"location":"llm_config/#reasoning-models","title":"Reasoning models","text":"<p>To use reasoning models such as OpenAI o-series (e.g., o1, o3, o3-mini, o4-mini), some special processing is required. We provide dedicated configuration classes for them.</p>"},{"location":"llm_config/#general-reasoning-models","title":"General reasoning models","text":"<p>Most reasoning models can be configured with <code>ReasoningLLMConfig</code>. By specifying the start and end thinking tags, the reasoning tokens will be excluded from model response while stored in messages log (if <code>return_messages_log=True</code>).</p> <pre><code>from llm_ie.engines import VLLMInferenceEngine, ReasoningLLMConfig\n\nllm = VLLMInferenceEngine(model=\"Qwen/Qwen3-30B-A3B-Thinking-2507\", \n                          config=ReasoningLLMConfig(thinking_token_start=\"&lt;think&gt;\", thinking_token_end=\"&lt;/think&gt;\", temperature=0.8, max_new_tokens=8192))\n</code></pre>"},{"location":"llm_config/#openai-o-series-reasoning-models","title":"OpenAI o-series reasoning models","text":"<p>OpenAI o-series reasoning model API does not allow setting system prompts. Contents in the system should be included in user prompts. Also, custom temperature is not allowed. We provide a dedicated configuration class <code>OpenAIReasoningLLMConfig</code> for these models. </p> <pre><code>from llm_ie.engines import OpenAIInferenceEngine, OpenAIReasoningLLMConfig\n\ninference_engine = OpenAIInferenceEngine(model=\"o4-mini\", \n                                         config=OpenAIReasoningLLMConfig(reasoning_effort=\"low\"))\n</code></pre>"},{"location":"llm_config/#qwen3-hybrid-thinking-mode","title":"Qwen3 (hybrid thinking mode)","text":"<p>Does NOT work for Qwen3 2507 models. Use <code>ReasoningLLMConfig</code> and <code>BasicLLMConfig</code> instead.</p> <p>Qwen3 has a special way to manage reasoning behavior. The same models have thinking mode and non-thinking mode, controled by the prompting template. When a special token \"/think\" is appended to the user prompt, the models generate thinking tokens in a <code>&lt;think&gt;... &lt;/think&gt;</code> block. When  a special token \"/no_think\" is appended to the user prompt, the models generate an empty <code>&lt;think&gt;... &lt;/think&gt;</code> block. We provide a dedicated configuration class <code>Qwen3LLMConfig</code> for these models. </p> <pre><code>from llm_ie.engines import VLLMInferenceEngine, Qwen3LLMConfig\n\n# Thinking mode\nllm = VLLMInferenceEngine(model=\"Qwen/Qwen3-30B-A3B\", \n                          config=Qwen3LLMConfig(thinking_mode=True, temperature=0.6, top_p=0.95, top_k=20, max_new_tokens=8192))\n\n# Non-thinking mode\nllm = VLLMInferenceEngine(model=\"Qwen/Qwen3-30B-A3B\", \n                          config=Qwen3LLMConfig(thinking_mode=False, temperature=0.0, max_new_tokens=2048))\n</code></pre>"},{"location":"llm_inference_engine/","title":"LLM Inference Engine","text":"<p>We provide an interface for different LLM inference engines to work in the information extraction workflow. The built-in engines are <code>VLLMInferenceEngine</code>, <code>LiteLLMInferenceEngine</code>, <code>OpenAIInferenceEngine</code>, <code>HuggingFaceHubInferenceEngine</code>, <code>OllamaInferenceEngine</code>, and <code>LlamaCppInferenceEngine</code>. For customization, see customize inference engine. Inference engines accept a LLMConfig class where sampling parameters (e.g., temperature, top-p, top-k, maximum new tokens) and reasoning configuration (e.g., OpenAI o-series models, Qwen3) can be set.</p>"},{"location":"llm_inference_engine/#vllm","title":"vLLM","text":"<p>The vLLM support follows the OpenAI Compatible Server. For more parameters, please refer to the documentation. Below are examples for different models.</p>"},{"location":"llm_inference_engine/#meta-llama-31-8b-instruct","title":"Meta-Llama-3.1-8B-Instruct","text":"<p>Start the server in command line. <pre><code>CUDA_VISIBLE_DEVICES=&lt;GPU#&gt; vllm serve meta-llama/Meta-Llama-3.1-8B-Instruct \\\n    --api-key MY_API_KEY \\\n    --tensor-parallel-size &lt;# of GPUs to use&gt;\n</code></pre> Use <code>CUDA_VISIBLE_DEVICES</code> to specify GPUs to use. The <code>--tensor-parallel-size</code> should be set accordingly. The <code>--api-key</code> is optional.  the default port is 8000. <code>--port</code> sets the port. </p> <p>Define inference engine <pre><code>from llm_ie.engines import VLLMInferenceEngine\n\ninference_engine = VLLMInferenceEngine(model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\")\n</code></pre> The <code>model</code> must match the repo name specified in the server.</p>"},{"location":"llm_inference_engine/#qwen3-30b-a3b-hybrid-thinking-mode","title":"Qwen3-30B-A3B (hybrid thinking mode)","text":"<p>Start the server in command line. Specify <code>--reasoning-parser qwen3</code> to enable the reasoning parser.  <pre><code>vllm serve Qwen/Qwen3-30B-A3B \\\n    --tensor-parallel-size 4 \\\n    --enable-prefix-caching \\\n    --gpu-memory-utilization 0.9 \\\n    --reasoning-parser qwen3\n</code></pre> Define inference engine <pre><code>from llm_ie.engines import VLLMInferenceEngine, Qwen3LLMConfig\n\n# Thinking mode\ninference_engine = VLLMInferenceEngine(model=\"Qwen/Qwen3-30B-A3B\", \n                                       config=Qwen3LLMConfig(thinking_mode=True, temperature=0.6, top_p=0.95, top_k=20))\n# Non-thinking mode\ninference_engine = VLLMInferenceEngine(model=\"Qwen/Qwen3-30B-A3B\", \n                                       config=Qwen3LLMConfig(thinking_mode=False, temperature=0.7, top_p=0.8, top_k=20))\n</code></pre></p>"},{"location":"llm_inference_engine/#qwen3-30b-thinking-2507","title":"Qwen3-30B-Thinking-2507","text":"<p>Start the server in command line. Specify <code>--reasoning-parser qwen3</code> to enable the reasoning parser.  <pre><code>vllm serve Qwen/Qwen3-30B-A3B-Thinking-2507 \\\n   --tensor-parallel-size 4 \\\n   --enable-prefix-caching \\\n   --reasoning-parser qwen3\n</code></pre> Define inference engine <pre><code>from llm_ie.engines import VLLMInferenceEngine, ReasoningLLMConfig\n\ninference_engine = VLLMInferenceEngine(model=\"Qwen/Qwen3-30B-A3B-Thinking-2507\", \n                                       config=ReasoningLLMConfig(temperature=0.6, top_p=0.95, top_k=20))\n</code></pre></p>"},{"location":"llm_inference_engine/#gpt-oss-120b","title":"gpt-oss-120b","text":"<p>Start the server in command line. Specify <code>--reasoning-parser GptOss</code> to enable the reasoning parser.  <pre><code>vllm serve openai/gpt-oss-120b \\\n    --tensor-parallel-size 4 \\\n    --enable-prefix-caching \\\n    --reasoning-parser GptOss\n</code></pre> Define inference engine <pre><code>from llm_ie.engines import VLLMInferenceEngine, ReasoningLLMConfig\n\ninference_engine = VLLMInferenceEngine(model=\"openai/gpt-oss-120b\", \n                                       config=ReasoningLLMConfig(temperature=1.0, top_p=1.0, top_k=0))\n</code></pre></p>"},{"location":"llm_inference_engine/#openai-api-compatible-services","title":"OpenAI API &amp; Compatible Services","text":"<p>In bash, save API key to the environmental variable <code>OPENAI_API_KEY</code>. <pre><code>export OPENAI_API_KEY=&lt;your_API_key&gt;\n</code></pre></p> <p>In Python, create inference engine and specify model name. For the available models, refer to OpenAI webpage.  For more parameters, see OpenAI API reference.</p>"},{"location":"llm_inference_engine/#openai-models","title":"OpenAI models","text":""},{"location":"llm_inference_engine/#gpt-41-mini","title":"gpt-4.1-mini","text":"<pre><code>from llm_ie.engines import OpenAIInferenceEngine, BasicLLMConfig\n\ninference_engine = OpenAIInferenceEngine(model=\"gpt-4.1-mini\", \n                                         config=BasicLLMConfig(temperature=0.0, max_new_tokens=1024))\n</code></pre>"},{"location":"llm_inference_engine/#o-series-reasoning-models","title":"o-series reasoning models","text":"<p>For OpenAI reasoning models (o-series), pass a <code>OpenAIReasoningLLMConfig</code> object to <code>OpenAIInferenceEngine</code> constructor. </p> <pre><code>from llm_ie.engines import OpenAIInferenceEngine, OpenAIReasoningLLMConfig\n\ninference_engine = OpenAIInferenceEngine(model=\"o4-mini\", \n                                         config=OpenAIReasoningLLMConfig(reasoning_effort=\"low\"))\n</code></pre>"},{"location":"llm_inference_engine/#openai-compatible-services","title":"OpenAI compatible services","text":"<p>For OpenAI compatible services (OpenRouter for example): <pre><code>from llm_ie.engines import OpenAIInferenceEngine, BasicLLMConfig\n\ninference_engine = OpenAIInferenceEngine(base_url=\"https://openrouter.ai/api/v1\", model=\"meta-llama/llama-4-scout\",\n                                         config=BasicLLMConfig(temperature=0.0, max_new_tokens=1024))\n</code></pre></p>"},{"location":"llm_inference_engine/#azure-openai-api","title":"Azure OpenAI API","text":"<p>In bash, save the endpoint name and API key to environmental variables <code>AZURE_OPENAI_ENDPOINT</code> and <code>AZURE_OPENAI_API_KEY</code>. <pre><code>export AZURE_OPENAI_API_KEY=\"&lt;your_API_key&gt;\"\nexport AZURE_OPENAI_ENDPOINT=\"&lt;your_endpoint&gt;\"\n</code></pre></p> <p>In Python, create inference engine and specify model name. For the available models, refer to OpenAI webpage.  For more parameters, see Azure OpenAI reference.</p>"},{"location":"llm_inference_engine/#gpt-41-mini_1","title":"gpt-4.1-mini","text":"<pre><code>from llm_ie.engines import AzureOpenAIInferenceEngine, BasicLLMConfig\n\ninference_engine = AzureOpenAIInferenceEngine(model=\"gpt-4.1-mini\", config=BasicLLMConfig(temperature=0.0, max_new_tokens=1024))\n</code></pre>"},{"location":"llm_inference_engine/#o-series-reasoning-models_1","title":"o-series reasoning models","text":"<p>For reasoning models (o-series), pass a <code>OpenAIReasoningLLMConfig</code> object to <code>OpenAIInferenceEngine</code> constructor. </p> <pre><code>from llm_ie.engines import AzureOpenAIInferenceEngine, OpenAIReasoningLLMConfig\n\ninference_engine = AzureOpenAIInferenceEngine(model=\"o1-mini\", \n                                              config=OpenAIReasoningLLMConfig(reasoning_effort=\"low\"))\n</code></pre>"},{"location":"llm_inference_engine/#openrouter","title":"OpenRouter","text":"<p>We provide an interface for OpenRouter service. To use OpenRouter, sign up on their website and get an API key. For more details, refer to OpenRouter.</p> <p>In bash, save API key to the environmental variable <code>OPENROUTER_API_KEY</code>. <pre><code>export OPENROUTER_API_KEY=&lt;your_API_key&gt;\n</code></pre></p>"},{"location":"llm_inference_engine/#meta-llama-31-8b-instruct_1","title":"Meta-Llama-3.1-8B-Instruct","text":"<p>Define inference engine <pre><code>import os\nfrom llm_ie.engines import OpenRouterInferenceEngine\n\ninference_engine = OpenRouterInferenceEngine(model=\"meta-llama/llama-3.1-8b-instruct\")\n</code></pre> The <code>model</code> must match the repo name specified on OpenRouter.</p>"},{"location":"llm_inference_engine/#qwen3-30b-a3b-hybrid-thinking-mode_1","title":"Qwen3-30B-A3B (hybrid thinking mode)","text":"<p>Define inference engine <pre><code>from llm_ie.engines import OpenRouterInferenceEngine, Qwen3LLMConfig\n\n# Thinking mode\ninference_engine = OpenRouterInferenceEngine(model=\"qwen/qwen3-30b-a3b\", \n                                             config=Qwen3LLMConfig(thinking_mode=True, temperature=0.6, top_p=0.95, top_k=20))\n# Non-thinking mode\ninference_engine = OpenRouterInferenceEngine(model=\"qwen/qwen3-30b-a3b\", \n                                             config=Qwen3LLMConfig(thinking_mode=False, temperature=0.7, top_p=0.8, top_k=20))\n</code></pre></p>"},{"location":"llm_inference_engine/#qwen3-30b-thinking-2507_1","title":"Qwen3-30B-Thinking-2507","text":"<p>Define inference engine <pre><code>from llm_ie.engines import OpenRouterInferenceEngine, ReasoningLLMConfig\n\ninference_engine = OpenRouterInferenceEngine(model=\"qwen/qwen3-30b-a3b-thinking-2507\",\n                                             config=ReasoningLLMConfig(temperature=0.6, top_p=0.95, top_k=20))\n</code></pre></p>"},{"location":"llm_inference_engine/#gpt-oss-120b_1","title":"gpt-oss-120b","text":"<p>Define inference engine <pre><code>from llm_ie.engines import OpenRouterInferenceEngine, ReasoningLLMConfig\n\ninference_engine = OpenRouterInferenceEngine(model=\"openai/gpt-oss-120b\", \n                                             config=ReasoningLLMConfig(temperature=1.0, top_p=1.0, top_k=0))\n</code></pre></p>"},{"location":"llm_inference_engine/#openai-api-compatible-services_1","title":"OpenAI API &amp; Compatible Services","text":"<p>In bash, save API key to the environmental variable <code>OPENAI_API_KEY</code>. <pre><code>export OPENAI_API_KEY=&lt;your_API_key&gt;\n</code></pre></p> <p>In Python, create inference engine and specify model name. For the available models, refer to OpenAI webpage.  For more parameters, see OpenAI API reference.</p>"},{"location":"llm_inference_engine/#huggingface_hub","title":"Huggingface_hub","text":"<p>The <code>model</code> can be a model id hosted on the Hugging Face Hub or a URL to a deployed Inference Endpoint. Refer to the Inference Client documentation for more details. </p> <pre><code>from llm_ie.engines import HuggingFaceHubInferenceEngine\n\ninference_engine = HuggingFaceHubInferenceEngine(model=\"meta-llama/Meta-Llama-3-8B-Instruct\")\n</code></pre>"},{"location":"llm_inference_engine/#ollama","title":"Ollama","text":"<p>The <code>model_name</code> must match the names on the Ollama library. Use the command line <code>ollama ls</code> to check your local model list. <code>num_ctx</code> determines the context length LLM will consider during text generation. Empirically, longer context length gives better performance, while consuming more memory and increases computation. <code>keep_alive</code> regulates the lifespan of LLM. It indicates a number of seconds to hold the LLM after the last API call. Default is 5 minutes (300 sec).</p> <pre><code>from llm_ie.engines import OllamaInferenceEngine\n\ninference_engine = OllamaInferenceEngine(model_name=\"llama3.1:8b-instruct-q8_0\", num_ctx=4096, keep_alive=300)\n</code></pre>"},{"location":"llm_inference_engine/#llama-cpp-python","title":"Llama-cpp-python","text":"<p>The <code>repo_id</code> and <code>gguf_filename</code> must match the ones on the Huggingface repo to ensure the correct model is loaded. <code>n_ctx</code> determines the context length LLM will consider during text generation. Empirically, longer context length gives better performance, while consuming more memory and increases computation. Note that when <code>n_ctx</code> is less than the prompt length, Llama.cpp throws exceptions. <code>n_gpu_layers</code> indicates a number of model layers to offload to GPU. Default is -1 for all layers (entire LLM). Flash attention <code>flash_attn</code> is supported by Llama.cpp. The <code>verbose</code> indicates whether model information should be displayed. For more input parameters, see \ud83e\udd99 Llama-cpp-python. </p> <pre><code>from llm_ie.engines import LlamaCppInferenceEngine\n\ninference_engine = LlamaCppInferenceEngine(repo_id=\"bullerwins/Meta-Llama-3.1-8B-Instruct-GGUF\",\n                                           gguf_filename=\"Meta-Llama-3.1-8B-Instruct-Q8_0.gguf\",\n                                           n_ctx=4096,\n                                           n_gpu_layers=-1,\n                                           flash_attn=True,\n                                           verbose=False)\n</code></pre>"},{"location":"llm_inference_engine/#litellm","title":"LiteLLM","text":"<p>The LiteLLM is an adaptor project that unifies many proprietary and open-source LLM APIs. Popular inferncing servers, including OpenAI, Huggingface Hub, and Ollama are supported via its interface. For more details, refer to LiteLLM GitHub page. </p> <p>To use LiteLLM with LLM-IE, import the <code>LiteLLMInferenceEngine</code> and follow the required model naming. <pre><code>from llm_ie.engines import LiteLLMInferenceEngine\n\n# Huggingface serverless inferencing\nos.environ['HF_TOKEN']\ninference_engine = LiteLLMInferenceEngine(model=\"huggingface/meta-llama/Meta-Llama-3-8B-Instruct\")\n\n# OpenAI GPT models\nos.environ['OPENAI_API_KEY']\ninference_engine = LiteLLMInferenceEngine(model=\"openai/gpt-4o-mini\")\n\n# OpenAI compatible local server\ninference_engine = LiteLLMInferenceEngine(model=\"openai/Llama-3.1-8B-Instruct\", base_url=\"http://localhost:8000/v1\", api_key=\"EMPTY\")\n\n# Ollama \ninference_engine = LiteLLMInferenceEngine(model=\"ollama/llama3.1:8b-instruct-q8_0\")\n</code></pre></p>"},{"location":"llm_inference_engine/#test-inference-engine-configuration","title":"Test inference engine configuration","text":"<p>To test the inference engine, use the <code>chat()</code> method. </p> <p><pre><code>from llm_ie.engines import OllamaInferenceEngine\n\ninference_engine = OllamaInferenceEngine(model_name=\"llama3.1:8b-instruct-q8_0\")\ninference_engine.chat(messages=[{\"role\": \"user\", \"content\":\"Hi\"}], verbose=True)\n</code></pre> The output should be something like (might vary by LLMs and versions)</p> <pre><code>'How can I help you today?'\n</code></pre>"},{"location":"llm_inference_engine/#customize-inference-engine","title":"Customize inference engine","text":"<p>The abstract class <code>InferenceEngine</code> defines the interface and required method <code>chat()</code>. Inherit this class for customized API.  <pre><code>class InferenceEngine:\n    @abc.abstractmethod\n    def __init__(self, config:LLMConfig, **kwrs):\n        \"\"\"\n        This is an abstract class to provide interfaces for LLM inference engines. \n        Children classes that inherts this class can be used in extrators. Must implement chat() method.\n\n        Parameters:\n        ----------\n        config : LLMConfig\n            the LLM configuration. Must be a child class of LLMConfig.\n        \"\"\"\n        return NotImplemented\n\n\n    @abc.abstractmethod\n    def chat(self, messages:List[Dict[str,str]], \n             verbose:bool=False, stream:bool=False) -&gt; Union[str, Generator[Dict[str, str], None, None]]:\n        \"\"\"\n        This method inputs chat messages and outputs LLM generated text.\n\n        Parameters:\n        ----------\n        messages : List[Dict[str,str]]\n            a list of dict with role and content. role must be one of {\"system\", \"user\", \"assistant\"}\n        verbose : bool, Optional\n            if True, LLM generated text will be printed in terminal in real-time.\n        stream : bool, Optional\n            if True, returns a generator that yields the output in real-time.  \n        \"\"\"\n        return NotImplemented\n\n    def _format_config(self) -&gt; Dict[str, Any]:\n        \"\"\"\n        This method format the LLM configuration with the correct key for the inference engine. \n\n        Return : Dict[str, Any]\n            the config parameters.\n        \"\"\"\n        return NotImplemented\n</code></pre></p>"},{"location":"prompt_editor/","title":"Prompt Editor","text":"<p>The prompt editor is an LLM agent that help users write prompt templates following the defined schema and guideline of each extractor. It is recommanded to initiate prompt editors with larger/smarter LLMs for better quality!. </p>"},{"location":"prompt_editor/#access-prompt-editor-by-chat-interface","title":"Access Prompt Editor by chat interface","text":"<p>The easiest way to access the prompt editor is through the chat interface. This allows users to interact with the editor and graduately build up the prompt template. First, define an LLM inference engine and a prompt editor. Then, start a chat session by calling the <code>chat</code> method. A chat session will start. chat history (messages) can be accessed through the <code>messages</code> attribute. The <code>export_chat</code> method exports the chat history as a JSON file. The <code>import_chat</code> method imports the chat history from a JSON file. The <code>clear_chat</code> method clears the chat history.</p> <pre><code>from llm_ie import OllamaInferenceEngine, PromptEditor, DirectFrameExtractor\n\n# Define an LLM inference engine\ninference_engine = OllamaInferenceEngine(model_name=\"llama4:scout\")\n\n# Define editor\neditor = PromptEditor(inference_engine, DirectFrameExtractor)\n\n# Start chat session\neditor.chat()\n\n# After a conversation, you can view the chat history\nprint(editor.messages)\n\n# Export chat history as a JSON file\neditor.export_chat(\"&lt;your chat history file name&gt;.json\")\n\n# In a new session, you can load the chat history and continue\nnew_editor = PromptEditor(inference_engine, DirectFrameExtractor)\n\n# Import chat history from a JSON file\nnew_editor.import_chat(\"&lt;your chat history file name&gt;.json\")\n\n# Continue the chat\nnew_editor.chat()\n\n# To delete the chat history\neditor.clear_messages()\n</code></pre> <p>In a terminal environment, an interactive chat session will start: </p> <p>In the Jupyter/IPython environment, an ipywidgets session will start: </p>"},{"location":"prompt_editor/#prompt-editor-can-make-mistakes-always-check-the-output","title":"\u26a0\ufe0fPrompt Editor can make mistakes. Always check the output.","text":"<p>The Prompt Editor agent does not have access to documents that we are about to process unless included in the chat. Also, It does not have information about our purpose and backgournd of the request/project. Therefore, it is possible for Prompt Editor agent to propose suboptimal schema, provide incorrect examples, or generate irrelevant hints. It is very important that we check its output. It is a good practice to point out errors and ask Prompt Editor to revise it. Manual amendment is also recommended for minor issues. </p>"},{"location":"prompt_editor/#access-prompt-editor-by-python-functions","title":"Access Prompt Editor by Python functions","text":"<p>We can also use the <code>rewrite()</code> and <code>comment()</code> methods to programmingly interact with the prompt editor: </p> <ol> <li>start with a short description of the task</li> <li>have the prompt editor generate a prompt template as a starting point</li> <li>manually revise the prompt template</li> <li>have the prompt editor to comment/ rewrite it</li> </ol> <pre><code>from llm_ie import OllamaInferenceEngine, PromptEditor, DirectFrameExtractor\n\n# Define an LLM inference engine\ninference_engine = OllamaInferenceEngine(model_name=\"llama4:scout\")\n\n# Define editor\neditor = PromptEditor(inference_engine, DirectFrameExtractor)\n\n# Have editor to generate initial prompt template\ninitial_version = editor.rewrite(\"Extract treatment events from the discharge summary.\")\nprint(initial_version)\n</code></pre>"},{"location":"prompt_templates/","title":"Prompt Templates","text":"<p>A prompt template is a string with one or many placeholders <code>{{&lt;placeholder_name&gt;}}</code>. When input to an extractor, the <code>text_content</code> will be inserted into the placeholders to construct a prompt. Below is a demo:</p> <pre><code>### Task description\nThe paragraph below contains a clinical note with diagnoses listed. Please carefully review it and extract the diagnoses, including the diagnosis date and status.\n\n### Schema definition\nYour output should contain: \n    \"entity_text\" which is the name of the diagnosis spelled as it appears in the text,\n    \"Date\" which is the date when the diagnosis was made,\n    \"Status\" which is the current status of the diagnosis (e.g. active, resolved, etc.)\n\n### Output format definition\nYour output should follow JSON format, for example:\n[\n    {\"entity_text\": \"&lt;Diagnosis text&gt;\", \"attr\": {\"Date\": \"&lt;date in YYYY-MM-DD format&gt;\", \"Status\": \"&lt;status&gt;\"}},\n    {\"entity_text\": \"&lt;Diagnosis text&gt;\", \"attr\": {\"Date\": \"&lt;date in YYYY-MM-DD format&gt;\", \"Status\": \"&lt;status&gt;\"}}\n]\n\n### Additional hints\n- Your output should be 100% based on the provided content. DO NOT output fake information.\n- If there is no specific date or status, just omit those keys.\n\n### Context\nThe text below is from the clinical note:\n\"{{input}}\"\n</code></pre>"},{"location":"prompt_templates/#placeholder","title":"Placeholder","text":"<p>When only one placeholder is defined in the prompt template, the <code>text_content</code> can be a string or a dictionary with one key (regardless of the key name). When multiple placeholders are defined in the prompt template, the <code>text_content</code> should be a dictionary with:</p> <p><pre><code>{\"&lt;placeholder 1&gt;\": \"&lt;some text&gt;\", \"&lt;placeholder 2&gt;\": \"&lt;some text&gt;\"...}\n</code></pre> This is commonly used for injecting external knowledge, for example,</p> <pre><code>    Below is a medical note. Your task is to extract diagnosis information. \n\n    # Backgound knowledge\n    {{knowledge}}\n    Your output should include: \n        \"Diagnosis\": extract diagnosis names, \n        \"Datetime\": date/ time of diagnosis, \n        \"Status\": status of present, history, or family history\n\n    Your output should follow a JSON format:\n    [\n        {\"Diagnosis\": &lt;exact words as in the document&gt;, \"Datetime\": &lt;diagnosis datetime&gt;, \"Status\": &lt;one of \"present\", \"history\"&gt;},\n        {\"Diagnosis\": &lt;exact words as in the document&gt;, \"Datetime\": &lt;diagnosis datetime&gt;, \"Status\": &lt;one of \"present\", \"history\"&gt;},\n        ...\n    ]\n\n    Below is the medical note:\n    \"{{note}}\"\n</code></pre>"},{"location":"prompt_templates/#prompt-writing-guide","title":"Prompt writing guide","text":"<p>The quality of the prompt template can significantly impact the performance of information extraction. Also, the schema defined in prompt templates is dependent on the choice of extractors. When designing a prompt template schema, it is important to consider which extractor will be used. </p> <p>The <code>Extractor</code> class provides documentation and examples for prompt template writing. This is used by the Pormpt Editor. </p> <pre><code>from llm_ie.extractors import DirectFrameExtractor\n\nprint(DirectFrameExtractor.get_prompt_guide())\n</code></pre>"},{"location":"quick_start/","title":"Quick Start","text":"<p>We use a synthesized medical note by ChatGPT to demo the information extraction process. Our task is to extract diagnosis names, spans, and corresponding attributes (i.e., diagnosis datetime, status).</p> Synthesized Clinical Note <pre><code>### Discharge Summary Note\n\n**Patient Name:** John Doe  \n**Medical Record Number:** 12345678  \n**Date of Birth:** January 15, 1975  \n**Date of Admission:** July 20, 2024  \n**Date of Discharge:** July 27, 2024  \n\n**Attending Physician:** Dr. Jane Smith, MD  \n**Consulting Physicians:** Dr. Emily Brown, MD (Cardiology), Dr. Michael Green, MD (Pulmonology)\n\n#### Reason for Admission\nJohn Doe, a 49-year-old male, was admitted to the hospital with complaints of chest pain, shortness of breath, and dizziness. The patient has a history of hypertension, hyperlipidemia, and Type 2 diabetes mellitus.\n\n#### History of Present Illness\nThe patient reported that the chest pain started two days prior to admission. The pain was described as a pressure-like sensation in the central chest, radiating to the left arm and jaw. He also experienced dyspnea on exertion and occasional palpitations. The patient denied any recent upper respiratory infection, cough, or fever.\n\n#### Past Medical History\n- Hypertension (diagnosed in 2010)\n- Hyperlipidemia (diagnosed in 2015)\n- Type 2 Diabetes Mellitus (diagnosed in 2018)\n\n#### Social History\n- Former smoker (quit in 2010)\n- Occasional alcohol consumption\n- Works as an accountant\n- Married with two children\n\n#### Family History\n- Father: myocardial infarction at age 55\n- Mother: Type 2 diabetes mellitus\n\n#### Physical Examination\n- **Vital Signs:** Blood pressure 160/95 mmHg, heart rate 88 bpm, respiratory rate 20 breaths/min, temperature 98.6\u00b0F, oxygen saturation 96% on room air.\n- **General:** Alert and oriented, in mild distress.\n- **Cardiovascular:** Regular rhythm, no murmurs, rubs, or gallops. Jugular venous pressure not elevated.\n- **Respiratory:** Clear to auscultation bilaterally, no wheezes, rales, or rhonchi.\n- **Abdomen:** Soft, non-tender, no hepatosplenomegaly.\n- **Extremities:** No edema, pulses 2+ bilaterally.\n\n#### Laboratory and Diagnostic Tests\n- **EKG:** ST-segment depression in leads V4-V6.\n- **Troponin I:** Elevated at 0.15 ng/mL (normal &lt;0.04 ng/mL).\n- **Chest X-ray:** No acute infiltrates, normal cardiac silhouette.\n- **Echocardiogram:** Mild left ventricular hypertrophy, ejection fraction 55%.\n- **CBC:** WBC 8.5 x 10^3/uL, Hgb 13.5 g/dL, Platelets 250 x 10^3/uL.\n- **CMP:** Na 138 mmol/L, K 4.0 mmol/L, BUN 15 mg/dL, Creatinine 0.9 mg/dL, Glucose 180 mg/dL, HbA1c 7.8%.\n\n#### Hospital Course\nJohn Doe was diagnosed with acute coronary syndrome (ACS). He was started on dual antiplatelet therapy with aspirin and clopidogrel, along with high-dose atorvastatin, and a beta-blocker. A cardiology consultation was obtained, and the patient underwent coronary angiography, which revealed a 70% stenosis in the left anterior descending artery. A drug-eluting stent was placed successfully.\n\nPost-procedure, the patient was monitored in the coronary care unit. He remained hemodynamically stable, with no recurrent chest pain. He was gradually advanced to a regular cardiac diet and was ambulating without difficulty by day three of hospitalization. Diabetes management was optimized with the addition of metformin, and his blood pressure was controlled with the continuation of his antihypertensive regimen.\n\n#### Discharge Medications\n- Aspirin 81 mg daily\n- Clopidogrel 75 mg daily\n- Atorvastatin 40 mg daily\n- Metoprolol 50 mg twice daily\n- Lisinopril 20 mg daily\n- Metformin 1000 mg twice daily\n\n#### Discharge Instructions\nJohn Doe was advised to follow a heart-healthy diet, engage in regular physical activity, and monitor his blood glucose levels. He was instructed to avoid smoking and limit alcohol intake. Follow-up appointments were scheduled with his primary care physician, cardiologist, and endocrinologist.\n\nThe patient was educated on the signs and symptoms of recurrent chest pain and instructed to seek immediate medical attention if they occur. He was provided with a prescription for a nitroglycerin tablet to use as needed for chest pain.\n\n#### Follow-Up Appointments\n- Primary Care Physician: August 3, 2024\n- Cardiology: August 10, 2024\n- Endocrinology: August 17, 2024\n\n**Discharge Summary Prepared by:**  \nDr. Jane Smith, MD  \nJuly 27, 2024\n</code></pre>"},{"location":"quick_start/#choose-an-llm-inference-engine","title":"Choose an LLM inference engine","text":"<p>LLM-IE works with both local and remote LLM deployments. In this quick start demo, we use vLLM to run gpt-oss-120b for prompt engineering and for entity and attribute extraction. The outputs might be slightly different with other inference engines, LLMs, or quantization. To use other inference engines (e.g., Ollama, Huggingface Hub) and models (e.g., GPT-4.1, Qwen3), see LLM Inference Engine and LLM Configuration.</p>"},{"location":"quick_start/#prompt-engineering-by-chatting-with-llm-agent","title":"Prompt engineering by chatting with LLM agent","text":"<p>Start the server in command line. Specify <code>--reasoning-parser GptOss</code> to enable the reasoning parser.  <pre><code>vllm serve openai/gpt-oss-120b \\\n    --tensor-parallel-size 4 \\\n    --enable-prefix-caching \\\n    --reasoning-parser GptOss\n</code></pre></p> <pre><code>from llm_ie import VLLMInferenceEngine, ReasoningLLMConfig, DirectFrameExtractor, PromptEditor, SentenceUnitChunker, SlideWindowContextChunker\n\n# Define a LLM inference engine for the prompt editor\nprompt_editor_llm = VLLMInferenceEngine(model=\"openai/gpt-oss-120b\", \n                                        config=ReasoningLLMConfig(reasoning_effort=\"medium\", \n                                                                  temperature=1.0, \n                                                                  top_p=1.0, \n                                                                  max_new_tokens=8192))\n# Define LLM prompt editor\neditor = PromptEditor(prompt_editor_llm, DirectFrameExtractor)\n# Start chat\neditor.chat()\n</code></pre> <p>This opens an interactive session where we can chat with the Prompt Editor agent: </p> <p>The agent drafts a prompt template following the schema required by the <code>DirectFrameExtractor</code>. After a few rounds of chatting, we have a prompt template to start with: <pre><code>### Task description\nThe paragraph below contains a clinical note with diagnoses listed. Please carefully review it and extract the diagnoses, including the diagnosis date and status.\n\n### Schema definition\nYour output should contain: \n    \"entity_text\" which is the diagnosis spelled as it appears in the text,\n    \"Date\" which is the date when the diagnosis was made,\n    \"Status\" which is the current status of the diagnosis (e.g. active, resolved, etc.)\n\n### Output format definition\nYour output should follow JSON format, for example:\n[\n    {\"entity_text\": \"&lt;Diagnosis&gt;\", \"attr\": {\"Date\": \"&lt;date in YYYY-MM-DD format&gt;\", \"Status\": \"&lt;status&gt;\"}},\n    {\"entity_text\": \"&lt;Diagnosis&gt;\", \"attr\": {\"Date\": \"&lt;date in YYYY-MM-DD format&gt;\", \"Status\": \"&lt;status&gt;\"}}\n]\n\n### Additional hints\n- Your output should be 100% based on the provided content. DO NOT output fake information.\n- If there is no specific date or status, just omit those keys.\n\n### Context\nThe text below is from the clinical note:\n\"{{input}}\"\n</code></pre></p>"},{"location":"quick_start/#design-prompting-algorithm-for-information-extraction","title":"Design prompting algorithm for information extraction","text":"<p>Instead of prompting LLMs with the entire document (which, by our experiments, has worse performance), we divide the input document into units (e.g., sentences, text lines, paragraphs). LLM only focus on one unit at a time, before moving to the next unit. This is achieved by the <code>UnitChunker</code> classes. In this demo, we use <code>SentenceUnitChunker</code> for sentence-by-sentence prompting. LLM only focus on one sentence at a time. We supply a context, in this case, a slide window of 2 sentences as context. This provides LLM with additional information. This is achieved by the <code>SlideWindowContextChunker</code> class. To learn more about prompting algorithm, see Extractors. For information extraction, we use a lower reasoning effort (<code>reasoning_effort=\"low\"</code>) for higher throughput. We set <code>temperature=1.0</code> and <code>top_p=1.0</code> to following the recommendation.</p> <pre><code># Load synthesized medical note\nwith open(\"./demo/document/synthesized_note.txt\", 'r') as f:\n    note_text = f.read()\n\n# Define a LLM inference engine for the extractor\nextractor_llm = VLLMInferenceEngine(model=\"openai/gpt-oss-120b\", \n                                    config=ReasoningLLMConfig(reasoning_effort=\"low\", \n                                                              temperature=1.0, \n                                                              top_p=1.0, \n                                                              max_new_tokens=8192))\n# Define unit chunker. Prompts sentences-by-sentence.\nunit_chunker = SentenceUnitChunker()\n# Define context chunker. Provides context for units.\ncontext_chunker = SlideWindowContextChunker(window_size=2)\n# Define extractor\nextractor = DirectFrameExtractor(inference_engine=extractor_llm, \n                                 unit_chunker=unit_chunker,\n                                 context_chunker=context_chunker,\n                                 prompt_template=prompt_template)\n</code></pre> <p>To run the frame extraction, use <code>extract_frames</code> method. A list of entities with attributes (\"frames\") will be returned. Concurrent processing is supported by setting <code>concurrent=True</code>. <pre><code># To stream the extraction process, use concurrent=False, stream=True:\nframes, messages_log =  extractor.extract_frames(note_text, concurrent=False, verbose=True, return_messages_log=True)\n# For faster extraction, use concurrent=True to enable asynchronous prompting\n# frames, messages_log =  extractor.extract_frames(note_text, concurrent=True, return_messages_log=True)\n\n# Check extractions\nfor frame in frames:\n    print(frame.to_dict())\n</code></pre> The output is a list of frames. Each frame has a <code>entity_text</code>, <code>start</code>, <code>end</code>, and a dictionary of <code>attr</code>. </p> <pre><code>{'frame_id': '0', 'start': 537, 'end': 549, 'entity_text': 'hypertension', 'attr': {'Status': ''}}\n{'frame_id': '1', 'start': 551, 'end': 565, 'entity_text': 'hyperlipidemia', 'attr': {'Status': ''}}\n{'frame_id': '2', 'start': 571, 'end': 595, 'entity_text': 'Type 2 diabetes mellitus', 'attr': {'Status': ''}}\n{'frame_id': '3', 'start': 991, 'end': 1003, 'entity_text': 'Hypertension', 'attr': {'Date': '2010', 'Status': None}}\n{'frame_id': '4', 'start': 1026, 'end': 1040, 'entity_text': 'Hyperlipidemia', 'attr': {'Date': '2015', 'Status': None}}\n{'frame_id': '5', 'start': 1063, 'end': 1087, 'entity_text': 'Type 2 Diabetes Mellitus', 'attr': {'Date': '2018', 'Status': None}}\n{'frame_id': '6', 'start': 1646, 'end': 1682, 'entity_text': 'Jugular venous pressure not elevated', 'attr': {}}\n{'frame_id': '7', 'start': 1703, 'end': 1767, 'entity_text': 'Clear to auscultation bilaterally, no wheezes, rales, or rhonchi', 'attr': {}}\n{'frame_id': '8', 'start': 1802, 'end': 1823, 'entity_text': 'no hepatosplenomegaly', 'attr': {}}\n{'frame_id': '9', 'start': 1926, 'end': 1962, 'entity_text': 'ST-segment depression in leads V4-V6', 'attr': {}}\n{'frame_id': '10', 'start': 1982, 'end': 2004, 'entity_text': 'Elevated at 0.15 ng/mL', 'attr': {'Date': '', 'Status': ''}}\n{'frame_id': '11', 'start': 2046, 'end': 2066, 'entity_text': 'No acute infiltrates', 'attr': {}}\n{'frame_id': '12', 'start': 2068, 'end': 2093, 'entity_text': 'normal cardiac silhouette', 'attr': {}}\n{'frame_id': '13', 'start': 2117, 'end': 2150, 'entity_text': 'Mild left ventricular hypertrophy', 'attr': {'Date': '', 'Status': ''}}\n{'frame_id': '14', 'start': 2321, 'end': 2338, 'entity_text': 'Glucose 180 mg/dL', 'attr': {}}\n{'frame_id': '15', 'start': 2340, 'end': 2350, 'entity_text': 'HbA1c 7.8%', 'attr': {}}\n{'frame_id': '16', 'start': 2402, 'end': 2431, 'entity_text': 'acute coronary syndrome (ACS)', 'attr': {'Date': None, 'Status': None}}\n{'frame_id': '17', 'start': 3025, 'end': 3033, 'entity_text': 'Diabetes', 'attr': {}}\n{'frame_id': '18', 'start': 3925, 'end': 3935, 'entity_text': 'chest pain', 'attr': {'Date': '', 'Status': ''}}\n</code></pre>"},{"location":"quick_start/#data-management-and-visualization","title":"Data management and visualization","text":"<p>We can save the frames to a document object for better management. The document holds <code>text</code> and <code>frames</code>. The <code>add_frame()</code> method performs validation and (if passed) adds a frame to the document. The <code>valid_mode</code> controls how frame validation should be performed. For example, the <code>valid_mode = \"span\"</code> will prevent new frames from being added if the frame spans (<code>start</code>, <code>end</code>) has already exist. The <code>create_id = True</code> allows the document to assign unique frame IDs.  </p> <pre><code>from llm_ie.data_types import LLMInformationExtractionDocument\n\n# Define document\ndoc = LLMInformationExtractionDocument(doc_id=\"Synthesized medical note\",\n                                       text=note_text)\n# Add frames to a document\ndoc.add_frames(frames, create_id=True)\n\n# Save document to file (.llmie)\ndoc.save(\"&lt;your filename&gt;.llmie\")\n</code></pre> <p>To visualize the extracted frames, we use the <code>viz_serve()</code> method.  <pre><code>doc.viz_serve()\n</code></pre> A Flask App starts at port 5000 (default). To learn more about visualization, see Visualization.</p> <p></p>"},{"location":"visualization/","title":"Visualization","text":"<p>The <code>LLMInformationExtractionDocument</code> class supports named entity, entity attributes, and relation visualization. The implementation is through our plug-in package ie-viz. </p> <pre><code>pip install ie-viz\n</code></pre> <p>The <code>viz_serve()</code> method starts a Flask App on localhost port 5000 by default.  <pre><code>from llm_ie.data_types import LLMInformationExtractionDocument\n\n# Define document\ndoc = LLMInformationExtractionDocument(doc_id=\"Medical note\",\n                                       text=note_text)\n# Add extracted frames and relations to document\ndoc.add_frames(frames)\ndoc.add_relations(relations)\n# Visualize the document\ndoc.viz_serve()\n</code></pre></p> <p></p> <p>Alternatively, the <code>viz_render()</code> method returns a self-contained (HTML + JS + CSS) string. Save it to file and open with a browser. <pre><code>html = doc.viz_render()\n\nwith open(\"Medical note.html\", \"w\") as f:\n    f.write(html)\n</code></pre></p> <p>To customize colors for different entities, use <code>color_attr_key</code> (simple) or <code>color_map_func</code> (advanced). </p> <p>The <code>color_attr_key</code> automatically assign colors based on the specified attribute key. For example, \"EntityType\". <pre><code>doc.viz_serve(color_attr_key=\"EntityType\")\n</code></pre></p> <p>The <code>color_map_func</code> allow users to define a custom entity-color mapping function. For example, <pre><code>def color_map_func(entity) -&gt; str:\n    if entity['attr']['&lt;attribute key&gt;'] == \"&lt;a certain value&gt;\":\n        return \"#7f7f7f\"\n    else:\n        return \"#03A9F4\"\n\ndoc.viz_serve(color_map_func=color_map_func)\n</code></pre></p>"},{"location":"web_application/","title":"Web Application","text":"<p>We provide a drag-and-drop web Application for no-code access to the LLM-IE. The web Application streamlines the workflow:</p> <ol> <li>Prompt engineering with LLM agent: Prompt Editor Tab</li> <li>Prompting algorithm design: Frame extraction Tab</li> <li>Visualization &amp; Validation: Result viewer Tab</li> <li>Repeat step #1-#3 until achieves high accuracy</li> </ol>"},{"location":"web_application/#installation","title":"Installation","text":"<p>The image is available on Docker Hub. Use the command below to pull and run locally: <pre><code>docker pull daviden1013/llm-ie-web-app:latest\ndocker run -p 5000:5000 daviden1013/llm-ie-web-app:latest\n</code></pre></p>"},{"location":"web_application/#prompt-editor-tab","title":"Prompt Editor Tab","text":""},{"location":"web_application/#select-an-inference-api","title":"Select an inference API","text":"<p>Select from the dropdown (e.g., OpenAI, Azure, Ollama, Huggingface Hub) and specify an LLM. It is recomanded to select larger/smarter LLMs for better performance. Supply the API key, base URL, and deployment (for Azure) if needed. </p>"},{"location":"web_application/#start-chatting","title":"Start chatting","text":"<p>Describe your task and the Prompt Editor LLM agent behind the scene will help construct a structured prompt template. Feel free to ask the Prompt Editor questions, request it to revise, or ask it to add examples. </p>"},{"location":"web_application/#frame-extraction-tab","title":"Frame Extraction Tab","text":"<ul> <li> <p>Unit: a text snippet that LLM extrator will process at a time. It could be a sentence, a line of text, or a paragraph. </p> </li> <li>Context: the context around the unit. For exapmle, a slidewindow of 2 sentences before and after. Context is optional. </li> </ul> <p>See more details in Extractors</p> <ul> <li> </li> <li> </li> </ul> <p></p>"},{"location":"web_application/#select-an-inference-api_1","title":"Select an inference API","text":"<p>Select an inferencing API and specify an LLM. This model will perform the frame extraction task, so keep the balance of cost and performance in mind. If you have a lot of document to process, the API calls/cost can be high! </p>"},{"location":"web_application/#paste-your-document-and-prompt-temlpate","title":"Paste your document and prompt temlpate","text":"<p>In the \"Input Text\" textbox, paste the document text that you want to process. In the \"Prompt Template\" textbox, paste the prompt template you obtained from the previous step Prompt Editor Tab.</p>"},{"location":"web_application/#select-a-chunking-method","title":"Select a chunking method","text":"<p>Frame extraction adopts an unit-context schema. The purpose is to avoid having LLM to process long context and suffer from needle in the haystack challenge. We split an input document into multiple units. LLM only process a unit of text at a time. </p>"},{"location":"web_application/#watch-the-extraction-process","title":"Watch the extraction process","text":"<p>Click on the \"Start Extraction\" button and watch LLM processes unit-by-unit on the right panel. It is recommanded to monitor the process and look for errors.</p>"},{"location":"web_application/#download-results","title":"Download results","text":"<p>Use the download button on the top-right to download results. The resulting JSON will be saved as a <code>.llmie</code> file to your download folder. </p>"},{"location":"web_application/#result-viewer-tab","title":"Result Viewer Tab","text":""},{"location":"web_application/#upload-the-result-llmie-file","title":"Upload the result <code>.llmie</code> file.","text":"<p>Drop the result file from the previous step Frame extraction Tab to upload. </p>"},{"location":"web_application/#select-color-key-optional","title":"Select color key (Optional)","text":"<p>Optionally, select the attribute key in the dropdown for color coding.</p>"},{"location":"api/chunkers/","title":"Chunkers API","text":"<p>This module provides classes for splitting documents into manageable units for processing by LLMs and for providing context to those units.</p>"},{"location":"api/chunkers/#unit-chunkers","title":"Unit Chunkers","text":"<p>Unit chunkers determine how a document is divided into smaller pieces for frame extraction. Each piece is a <code>FrameExtractionUnit</code>.</p>"},{"location":"api/chunkers/#llm_ie.chunkers.UnitChunker","title":"llm_ie.chunkers.UnitChunker","text":"<pre><code>UnitChunker()\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>This is the abstract class for frame extraction unit chunker. It chunks a document into units (e.g., sentences). LLMs process unit by unit.</p> Source code in <code>package/llm-ie/src/llm_ie/chunkers.py</code> <pre><code>def __init__(self):\n    \"\"\"\n    This is the abstract class for frame extraction unit chunker.\n    It chunks a document into units (e.g., sentences). LLMs process unit by unit. \n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/chunkers/#llm_ie.chunkers.UnitChunker.chunk","title":"chunk  <code>abstractmethod</code>","text":"<pre><code>chunk(\n    text: str, doc_id: str = None\n) -&gt; List[FrameExtractionUnit]\n</code></pre> Parameters: <p>text : str     The document text.</p> Source code in <code>package/llm-ie/src/llm_ie/chunkers.py</code> <pre><code>@abc.abstractmethod\ndef chunk(self, text:str, doc_id:str=None) -&gt; List[FrameExtractionUnit]:\n    \"\"\"\n    Parameters:\n    ----------\n    text : str\n        The document text.\n    \"\"\"\n    return NotImplemented\n</code></pre>"},{"location":"api/chunkers/#llm_ie.chunkers.UnitChunker.chunk_async","title":"chunk_async  <code>async</code>","text":"<pre><code>chunk_async(\n    text: str, doc_id: str = None, executor=None\n) -&gt; List[FrameExtractionUnit]\n</code></pre> <p>asynchronous version of chunk method.</p> Source code in <code>package/llm-ie/src/llm_ie/chunkers.py</code> <pre><code>async def chunk_async(self, text:str, doc_id:str=None, executor=None) -&gt; List[FrameExtractionUnit]:\n    \"\"\"\n    asynchronous version of chunk method.\n    \"\"\"\n    loop = asyncio.get_running_loop()\n    return await loop.run_in_executor(executor, self.chunk, text, doc_id)\n</code></pre>"},{"location":"api/chunkers/#llm_ie.chunkers.WholeDocumentUnitChunker","title":"llm_ie.chunkers.WholeDocumentUnitChunker","text":"<pre><code>WholeDocumentUnitChunker()\n</code></pre> <p>               Bases: <code>UnitChunker</code></p> <p>This class chunks the whole document into a single unit (no chunking).</p> Source code in <code>package/llm-ie/src/llm_ie/chunkers.py</code> <pre><code>def __init__(self):\n    \"\"\"\n    This class chunks the whole document into a single unit (no chunking).\n    \"\"\"\n    super().__init__()\n</code></pre>"},{"location":"api/chunkers/#llm_ie.chunkers.WholeDocumentUnitChunker.chunk","title":"chunk","text":"<pre><code>chunk(\n    text: str, doc_id: str = None\n) -&gt; List[FrameExtractionUnit]\n</code></pre> Parameters: <p>text : str     The document text.</p> Source code in <code>package/llm-ie/src/llm_ie/chunkers.py</code> <pre><code>def chunk(self, text:str, doc_id:str=None) -&gt; List[FrameExtractionUnit]:\n    \"\"\"\n    Parameters:\n    ----------\n    text : str\n        The document text.\n    \"\"\"\n    return [FrameExtractionUnit(\n        doc_id=doc_id if doc_id is not None else str(uuid.uuid4()),\n        start=0,\n        end=len(text),\n        text=text\n    )]\n</code></pre>"},{"location":"api/chunkers/#llm_ie.chunkers.SentenceUnitChunker","title":"llm_ie.chunkers.SentenceUnitChunker","text":"<pre><code>SentenceUnitChunker()\n</code></pre> <p>               Bases: <code>UnitChunker</code></p> <p>This class uses the NLTK PunktSentenceTokenizer to chunk a document into sentences.</p> Source code in <code>package/llm-ie/src/llm_ie/chunkers.py</code> <pre><code>def __init__(self):\n    \"\"\"\n    This class uses the NLTK PunktSentenceTokenizer to chunk a document into sentences.\n    \"\"\"\n    super().__init__()\n</code></pre>"},{"location":"api/chunkers/#llm_ie.chunkers.SentenceUnitChunker.chunk","title":"chunk","text":"<pre><code>chunk(\n    text: str, doc_id: str = None\n) -&gt; List[FrameExtractionUnit]\n</code></pre> Parameters: <p>text : str     The document text.</p> Source code in <code>package/llm-ie/src/llm_ie/chunkers.py</code> <pre><code>def chunk(self, text:str, doc_id:str=None) -&gt; List[FrameExtractionUnit]:\n    \"\"\"\n    Parameters:\n    ----------\n    text : str\n        The document text.\n    \"\"\"\n    sentences = []\n    for start, end in self.PunktSentenceTokenizer().span_tokenize(text):\n        sentences.append(FrameExtractionUnit(\n            doc_id=doc_id if doc_id is not None else str(uuid.uuid4()),\n            start=start,\n            end=end,\n            text=text[start:end]\n        ))    \n    return sentences\n</code></pre>"},{"location":"api/chunkers/#llm_ie.chunkers.TextLineUnitChunker","title":"llm_ie.chunkers.TextLineUnitChunker","text":"<pre><code>TextLineUnitChunker()\n</code></pre> <p>               Bases: <code>UnitChunker</code></p> <p>This class chunks a document into lines.</p> Source code in <code>package/llm-ie/src/llm_ie/chunkers.py</code> <pre><code>def __init__(self):\n    \"\"\"\n    This class chunks a document into lines.\n    \"\"\"\n    super().__init__()\n</code></pre>"},{"location":"api/chunkers/#llm_ie.chunkers.TextLineUnitChunker.chunk","title":"chunk","text":"<pre><code>chunk(\n    text: str, doc_id: str = None\n) -&gt; List[FrameExtractionUnit]\n</code></pre> Parameters: <p>text : str     The document text.</p> Source code in <code>package/llm-ie/src/llm_ie/chunkers.py</code> <pre><code>def chunk(self, text:str, doc_id:str=None) -&gt; List[FrameExtractionUnit]:\n    \"\"\"\n    Parameters:\n    ----------\n    text : str\n        The document text.\n    \"\"\"\n    lines = text.split('\\n')\n    line_units = []\n    start = 0\n    for line in lines:\n        end = start + len(line)\n        line_units.append(FrameExtractionUnit(\n            doc_id=doc_id if doc_id is not None else str(uuid.uuid4()),\n            start=start,\n            end=end,\n            text=line\n        ))\n        start = end + 1 \n    return line_units\n</code></pre>"},{"location":"api/chunkers/#context-chunkers","title":"Context Chunkers","text":"<p>Context chunkers determine what contextual information is provided to the LLM alongside a specific <code>FrameExtractionUnit</code>.</p>"},{"location":"api/chunkers/#llm_ie.chunkers.ContextChunker","title":"llm_ie.chunkers.ContextChunker","text":"<pre><code>ContextChunker()\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>This is the abstract class for context chunker. Given a frame extraction unit, it returns the context for it.</p> Source code in <code>package/llm-ie/src/llm_ie/chunkers.py</code> <pre><code>def __init__(self):\n    \"\"\"\n    This is the abstract class for context chunker. Given a frame extraction unit,\n    it returns the context for it.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/chunkers/#llm_ie.chunkers.ContextChunker.fit","title":"fit  <code>abstractmethod</code>","text":"<pre><code>fit(text: str, units: List[FrameExtractionUnit])\n</code></pre> Parameters: <p>text : str     The document text.</p> Source code in <code>package/llm-ie/src/llm_ie/chunkers.py</code> <pre><code>@abc.abstractmethod\ndef fit(self, text:str, units:List[FrameExtractionUnit]):\n    \"\"\"\n    Parameters:\n    ----------\n    text : str\n        The document text.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/chunkers/#llm_ie.chunkers.ContextChunker.fit_async","title":"fit_async  <code>async</code>","text":"<pre><code>fit_async(\n    text: str,\n    units: List[FrameExtractionUnit],\n    executor=None,\n)\n</code></pre> <p>asynchronous version of fit method.</p> Source code in <code>package/llm-ie/src/llm_ie/chunkers.py</code> <pre><code>async def fit_async(self, text:str, units:List[FrameExtractionUnit], executor=None):\n    \"\"\"\n    asynchronous version of fit method.\n    \"\"\"\n    loop = asyncio.get_running_loop()\n    return await loop.run_in_executor(executor, self.fit, text, units)\n</code></pre>"},{"location":"api/chunkers/#llm_ie.chunkers.ContextChunker.chunk","title":"chunk  <code>abstractmethod</code>","text":"<pre><code>chunk(unit: FrameExtractionUnit) -&gt; str\n</code></pre> Parameters: <p>unit : FrameExtractionUnit     The frame extraction unit.</p> <p>Return : str      The context for the frame extraction unit.</p> Source code in <code>package/llm-ie/src/llm_ie/chunkers.py</code> <pre><code>@abc.abstractmethod\ndef chunk(self, unit:FrameExtractionUnit) -&gt; str:\n    \"\"\"\n    Parameters:\n    ----------\n    unit : FrameExtractionUnit\n        The frame extraction unit.\n\n    Return : str \n        The context for the frame extraction unit.\n    \"\"\"\n    return NotImplemented\n</code></pre>"},{"location":"api/chunkers/#llm_ie.chunkers.ContextChunker.chunk_async","title":"chunk_async  <code>async</code>","text":"<pre><code>chunk_async(\n    unit: FrameExtractionUnit, executor=None\n) -&gt; str\n</code></pre> <p>asynchronous version of chunk method.</p> Source code in <code>package/llm-ie/src/llm_ie/chunkers.py</code> <pre><code>async def chunk_async(self, unit:FrameExtractionUnit, executor=None) -&gt; str:\n    \"\"\"\n    asynchronous version of chunk method.\n    \"\"\"\n    loop = asyncio.get_running_loop()\n    return await loop.run_in_executor(executor, self.chunk, unit)\n</code></pre>"},{"location":"api/chunkers/#llm_ie.chunkers.NoContextChunker","title":"llm_ie.chunkers.NoContextChunker","text":"<pre><code>NoContextChunker()\n</code></pre> <p>               Bases: <code>ContextChunker</code></p> <p>This class does not provide any context.</p> Source code in <code>package/llm-ie/src/llm_ie/chunkers.py</code> <pre><code>def __init__(self):\n    \"\"\"\n    This class does not provide any context.\n    \"\"\"\n    super().__init__()\n</code></pre>"},{"location":"api/chunkers/#llm_ie.chunkers.NoContextChunker.fit","title":"fit","text":"<pre><code>fit(text: str, units: List[FrameExtractionUnit])\n</code></pre> Parameters: <p>text : str     The document text.</p> Source code in <code>package/llm-ie/src/llm_ie/chunkers.py</code> <pre><code>def fit(self, text:str, units:List[FrameExtractionUnit]):\n    \"\"\"\n    Parameters:\n    ----------\n    text : str\n        The document text.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/chunkers/#llm_ie.chunkers.WholeDocumentContextChunker","title":"llm_ie.chunkers.WholeDocumentContextChunker","text":"<pre><code>WholeDocumentContextChunker()\n</code></pre> <p>               Bases: <code>ContextChunker</code></p> <p>This class provides the whole document as context.</p> Source code in <code>package/llm-ie/src/llm_ie/chunkers.py</code> <pre><code>def __init__(self):\n    \"\"\"\n    This class provides the whole document as context.\n    \"\"\"\n    super().__init__()\n    self.text = None\n</code></pre>"},{"location":"api/chunkers/#llm_ie.chunkers.WholeDocumentContextChunker.fit","title":"fit","text":"<pre><code>fit(text: str, units: List[FrameExtractionUnit])\n</code></pre> Parameters: <p>text : str     The document text.</p> Source code in <code>package/llm-ie/src/llm_ie/chunkers.py</code> <pre><code>def fit(self, text:str, units:List[FrameExtractionUnit]):\n    \"\"\"\n    Parameters:\n    ----------\n    text : str\n        The document text.\n    \"\"\"\n    self.text = text\n</code></pre>"},{"location":"api/chunkers/#llm_ie.chunkers.SlideWindowContextChunker","title":"llm_ie.chunkers.SlideWindowContextChunker","text":"<pre><code>SlideWindowContextChunker(window_size: int)\n</code></pre> <p>               Bases: <code>ContextChunker</code></p> <p>This class provides a sliding window context. For example, +-2 sentences around a unit sentence.</p> Source code in <code>package/llm-ie/src/llm_ie/chunkers.py</code> <pre><code>def __init__(self, window_size:int):\n    \"\"\"\n    This class provides a sliding window context. For example, +-2 sentences around a unit sentence. \n    \"\"\"\n    super().__init__()\n    self.window_size = window_size\n    self.units = None\n</code></pre>"},{"location":"api/chunkers/#llm_ie.chunkers.SlideWindowContextChunker.fit","title":"fit","text":"<pre><code>fit(text: str, units: List[FrameExtractionUnit])\n</code></pre> Parameters: <p>units : List[FrameExtractionUnit]     The list of frame extraction units.</p> Source code in <code>package/llm-ie/src/llm_ie/chunkers.py</code> <pre><code>def fit(self, text:str, units:List[FrameExtractionUnit]):\n    \"\"\"\n    Parameters:\n    ----------\n    units : List[FrameExtractionUnit]\n        The list of frame extraction units.\n    \"\"\"\n    self.units = sorted(units)\n</code></pre>"},{"location":"api/extractors/","title":"Extractors API","text":""},{"location":"api/extractors/#frame-extractors","title":"Frame Extractors","text":""},{"location":"api/extractors/#llm_ie.extractors.DirectFrameExtractor","title":"llm_ie.extractors.DirectFrameExtractor","text":"<pre><code>DirectFrameExtractor(\n    inference_engine: InferenceEngine,\n    unit_chunker: UnitChunker,\n    prompt_template: str,\n    system_prompt: str = None,\n    context_chunker: ContextChunker = None,\n)\n</code></pre> <p>               Bases: <code>FrameExtractor</code></p> <p>This class is for general unit-context frame extraction. Input LLM inference engine, system prompt (optional), prompt template (with instruction, few-shot examples).</p> Parameters: <p>inference_engine : InferenceEngine     the LLM inferencing engine object. Must implements the chat() method. unit_chunker : UnitChunker     the unit chunker object that determines how to chunk the document text into units. prompt_template : str     prompt template with \"{{}}\" placeholder. system_prompt : str, Optional     system prompt. context_chunker : ContextChunker     the context chunker object that determines how to get context for each unit. Source code in <code>package/llm-ie/src/llm_ie/extractors.py</code> <pre><code>def __init__(self, inference_engine:InferenceEngine, unit_chunker:UnitChunker, \n             prompt_template:str, system_prompt:str=None, context_chunker:ContextChunker=None):\n    \"\"\"\n    This class is for general unit-context frame extraction.\n    Input LLM inference engine, system prompt (optional), prompt template (with instruction, few-shot examples).\n\n    Parameters:\n    ----------\n    inference_engine : InferenceEngine\n        the LLM inferencing engine object. Must implements the chat() method.\n    unit_chunker : UnitChunker\n        the unit chunker object that determines how to chunk the document text into units.\n    prompt_template : str\n        prompt template with \"{{&lt;placeholder name&gt;}}\" placeholder.\n    system_prompt : str, Optional\n        system prompt.\n    context_chunker : ContextChunker\n        the context chunker object that determines how to get context for each unit.\n    \"\"\"\n    super().__init__(inference_engine=inference_engine,\n                     unit_chunker=unit_chunker,\n                     prompt_template=prompt_template,\n                     system_prompt=system_prompt,\n                     context_chunker=context_chunker)\n</code></pre>"},{"location":"api/extractors/#llm_ie.extractors.DirectFrameExtractor.extract","title":"extract","text":"<pre><code>extract(\n    text_content: Union[str, Dict[str, str]],\n    document_key: str = None,\n    verbose: bool = False,\n    return_messages_log: bool = False,\n) -&gt; List[FrameExtractionUnit]\n</code></pre> <p>This method inputs a text and outputs a list of outputs per unit.</p> Parameters: <p>text_content : Union[str, Dict[str,str]]     the input text content to put in prompt template.      If str, the prompt template must has only 1 placeholder {{}}, regardless of placeholder name.     If dict, all the keys must be included in the prompt template placeholder {{}}. document_key : str, Optional     specify the key in text_content where document text is.      If text_content is str, this parameter will be ignored. verbose : bool, Optional     if True, LLM generated text will be printed in terminal in real-time.  return_messages_log : bool, Optional     if True, a list of messages will be returned. <p>Return : List[FrameExtractionUnit]     the output from LLM for each unit. Contains the start, end, text, and generated text.</p> Source code in <code>package/llm-ie/src/llm_ie/extractors.py</code> <pre><code>def extract(self, text_content:Union[str, Dict[str,str]], \n            document_key:str=None, verbose:bool=False, return_messages_log:bool=False) -&gt; List[FrameExtractionUnit]:\n    \"\"\"\n    This method inputs a text and outputs a list of outputs per unit.\n\n    Parameters:\n    ----------\n    text_content : Union[str, Dict[str,str]]\n        the input text content to put in prompt template. \n        If str, the prompt template must has only 1 placeholder {{&lt;placeholder name&gt;}}, regardless of placeholder name.\n        If dict, all the keys must be included in the prompt template placeholder {{&lt;placeholder name&gt;}}.\n    document_key : str, Optional\n        specify the key in text_content where document text is. \n        If text_content is str, this parameter will be ignored.\n    verbose : bool, Optional\n        if True, LLM generated text will be printed in terminal in real-time. \n    return_messages_log : bool, Optional\n        if True, a list of messages will be returned.\n\n    Return : List[FrameExtractionUnit]\n        the output from LLM for each unit. Contains the start, end, text, and generated text.\n    \"\"\"\n    # unit chunking\n    if isinstance(text_content, str):\n        doc_text = text_content\n\n    elif isinstance(text_content, dict):\n        if document_key is None:\n            raise ValueError(\"document_key must be provided when text_content is dict.\")\n        doc_text = text_content[document_key]\n\n    units = self.unit_chunker.chunk(doc_text)\n    # context chunker init\n    self.context_chunker.fit(doc_text, units)\n\n    # messages log\n    messages_logger = MessagesLogger() if return_messages_log else None\n\n    # generate unit by unit\n    for i, unit in enumerate(units):\n        try:\n            # construct chat messages\n            messages = []\n            if self.system_prompt:\n                messages.append({'role': 'system', 'content': self.system_prompt})\n\n            context = self.context_chunker.chunk(unit)\n\n            if context == \"\":\n                # no context, just place unit in user prompt\n                if isinstance(text_content, str):\n                    messages.append({'role': 'user', 'content': self._get_user_prompt(unit.text)})\n                else:\n                    unit_content = text_content.copy()\n                    unit_content[document_key] = unit.text\n                    messages.append({'role': 'user', 'content': self._get_user_prompt(unit_content)})\n            else:\n                # insert context to user prompt\n                if isinstance(text_content, str):\n                    messages.append({'role': 'user', 'content': self._get_user_prompt(context)})\n                else:\n                    context_content = text_content.copy()\n                    context_content[document_key] = context\n                    messages.append({'role': 'user', 'content': self._get_user_prompt(context_content)})\n                # simulate conversation where assistant confirms\n                messages.append({'role': 'assistant', 'content': 'Sure, please provide the unit text (e.g., sentence, line, chunk) of interest.'})\n                # place unit of interest\n                messages.append({'role': 'user', 'content': unit.text})\n\n            if verbose:\n                print(f\"\\n\\n{Fore.GREEN}Unit {i + 1}/{len(units)}:{Style.RESET_ALL}\\n{unit.text}\\n\")\n                if context != \"\":\n                    print(f\"{Fore.YELLOW}Context:{Style.RESET_ALL}\\n{context}\\n\")\n\n                print(f\"{Fore.BLUE}Extraction:{Style.RESET_ALL}\")\n\n\n            gen_text = self.inference_engine.chat(\n                            messages=messages, \n                            verbose=verbose,\n                            stream=False,\n                            messages_logger=messages_logger\n                        )\n\n            # add generated text to unit\n            unit.set_generated_text(gen_text[\"response\"])\n            unit.set_status(\"success\")\n        except Exception as e:\n            unit.set_status(\"fail\")\n            warnings.warn(f\"LLM inference failed for unit {i} ({unit.start}, {unit.end}): {e}\", RuntimeWarning)\n\n    if return_messages_log:\n        return units, messages_logger.get_messages_log()\n\n    return units\n</code></pre>"},{"location":"api/extractors/#llm_ie.extractors.DirectFrameExtractor.stream","title":"stream","text":"<pre><code>stream(\n    text_content: Union[str, Dict[str, str]],\n    document_key: str = None,\n) -&gt; Generator[\n    Dict[str, Any], None, List[FrameExtractionUnit]\n]\n</code></pre> <p>Streams LLM responses per unit with structured event types, and returns collected data for post-processing.</p> Yields: <p>Dict[str, Any]: (type, data)     - {\"type\": \"info\", \"data\": str_message}: General informational messages.     - {\"type\": \"unit\", \"data\": dict_unit_info}: Signals start of a new unit. dict_unit_info contains {'id', 'text', 'start', 'end'}     - {\"type\": \"context\", \"data\": str_context}: Context string for the current unit.     - {\"type\": \"reasoning\", \"data\": str_chunk}: A reasoning model thinking chunk from the LLM.     - {\"type\": \"response\", \"data\": str_chunk}: A response/answer chunk from the LLM.</p> Returns: <p>List[FrameExtractionUnit]:     A list of FrameExtractionUnit objects, each containing the     original unit details and the fully accumulated 'gen_text' from the LLM.</p> Source code in <code>package/llm-ie/src/llm_ie/extractors.py</code> <pre><code>def stream(self, text_content: Union[str, Dict[str, str]], \n           document_key: str = None) -&gt; Generator[Dict[str, Any], None, List[FrameExtractionUnit]]:\n    \"\"\"\n    Streams LLM responses per unit with structured event types,\n    and returns collected data for post-processing.\n\n    Yields:\n    -------\n    Dict[str, Any]: (type, data)\n        - {\"type\": \"info\", \"data\": str_message}: General informational messages.\n        - {\"type\": \"unit\", \"data\": dict_unit_info}: Signals start of a new unit. dict_unit_info contains {'id', 'text', 'start', 'end'}\n        - {\"type\": \"context\", \"data\": str_context}: Context string for the current unit.\n        - {\"type\": \"reasoning\", \"data\": str_chunk}: A reasoning model thinking chunk from the LLM.\n        - {\"type\": \"response\", \"data\": str_chunk}: A response/answer chunk from the LLM.\n\n    Returns:\n    --------\n    List[FrameExtractionUnit]:\n        A list of FrameExtractionUnit objects, each containing the\n        original unit details and the fully accumulated 'gen_text' from the LLM.\n    \"\"\"\n    if isinstance(text_content, str):\n        doc_text = text_content\n    elif isinstance(text_content, dict):\n        if document_key is None:\n            raise ValueError(\"document_key must be provided when text_content is dict.\")\n        if document_key not in text_content:\n            raise ValueError(f\"document_key '{document_key}' not found in text_content.\")\n        doc_text = text_content[document_key]\n    else:\n        raise TypeError(\"text_content must be a string or a dictionary.\")\n\n    units: List[FrameExtractionUnit] = self.unit_chunker.chunk(doc_text)\n    self.context_chunker.fit(doc_text, units)\n\n    yield {\"type\": \"info\", \"data\": f\"Starting LLM processing for {len(units)} units.\"}\n\n    for i, unit in enumerate(units):\n        unit_info_payload = {\"id\": i, \"text\": unit.text, \"start\": unit.start, \"end\": unit.end}\n        yield {\"type\": \"unit\", \"data\": unit_info_payload}\n\n        messages = []\n        if self.system_prompt:\n            messages.append({'role': 'system', 'content': self.system_prompt})\n\n        context_str = self.context_chunker.chunk(unit)\n\n        # Construct prompt input based on whether text_content was str or dict\n        if context_str:\n            yield {\"type\": \"context\", \"data\": context_str}\n            prompt_input_for_context = context_str\n            if isinstance(text_content, dict):\n                context_content_dict = text_content.copy()\n                context_content_dict[document_key] = context_str\n                prompt_input_for_context = context_content_dict\n            messages.append({'role': 'user', 'content': self._get_user_prompt(prompt_input_for_context)})\n            messages.append({'role': 'assistant', 'content': 'Sure, please provide the unit text (e.g., sentence, line, chunk) of interest.'})\n            messages.append({'role': 'user', 'content': unit.text})\n        else: # No context\n            prompt_input_for_unit = unit.text\n            if isinstance(text_content, dict):\n                unit_content_dict = text_content.copy()\n                unit_content_dict[document_key] = unit.text\n                prompt_input_for_unit = unit_content_dict\n            messages.append({'role': 'user', 'content': self._get_user_prompt(prompt_input_for_unit)})\n\n        current_gen_text = \"\"\n\n        response_stream = self.inference_engine.chat(\n            messages=messages,\n            stream=True\n        )\n        for chunk in response_stream:\n            yield chunk\n            if chunk[\"type\"] == \"response\":\n                current_gen_text += chunk[\"data\"]\n\n        # Store the result for this unit\n        unit.set_generated_text(current_gen_text)\n        unit.set_status(\"success\")\n\n    yield {\"type\": \"info\", \"data\": \"All units processed by LLM.\"}\n    return units\n</code></pre>"},{"location":"api/extractors/#llm_ie.extractors.DirectFrameExtractor.extract_async","title":"extract_async  <code>async</code>","text":"<pre><code>extract_async(\n    text_content: Union[str, Dict[str, str]],\n    document_key: str = None,\n    concurrent_batch_size: int = 32,\n    return_messages_log: bool = False,\n) -&gt; List[FrameExtractionUnit]\n</code></pre> <p>This is the asynchronous version of the extract() method.</p> Parameters: <p>text_content : Union[str, Dict[str,str]]     the input text content to put in prompt template.      If str, the prompt template must has only 1 placeholder {{}}, regardless of placeholder name.     If dict, all the keys must be included in the prompt template placeholder {{}}. document_key : str, Optional     specify the key in text_content where document text is.      If text_content is str, this parameter will be ignored. concurrent_batch_size : int, Optional     the batch size for concurrent processing.  return_messages_log : bool, Optional     if True, a list of messages will be returned. <p>Return : List[FrameExtractionUnit]     the output from LLM for each unit. Contains the start, end, text, and generated text.</p> Source code in <code>package/llm-ie/src/llm_ie/extractors.py</code> <pre><code>async def extract_async(self, text_content:Union[str, Dict[str,str]], document_key:str=None, \n                        concurrent_batch_size:int=32, return_messages_log:bool=False) -&gt; List[FrameExtractionUnit]:\n    \"\"\"\n    This is the asynchronous version of the extract() method.\n\n    Parameters:\n    ----------\n    text_content : Union[str, Dict[str,str]]\n        the input text content to put in prompt template. \n        If str, the prompt template must has only 1 placeholder {{&lt;placeholder name&gt;}}, regardless of placeholder name.\n        If dict, all the keys must be included in the prompt template placeholder {{&lt;placeholder name&gt;}}.\n    document_key : str, Optional\n        specify the key in text_content where document text is. \n        If text_content is str, this parameter will be ignored.\n    concurrent_batch_size : int, Optional\n        the batch size for concurrent processing. \n    return_messages_log : bool, Optional\n        if True, a list of messages will be returned.\n\n    Return : List[FrameExtractionUnit]\n        the output from LLM for each unit. Contains the start, end, text, and generated text.\n    \"\"\"\n    if isinstance(text_content, str):\n        doc_text = text_content\n    elif isinstance(text_content, dict):\n        if document_key is None:\n            raise ValueError(\"document_key must be provided when text_content is dict.\")\n        if document_key not in text_content:\n             raise ValueError(f\"document_key '{document_key}' not found in text_content dictionary.\")\n        doc_text = text_content[document_key]\n    else:\n        raise TypeError(\"text_content must be a string or a dictionary.\")\n\n    units = self.unit_chunker.chunk(doc_text)\n\n    # context chunker init \n    self.context_chunker.fit(doc_text, units)\n\n    # messages logger init\n    messages_logger = MessagesLogger() if return_messages_log else None\n\n    # Prepare inputs for all units first\n    tasks_input = []\n    for i, unit in enumerate(units):\n        # construct chat messages\n        messages = []\n        if self.system_prompt:\n            messages.append({'role': 'system', 'content': self.system_prompt})\n\n        context = self.context_chunker.chunk(unit)\n\n        if context == \"\":\n             # no context, just place unit in user prompt\n            if isinstance(text_content, str):\n                messages.append({'role': 'user', 'content': self._get_user_prompt(unit.text)})\n            else:\n                unit_content = text_content.copy()\n                unit_content[document_key] = unit.text\n                messages.append({'role': 'user', 'content': self._get_user_prompt(unit_content)})\n        else:\n            # insert context to user prompt\n            if isinstance(text_content, str):\n                messages.append({'role': 'user', 'content': self._get_user_prompt(context)})\n            else:\n                context_content = text_content.copy()\n                context_content[document_key] = context\n                messages.append({'role': 'user', 'content': self._get_user_prompt(context_content)})\n            # simulate conversation where assistant confirms\n            messages.append({'role': 'assistant', 'content': 'Sure, please provide the unit text (e.g., sentence, line, chunk) of interest.'})\n            # place unit of interest\n            messages.append({'role': 'user', 'content': unit.text})\n\n        # Store unit and messages together for the task\n        tasks_input.append({\"unit\": unit, \"messages\": messages, \"original_index\": i})\n\n    # Process units concurrently with asyncio.Semaphore\n    semaphore = asyncio.Semaphore(concurrent_batch_size)\n\n    async def semaphore_helper(task_data: Dict, **kwrs):\n        unit = task_data[\"unit\"]\n        messages = task_data[\"messages\"]\n\n        async with semaphore:\n            gen_text = await self.inference_engine.chat_async(\n                messages=messages,\n                messages_logger=messages_logger\n            )\n\n        unit.set_generated_text(gen_text[\"response\"])\n        unit.set_status(\"success\")\n\n    # Create and gather tasks\n    tasks = []\n    for task_inp in tasks_input:\n        task = asyncio.create_task(semaphore_helper(\n            task_inp\n        ))\n        tasks.append(task)\n\n    await asyncio.gather(*tasks)\n\n    # Return units\n    if return_messages_log:\n        return units, messages_logger.get_messages_log()\n    else:\n        return units\n</code></pre>"},{"location":"api/extractors/#llm_ie.extractors.DirectFrameExtractor.extract_frames","title":"extract_frames","text":"<pre><code>extract_frames(\n    text_content: Union[str, Dict[str, str]],\n    document_key: str = None,\n    verbose: bool = False,\n    concurrent: bool = False,\n    concurrent_batch_size: int = 32,\n    case_sensitive: bool = False,\n    fuzzy_match: bool = True,\n    fuzzy_buffer_size: float = 0.2,\n    fuzzy_score_cutoff: float = 0.8,\n    allow_overlap_entities: bool = False,\n    return_messages_log: bool = False,\n) -&gt; List[LLMInformationExtractionFrame]\n</code></pre> <p>This method inputs a document text and outputs a list of LLMInformationExtractionFrame It use the extract() method and post-process outputs into frames.</p> Parameters: <p>text_content : Union[str, Dict[str,str]]     the input text content to put in prompt template.      If str, the prompt template must has only 1 placeholder {{}}, regardless of placeholder name.     If dict, all the keys must be included in the prompt template placeholder {{}}. document_key : str, Optional     specify the key in text_content where document text is.      If text_content is str, this parameter will be ignored. verbose : bool, Optional     if True, LLM generated text will be printed in terminal in real-time.  concurrent : bool, Optional     if True, the sentences will be extracted in concurrent. concurrent_batch_size : int, Optional     the number of sentences to process in concurrent. Only used when <code>concurrent</code> is True. case_sensitive : bool, Optional     if True, entity text matching will be case-sensitive. fuzzy_match : bool, Optional     if True, fuzzy matching will be applied to find entity text. fuzzy_buffer_size : float, Optional     the buffer size for fuzzy matching. Default is 20% of entity text length. fuzzy_score_cutoff : float, Optional     the Jaccard score cutoff for fuzzy matching.      Matched entity text must have a score higher than this value or a None will be returned. allow_overlap_entities : bool, Optional     if True, entities can overlap in the text.      Note that this can cause multiple frames to be generated on the same entity span if they have same entity text. return_messages_log : bool, Optional     if True, a list of messages will be returned. <p>Return : str     a list of frames.</p> Source code in <code>package/llm-ie/src/llm_ie/extractors.py</code> <pre><code>def extract_frames(self, text_content:Union[str, Dict[str,str]], document_key:str=None, \n                   verbose:bool=False, concurrent:bool=False, concurrent_batch_size:int=32,\n                    case_sensitive:bool=False, fuzzy_match:bool=True, fuzzy_buffer_size:float=0.2, fuzzy_score_cutoff:float=0.8,\n                    allow_overlap_entities:bool=False, return_messages_log:bool=False) -&gt; List[LLMInformationExtractionFrame]:\n    \"\"\"\n    This method inputs a document text and outputs a list of LLMInformationExtractionFrame\n    It use the extract() method and post-process outputs into frames.\n\n    Parameters:\n    ----------\n    text_content : Union[str, Dict[str,str]]\n        the input text content to put in prompt template. \n        If str, the prompt template must has only 1 placeholder {{&lt;placeholder name&gt;}}, regardless of placeholder name.\n        If dict, all the keys must be included in the prompt template placeholder {{&lt;placeholder name&gt;}}.\n    document_key : str, Optional\n        specify the key in text_content where document text is. \n        If text_content is str, this parameter will be ignored.\n    verbose : bool, Optional\n        if True, LLM generated text will be printed in terminal in real-time. \n    concurrent : bool, Optional\n        if True, the sentences will be extracted in concurrent.\n    concurrent_batch_size : int, Optional\n        the number of sentences to process in concurrent. Only used when `concurrent` is True.\n    case_sensitive : bool, Optional\n        if True, entity text matching will be case-sensitive.\n    fuzzy_match : bool, Optional\n        if True, fuzzy matching will be applied to find entity text.\n    fuzzy_buffer_size : float, Optional\n        the buffer size for fuzzy matching. Default is 20% of entity text length.\n    fuzzy_score_cutoff : float, Optional\n        the Jaccard score cutoff for fuzzy matching. \n        Matched entity text must have a score higher than this value or a None will be returned.\n    allow_overlap_entities : bool, Optional\n        if True, entities can overlap in the text. \n        Note that this can cause multiple frames to be generated on the same entity span if they have same entity text.\n    return_messages_log : bool, Optional\n        if True, a list of messages will be returned.\n\n    Return : str\n        a list of frames.\n    \"\"\"\n    ENTITY_KEY = \"entity_text\"\n    if concurrent:\n        if verbose:\n            warnings.warn(\"verbose=True is not supported in concurrent mode.\", RuntimeWarning)\n\n        nest_asyncio.apply() # For Jupyter notebook. Terminal does not need this.\n        extraction_results = asyncio.run(self.extract_async(text_content=text_content, \n                                            document_key=document_key,\n                                            concurrent_batch_size=concurrent_batch_size,\n                                            return_messages_log=return_messages_log)\n                                        )\n    else:\n        extraction_results = self.extract(text_content=text_content, \n                                            document_key=document_key,\n                                            verbose=verbose,\n                                            return_messages_log=return_messages_log)\n\n    units, messages_log = extraction_results if return_messages_log else (extraction_results, None)\n\n    frame_list = []\n    for unit in units:\n        entity_json = []\n        if unit.status != \"success\":\n            warnings.warn(f\"Skipping failed unit ({unit.start}, {unit.end}): {unit.text}\", RuntimeWarning)\n            continue\n        for entity in self._extract_json(gen_text=unit.gen_text):\n            if ENTITY_KEY in entity:\n                entity_json.append(entity)\n            else:\n                warnings.warn(f'Extractor output \"{entity}\" does not have entity_key (\"{ENTITY_KEY}\"). This frame will be dropped.', RuntimeWarning)\n\n        spans = self._find_entity_spans(text=unit.text, \n                                        entities=[e[ENTITY_KEY] for e in entity_json], \n                                        case_sensitive=case_sensitive,\n                                        fuzzy_match=fuzzy_match,\n                                        fuzzy_buffer_size=fuzzy_buffer_size,\n                                        fuzzy_score_cutoff=fuzzy_score_cutoff,\n                                        allow_overlap_entities=allow_overlap_entities)\n        for ent, span in zip(entity_json, spans):\n            if span is not None:\n                start, end = span\n                entity_text = unit.text[start:end]\n                start += unit.start\n                end += unit.start\n                attr = {}\n                if \"attr\" in ent and ent[\"attr\"] is not None:\n                    attr = ent[\"attr\"]\n\n                frame = LLMInformationExtractionFrame(frame_id=f\"{len(frame_list)}\", \n                            start=start,\n                            end=end,\n                            entity_text=entity_text,\n                            attr=attr)\n                frame_list.append(frame)\n\n    if return_messages_log:\n        return frame_list, messages_log\n    return frame_list\n</code></pre>"},{"location":"api/extractors/#llm_ie.extractors.DirectFrameExtractor.extract_frames_from_documents","title":"extract_frames_from_documents  <code>async</code>","text":"<pre><code>extract_frames_from_documents(\n    text_contents: List[Union[str, Dict[str, any]]],\n    document_key: str = \"text\",\n    cpu_concurrency: int = 4,\n    llm_concurrency: int = 32,\n    case_sensitive: bool = False,\n    fuzzy_match: bool = True,\n    fuzzy_buffer_size: float = 0.2,\n    fuzzy_score_cutoff: float = 0.8,\n    allow_overlap_entities: bool = False,\n    return_messages_log: bool = False,\n) -&gt; AsyncGenerator[Dict[str, any], None]\n</code></pre> <p>This method inputs a list of documents and yields the results for each document as soon as it is complete.</p> Parameters: <p>text_contents : List[Union[str,Dict[str, any]]]     a list of input text contents to put in prompt template.      If str, the prompt template must has only 1 placeholder {{}}, regardless of placeholder name.     If dict, all the keys must be included in the prompt template placeholder {{}}. document_key: str, optional     The key in the <code>text_contents</code> dictionaries that holds the document text. cpu_concurrency: int, optional     The number of parallel threads to use for CPU-bound tasks like chunking. llm_concurrency: int, optional     The number of concurrent requests to make to the LLM. case_sensitive : bool, Optional     if True, entity text matching will be case-sensitive. fuzzy_match : bool, Optional     if True, fuzzy matching will be applied to find entity text. fuzzy_buffer_size : float, Optional     the buffer size for fuzzy matching. Default is 20% of entity text length. fuzzy_score_cutoff : float, Optional     the Jaccard score cutoff for fuzzy matching.      Matched entity text must have a score higher than this value or a None will be returned. allow_overlap_entities : bool, Optional     if True, entities can overlap in the text. return_messages_log : bool, Optional     if True, a list of messages will be returned. Yields: <p>AsyncGenerator[Dict[str, any], None]     A dictionary for each completed document, containing its 'idx' and extracted 'frames'.</p> Source code in <code>package/llm-ie/src/llm_ie/extractors.py</code> <pre><code>async def extract_frames_from_documents(self, text_contents:List[Union[str,Dict[str, any]]], document_key:str=\"text\",\n        cpu_concurrency:int=4, llm_concurrency:int=32, case_sensitive:bool=False, \n        fuzzy_match:bool=True, fuzzy_buffer_size:float=0.2, fuzzy_score_cutoff:float=0.8,\n        allow_overlap_entities:bool=False, return_messages_log:bool=False) -&gt; AsyncGenerator[Dict[str, any], None]:\n    \"\"\"\n    This method inputs a list of documents and yields the results for each document as soon as it is complete.\n\n    Parameters:\n    -----------\n    text_contents : List[Union[str,Dict[str, any]]]\n        a list of input text contents to put in prompt template. \n        If str, the prompt template must has only 1 placeholder {{&lt;placeholder name&gt;}}, regardless of placeholder name.\n        If dict, all the keys must be included in the prompt template placeholder {{&lt;placeholder name&gt;}}.\n    document_key: str, optional\n        The key in the `text_contents` dictionaries that holds the document text.\n    cpu_concurrency: int, optional\n        The number of parallel threads to use for CPU-bound tasks like chunking.\n    llm_concurrency: int, optional\n        The number of concurrent requests to make to the LLM.\n    case_sensitive : bool, Optional\n        if True, entity text matching will be case-sensitive.\n    fuzzy_match : bool, Optional\n        if True, fuzzy matching will be applied to find entity text.\n    fuzzy_buffer_size : float, Optional\n        the buffer size for fuzzy matching. Default is 20% of entity text length.\n    fuzzy_score_cutoff : float, Optional\n        the Jaccard score cutoff for fuzzy matching. \n        Matched entity text must have a score higher than this value or a None will be returned.\n    allow_overlap_entities : bool, Optional\n        if True, entities can overlap in the text.\n    return_messages_log : bool, Optional\n        if True, a list of messages will be returned.\n\n    Yields:\n    -------\n    AsyncGenerator[Dict[str, any], None]\n        A dictionary for each completed document, containing its 'idx' and extracted 'frames'.\n    \"\"\"\n    # Validate text_contents must be a list of str or dict, and not both\n    if not isinstance(text_contents, list):\n        raise ValueError(\"text_contents must be a list of strings or dictionaries.\")\n    if all(isinstance(doc, str) for doc in text_contents):\n        pass  \n    elif all(isinstance(doc, dict) for doc in text_contents):\n        pass\n    # Set CPU executor and queues\n    cpu_executor = ThreadPoolExecutor(max_workers=cpu_concurrency)\n    tasks_queue = asyncio.Queue(maxsize=llm_concurrency * 2)\n    # Store to track units and pending counts\n    results_store = {\n        idx: {'pending': 0, 'units': [], 'text': doc if isinstance(doc, str) else doc.get(document_key, \"\")}\n        for idx, doc in enumerate(text_contents)\n    }\n\n    output_queue = asyncio.Queue()\n    messages_logger = MessagesLogger() if return_messages_log else None\n\n    async def producer():\n        try:\n            for idx, text_content in enumerate(text_contents):\n                text = text_content if isinstance(text_content, str) else text_content.get(document_key, \"\")\n                if not text:\n                    warnings.warn(f\"Document at index {idx} is empty or missing the document key '{document_key}'.\")\n                    # signal that this document is done\n                    await output_queue.put({'idx': idx, 'frames': []})\n                    continue\n\n                units = await self.unit_chunker.chunk_async(text, cpu_executor)\n                await self.context_chunker.fit_async(text, units, cpu_executor)\n                results_store[idx]['pending'] = len(units)\n\n                # Handle cases where a document yields no units\n                if not units: \n                    # signal that this document is done\n                    await output_queue.put({'idx': idx, 'frames': []})\n                    continue\n\n                # Iterate through units\n                for unit in units:\n                    context = await self.context_chunker.chunk_async(unit, cpu_executor)\n                    messages = []\n                    if self.system_prompt:\n                        messages.append({'role': 'system', 'content': self.system_prompt})\n\n                    if not context:\n                        if isinstance(text_content, str):\n                            messages.append({'role': 'user', 'content': self._get_user_prompt(unit.text)})\n                        else:\n                            unit_content = text_content.copy()\n                            unit_content[document_key] = unit.text\n                            messages.append({'role': 'user', 'content': self._get_user_prompt(unit_content)})\n                    else:\n                        # insert context to user prompt\n                        if isinstance(text_content, str):\n                            messages.append({'role': 'user', 'content': self._get_user_prompt(context)})\n                        else:\n                            context_content = text_content.copy()\n                            context_content[document_key] = context\n                            messages.append({'role': 'user', 'content': self._get_user_prompt(context_content)})\n                        # simulate conversation where assistant confirms\n                        messages.append({'role': 'assistant', 'content': 'Sure, please provide the unit text (e.g., sentence, line, chunk) of interest.'})\n                        # place unit of interest\n                        messages.append({'role': 'user', 'content': unit.text})\n\n                    await tasks_queue.put({'idx': idx, 'unit': unit, 'messages': messages})\n        finally:\n            for _ in range(llm_concurrency):\n                await tasks_queue.put(None)\n\n    async def worker():\n        while True:\n            task_item = await tasks_queue.get()\n            if task_item is None:\n                tasks_queue.task_done()\n                break\n\n            idx = task_item['idx']\n            unit = task_item['unit']\n            doc_results = results_store[idx]\n\n            try:\n                gen_text = await self.inference_engine.chat_async(\n                    messages=task_item['messages'], messages_logger=messages_logger\n                )\n                unit.set_generated_text(gen_text[\"response\"])\n                unit.set_status(\"success\")\n                doc_results['units'].append(unit)\n            except Exception as e:\n                warnings.warn(f\"Error processing unit for doc idx {idx}: {e}\")\n            finally:\n                doc_results['pending'] -= 1\n                if doc_results['pending'] &lt;= 0:\n                    final_frames = self._post_process_and_create_frames(doc_results, case_sensitive, fuzzy_match, fuzzy_buffer_size, fuzzy_score_cutoff, allow_overlap_entities)\n                    output_payload = {'idx': idx, 'frames': final_frames}\n                    if return_messages_log:\n                        output_payload['messages_log'] = messages_logger.get_messages_log()\n                    await output_queue.put(output_payload)\n\n                tasks_queue.task_done()\n\n    # Start producer and workers\n    producer_task = asyncio.create_task(producer())\n    worker_tasks = [asyncio.create_task(worker()) for _ in range(llm_concurrency)]\n\n    # Main loop to gather results\n    docs_completed = 0\n    while docs_completed &lt; len(text_contents):\n        result = await output_queue.get()\n        yield result\n        docs_completed += 1\n\n    # Final cleanup\n    await producer_task \n    await tasks_queue.join()\n\n    # Cancel any lingering worker tasks\n    for task in worker_tasks:\n        task.cancel()\n    await asyncio.gather(*worker_tasks, return_exceptions=True)\n\n    cpu_executor.shutdown(wait=False)\n</code></pre>"},{"location":"api/extractors/#llm_ie.extractors.ReviewFrameExtractor","title":"llm_ie.extractors.ReviewFrameExtractor","text":"<pre><code>ReviewFrameExtractor(\n    unit_chunker: UnitChunker,\n    context_chunker: ContextChunker,\n    inference_engine: InferenceEngine,\n    prompt_template: str,\n    review_mode: str,\n    review_prompt: str = None,\n    system_prompt: str = None,\n)\n</code></pre> <p>               Bases: <code>DirectFrameExtractor</code></p> <p>This class add a review step after the DirectFrameExtractor. The Review process asks LLM to review its output and:     1. add more frames while keep current. This is efficient for boosting recall.      2. or, regenerate frames (add new and delete existing).  Use the review_mode parameter to specify. Note that the review_prompt should instruct LLM accordingly.</p> Parameters: <p>unit_chunker : UnitChunker     the unit chunker object that determines how to chunk the document text into units. context_chunker : ContextChunker     the context chunker object that determines how to get context for each unit. inference_engine : InferenceEngine     the LLM inferencing engine object. Must implements the chat() method. prompt_template : str     prompt template with \"{{}}\" placeholder. review_prompt : str: Optional     the prompt text that ask LLM to review. Specify addition or revision in the instruction.     if not provided, a default review prompt will be used.  review_mode : str     review mode. Must be one of {\"addition\", \"revision\"}     addition mode only ask LLM to add new frames, while revision mode ask LLM to regenerate. system_prompt : str, Optional     system prompt. Source code in <code>package/llm-ie/src/llm_ie/extractors.py</code> <pre><code>def __init__(self, unit_chunker:UnitChunker, context_chunker:ContextChunker, inference_engine:InferenceEngine, \n             prompt_template:str, review_mode:str, review_prompt:str=None, system_prompt:str=None):\n    \"\"\"\n    This class add a review step after the DirectFrameExtractor.\n    The Review process asks LLM to review its output and:\n        1. add more frames while keep current. This is efficient for boosting recall. \n        2. or, regenerate frames (add new and delete existing). \n    Use the review_mode parameter to specify. Note that the review_prompt should instruct LLM accordingly.\n\n    Parameters:\n    ----------\n    unit_chunker : UnitChunker\n        the unit chunker object that determines how to chunk the document text into units.\n    context_chunker : ContextChunker\n        the context chunker object that determines how to get context for each unit.\n    inference_engine : InferenceEngine\n        the LLM inferencing engine object. Must implements the chat() method.\n    prompt_template : str\n        prompt template with \"{{&lt;placeholder name&gt;}}\" placeholder.\n    review_prompt : str: Optional\n        the prompt text that ask LLM to review. Specify addition or revision in the instruction.\n        if not provided, a default review prompt will be used. \n    review_mode : str\n        review mode. Must be one of {\"addition\", \"revision\"}\n        addition mode only ask LLM to add new frames, while revision mode ask LLM to regenerate.\n    system_prompt : str, Optional\n        system prompt.\n    \"\"\"\n    super().__init__(inference_engine=inference_engine, \n                     unit_chunker=unit_chunker, \n                     prompt_template=prompt_template, \n                     system_prompt=system_prompt, \n                     context_chunker=context_chunker)\n    # check review mode\n    if review_mode not in {\"addition\", \"revision\"}: \n        raise ValueError('review_mode must be one of {\"addition\", \"revision\"}.')\n    self.review_mode = review_mode\n    # assign review prompt\n    if review_prompt:\n        self.review_prompt = review_prompt\n    else:\n        self.review_prompt = None\n        original_class_name = self.__class__.__name__\n\n        current_class_name = original_class_name\n        for current_class_in_mro in self.__class__.__mro__:\n            if current_class_in_mro is object: \n                continue\n\n            current_class_name = current_class_in_mro.__name__\n            try:\n                file_path = importlib.resources.files('llm_ie.asset.default_prompts').\\\n                    joinpath(f\"{self.__class__.__name__}_{self.review_mode}_review_prompt.txt\")\n                with open(file_path, 'r', encoding=\"utf-8\") as f:\n                    self.review_prompt = f.read()\n            except FileNotFoundError:\n                pass\n\n            except Exception as e:\n                warnings.warn(\n                    f\"Error attempting to read default review prompt for '{current_class_name}' \"\n                    f\"from '{str(file_path)}': {e}. Trying next in MRO.\",\n                    UserWarning\n                )\n                continue \n\n    if self.review_prompt is None:\n        raise ValueError(f\"Cannot find review prompt for {self.__class__.__name__} in the package. Please provide a review_prompt.\")\n</code></pre>"},{"location":"api/extractors/#llm_ie.extractors.ReviewFrameExtractor.extract","title":"extract","text":"<pre><code>extract(\n    text_content: Union[str, Dict[str, str]],\n    document_key: str = None,\n    verbose: bool = False,\n    return_messages_log: bool = False,\n) -&gt; List[FrameExtractionUnit]\n</code></pre> <p>This method inputs a text and outputs a list of outputs per unit.</p> Parameters: <p>text_content : Union[str, Dict[str,str]]     the input text content to put in prompt template.      If str, the prompt template must has only 1 placeholder {{}}, regardless of placeholder name.     If dict, all the keys must be included in the prompt template placeholder {{}}. document_key : str, Optional     specify the key in text_content where document text is.      If text_content is str, this parameter will be ignored. verbose : bool, Optional     if True, LLM generated text will be printed in terminal in real-time.  return_messages_log : bool, Optional     if True, a list of messages will be returned. <p>Return : List[FrameExtractionUnitResult]     the output from LLM for each unit. Contains the start, end, text, and generated text.</p> Source code in <code>package/llm-ie/src/llm_ie/extractors.py</code> <pre><code>def extract(self, text_content:Union[str, Dict[str,str]], document_key:str=None, \n            verbose:bool=False, return_messages_log:bool=False) -&gt; List[FrameExtractionUnit]:\n    \"\"\"\n    This method inputs a text and outputs a list of outputs per unit.\n\n    Parameters:\n    ----------\n    text_content : Union[str, Dict[str,str]]\n        the input text content to put in prompt template. \n        If str, the prompt template must has only 1 placeholder {{&lt;placeholder name&gt;}}, regardless of placeholder name.\n        If dict, all the keys must be included in the prompt template placeholder {{&lt;placeholder name&gt;}}.\n    document_key : str, Optional\n        specify the key in text_content where document text is. \n        If text_content is str, this parameter will be ignored.\n    verbose : bool, Optional\n        if True, LLM generated text will be printed in terminal in real-time. \n    return_messages_log : bool, Optional\n        if True, a list of messages will be returned.\n\n    Return : List[FrameExtractionUnitResult]\n        the output from LLM for each unit. Contains the start, end, text, and generated text.\n    \"\"\"\n    # unit chunking\n    if isinstance(text_content, str):\n        doc_text = text_content\n\n    elif isinstance(text_content, dict):\n        if document_key is None:\n            raise ValueError(\"document_key must be provided when text_content is dict.\")\n        doc_text = text_content[document_key]\n\n    units = self.unit_chunker.chunk(doc_text)\n    # context chunker init\n    self.context_chunker.fit(doc_text, units)\n\n    # messages logger init\n    messages_logger = MessagesLogger() if return_messages_log else None\n\n    # generate unit by unit\n    for i, unit in enumerate(units):\n        # &lt;--- Initial generation step ---&gt;\n        # construct chat messages\n        messages = []\n        if self.system_prompt:\n            messages.append({'role': 'system', 'content': self.system_prompt})\n\n        context = self.context_chunker.chunk(unit)\n\n        if context == \"\":\n            # no context, just place unit in user prompt\n            if isinstance(text_content, str):\n                messages.append({'role': 'user', 'content': self._get_user_prompt(unit.text)})\n            else:\n                unit_content = text_content.copy()\n                unit_content[document_key] = unit.text\n                messages.append({'role': 'user', 'content': self._get_user_prompt(unit_content)})\n        else:\n            # insert context to user prompt\n            if isinstance(text_content, str):\n                messages.append({'role': 'user', 'content': self._get_user_prompt(context)})\n            else:\n                context_content = text_content.copy()\n                context_content[document_key] = context\n                messages.append({'role': 'user', 'content': self._get_user_prompt(context_content)})\n            # simulate conversation where assistant confirms\n            messages.append({'role': 'assistant', 'content': 'Sure, please provide the unit text (e.g., sentence, line, chunk) of interest.'})\n            # place unit of interest\n            messages.append({'role': 'user', 'content': unit.text})\n\n        if verbose:\n            print(f\"\\n\\n{Fore.GREEN}Unit {i + 1}/{len(units)}:{Style.RESET_ALL}\\n{unit.text}\\n\")\n            if context != \"\":\n                print(f\"{Fore.YELLOW}Context:{Style.RESET_ALL}\\n{context}\\n\")\n\n            print(f\"{Fore.BLUE}Extraction:{Style.RESET_ALL}\")\n\n\n        initial = self.inference_engine.chat(\n                        messages=messages, \n                        verbose=verbose,\n                        stream=False,\n                        messages_logger=messages_logger\n                    )\n\n        # &lt;--- Review step ---&gt;\n        if verbose:\n            print(f\"\\n{Fore.YELLOW}Review:{Style.RESET_ALL}\")\n\n        messages.append({'role': 'assistant', 'content': initial[\"response\"]})\n        messages.append({'role': 'user', 'content': self.review_prompt})\n\n        review = self.inference_engine.chat(\n                        messages=messages, \n                        verbose=verbose,\n                        stream=False,\n                        messages_logger=messages_logger\n                    )\n\n        # Output\n        if self.review_mode == \"revision\":\n            gen_text = review[\"response\"]\n        elif self.review_mode == \"addition\":\n            gen_text = initial[\"response\"] + '\\n' + review[\"response\"]\n\n        # add generated text to unit\n        unit.set_generated_text(gen_text)\n        unit.set_status(\"success\")\n\n    if return_messages_log:\n        return units, messages_logger.get_messages_log()\n\n    return units\n</code></pre>"},{"location":"api/extractors/#llm_ie.extractors.ReviewFrameExtractor.stream","title":"stream","text":"<pre><code>stream(\n    text_content: Union[str, Dict[str, str]],\n    document_key: str = None,\n) -&gt; Generator[str, None, None]\n</code></pre> <p>This method inputs a text and outputs a list of outputs per unit.</p> Parameters: <p>text_content : Union[str, Dict[str,str]]     the input text content to put in prompt template.      If str, the prompt template must has only 1 placeholder {{}}, regardless of placeholder name.     If dict, all the keys must be included in the prompt template placeholder {{}}. document_key : str, Optional     specify the key in text_content where document text is.      If text_content is str, this parameter will be ignored. <p>Return : List[FrameExtractionUnitResult]     the output from LLM for each unit. Contains the start, end, text, and generated text.</p> Source code in <code>package/llm-ie/src/llm_ie/extractors.py</code> <pre><code>def stream(self, text_content:Union[str, Dict[str,str]], document_key:str=None) -&gt; Generator[str, None, None]:\n    \"\"\"\n    This method inputs a text and outputs a list of outputs per unit.\n\n    Parameters:\n    ----------\n    text_content : Union[str, Dict[str,str]]\n        the input text content to put in prompt template. \n        If str, the prompt template must has only 1 placeholder {{&lt;placeholder name&gt;}}, regardless of placeholder name.\n        If dict, all the keys must be included in the prompt template placeholder {{&lt;placeholder name&gt;}}.\n    document_key : str, Optional\n        specify the key in text_content where document text is. \n        If text_content is str, this parameter will be ignored.\n\n    Return : List[FrameExtractionUnitResult]\n        the output from LLM for each unit. Contains the start, end, text, and generated text.\n    \"\"\"\n    # unit chunking\n    if isinstance(text_content, str):\n        doc_text = text_content\n\n    elif isinstance(text_content, dict):\n        if document_key is None:\n            raise ValueError(\"document_key must be provided when text_content is dict.\")\n        doc_text = text_content[document_key]\n\n    units = self.unit_chunker.chunk(doc_text)\n    # context chunker init\n    self.context_chunker.fit(doc_text, units)\n\n    # generate unit by unit\n    for i, unit in enumerate(units):\n        # &lt;--- Initial generation step ---&gt;\n        # construct chat messages\n        messages = []\n        if self.system_prompt:\n            messages.append({'role': 'system', 'content': self.system_prompt})\n\n        context = self.context_chunker.chunk(unit)\n\n        if context == \"\":\n            # no context, just place unit in user prompt\n            if isinstance(text_content, str):\n                messages.append({'role': 'user', 'content': self._get_user_prompt(unit.text)})\n            else:\n                unit_content = text_content.copy()\n                unit_content[document_key] = unit.text\n                messages.append({'role': 'user', 'content': self._get_user_prompt(unit_content)})\n        else:\n            # insert context to user prompt\n            if isinstance(text_content, str):\n                messages.append({'role': 'user', 'content': self._get_user_prompt(context)})\n            else:\n                context_content = text_content.copy()\n                context_content[document_key] = context\n                messages.append({'role': 'user', 'content': self._get_user_prompt(context_content)})\n            # simulate conversation where assistant confirms\n            messages.append({'role': 'assistant', 'content': 'Sure, please provide the unit text (e.g., sentence, line, chunk) of interest.'})\n            # place unit of interest\n            messages.append({'role': 'user', 'content': unit.text})\n\n\n        yield f\"\\n\\n{Fore.GREEN}Unit {i}:{Style.RESET_ALL}\\n{unit.text}\\n\"\n        if context != \"\":\n            yield f\"{Fore.YELLOW}Context:{Style.RESET_ALL}\\n{context}\\n\"\n\n        yield f\"{Fore.BLUE}Extraction:{Style.RESET_ALL}\\n\"\n\n        response_stream = self.inference_engine.chat(\n                        messages=messages, \n                        stream=True\n                    )\n\n        initial = \"\"\n        for chunk in response_stream:\n            initial += chunk\n            yield chunk\n\n        # &lt;--- Review step ---&gt;\n        yield f\"\\n{Fore.YELLOW}Review:{Style.RESET_ALL}\"\n\n        messages.append({'role': 'assistant', 'content': initial})\n        messages.append({'role': 'user', 'content': self.review_prompt})\n\n        response_stream = self.inference_engine.chat(\n                        messages=messages, \n                        stream=True\n                    )\n\n        for chunk in response_stream:\n            yield chunk\n</code></pre>"},{"location":"api/extractors/#llm_ie.extractors.ReviewFrameExtractor.extract_async","title":"extract_async  <code>async</code>","text":"<pre><code>extract_async(\n    text_content: Union[str, Dict[str, str]],\n    document_key: str = None,\n    concurrent_batch_size: int = 32,\n    return_messages_log: bool = False,\n    **kwrs\n) -&gt; List[FrameExtractionUnit]\n</code></pre> <p>This is the asynchronous version of the extract() method with the review step.</p> Parameters: <p>text_content : Union[str, Dict[str,str]]     the input text content to put in prompt template.     If str, the prompt template must has only 1 placeholder {{}}, regardless of placeholder name.     If dict, all the keys must be included in the prompt template placeholder {{}}. document_key : str, Optional     specify the key in text_content where document text is.     If text_content is str, this parameter will be ignored. concurrent_batch_size : int, Optional     the batch size for concurrent processing. return_messages_log : bool, Optional     if True, a list of messages will be returned, including review steps. <p>Return : List[FrameExtractionUnitResult]     the output from LLM for each unit after review. Contains the start, end, text, and generated text.</p> Source code in <code>package/llm-ie/src/llm_ie/extractors.py</code> <pre><code>async def extract_async(self, text_content:Union[str, Dict[str,str]], document_key:str=None,\n                        concurrent_batch_size:int=32, return_messages_log:bool=False, **kwrs) -&gt; List[FrameExtractionUnit]:\n    \"\"\"\n    This is the asynchronous version of the extract() method with the review step.\n\n    Parameters:\n    ----------\n    text_content : Union[str, Dict[str,str]]\n        the input text content to put in prompt template.\n        If str, the prompt template must has only 1 placeholder {{&lt;placeholder name&gt;}}, regardless of placeholder name.\n        If dict, all the keys must be included in the prompt template placeholder {{&lt;placeholder name&gt;}}.\n    document_key : str, Optional\n        specify the key in text_content where document text is.\n        If text_content is str, this parameter will be ignored.\n    concurrent_batch_size : int, Optional\n        the batch size for concurrent processing.\n    return_messages_log : bool, Optional\n        if True, a list of messages will be returned, including review steps.\n\n    Return : List[FrameExtractionUnitResult]\n        the output from LLM for each unit after review. Contains the start, end, text, and generated text.\n    \"\"\"\n    if isinstance(text_content, str):\n        doc_text = text_content\n    elif isinstance(text_content, dict):\n        if document_key is None:\n            raise ValueError(\"document_key must be provided when text_content is dict.\")\n        if document_key not in text_content:\n             raise ValueError(f\"document_key '{document_key}' not found in text_content dictionary.\")\n        doc_text = text_content[document_key]\n    else:\n        raise TypeError(\"text_content must be a string or a dictionary.\")\n\n    # unit chunking\n    units = self.unit_chunker.chunk(doc_text)\n\n    # context chunker init\n    self.context_chunker.fit(doc_text, units)\n\n    # messages logger init\n    messages_logger = MessagesLogger() if return_messages_log else None\n\n    # &lt;--- Initial generation step ---&gt;\n    initial_tasks_input = []\n    for i, unit in enumerate(units):\n        # construct chat messages for initial generation\n        messages = []\n        if self.system_prompt:\n            messages.append({'role': 'system', 'content': self.system_prompt})\n\n        context = self.context_chunker.chunk(unit)\n\n        if context == \"\":\n             # no context, just place unit in user prompt\n            if isinstance(text_content, str):\n                messages.append({'role': 'user', 'content': self._get_user_prompt(unit.text)})\n            else:\n                unit_content = text_content.copy()\n                unit_content[document_key] = unit.text\n                messages.append({'role': 'user', 'content': self._get_user_prompt(unit_content)})\n        else:\n            # insert context to user prompt\n            if isinstance(text_content, str):\n                messages.append({'role': 'user', 'content': self._get_user_prompt(context)})\n            else:\n                context_content = text_content.copy()\n                context_content[document_key] = context\n                messages.append({'role': 'user', 'content': self._get_user_prompt(context_content)})\n            # simulate conversation where assistant confirms\n            messages.append({'role': 'assistant', 'content': 'Sure, please provide the unit text (e.g., sentence, line, chunk) of interest.'})\n            # place unit of interest\n            messages.append({'role': 'user', 'content': unit.text})\n\n        # Store unit and messages together for the initial task\n        initial_tasks_input.append({\"unit\": unit, \"messages\": messages, \"original_index\": i})\n\n    semaphore = asyncio.Semaphore(concurrent_batch_size)\n\n    async def initial_semaphore_helper(task_data: Dict):\n        unit = task_data[\"unit\"]\n        messages = task_data[\"messages\"]\n        original_index = task_data[\"original_index\"]\n\n        async with semaphore:\n            gen_text = await self.inference_engine.chat_async(\n                messages=messages,\n                messages_logger=messages_logger\n            )\n        # Return initial generation result along with the messages used and the unit\n        out = {\"original_index\": original_index, \"unit\": unit, \"initial_gen_text\": gen_text[\"response\"], \"initial_messages\": messages}\n        if \"reasoning\" in gen_text:\n            out[\"reasoning\"] = gen_text[\"reasoning\"]\n        return out\n\n    # Create and gather initial generation tasks\n    initial_tasks = [\n        asyncio.create_task(initial_semaphore_helper(\n            task_inp\n        ))\n        for task_inp in initial_tasks_input\n    ]\n\n    initial_results_raw = await asyncio.gather(*initial_tasks)\n\n    # Sort initial results back into original order\n    initial_results_raw.sort(key=lambda x: x[\"original_index\"])\n\n    # &lt;--- Review step ---&gt;\n    review_tasks_input = []\n    for result_data in initial_results_raw:\n        # Prepare messages for the review step\n        initial_messages = result_data[\"initial_messages\"]\n        initial_gen_text = result_data[\"initial_gen_text\"]\n        review_messages = initial_messages + [\n            {'role': 'assistant', 'content': initial_gen_text},\n            {'role': 'user', 'content': self.review_prompt}\n        ]\n        # Store data needed for review task\n        if \"reasoning\" in result_data:\n            message = {'role': 'assistant', 'content': initial_gen_text, \"reasoning\": result_data[\"reasoning\"]}\n        else:\n            message = {'role': 'assistant', 'content': initial_gen_text}\n\n        review_tasks_input.append({\n            \"unit\": result_data[\"unit\"],\n            \"initial_gen_text\": initial_gen_text,\n            \"messages\": review_messages, \n            \"original_index\": result_data[\"original_index\"],\n            \"full_initial_log\": initial_messages + [message] + [{'role': 'user', 'content': self.review_prompt}] if return_messages_log else None\n        })\n\n\n    async def review_semaphore_helper(task_data: Dict, **kwrs):\n        messages = task_data[\"messages\"] \n\n        async with semaphore:\n            review_gen_text = await self.inference_engine.chat_async(\n                messages=messages,\n                messages_logger=messages_logger\n            )\n        # Combine initial and review results\n        task_data[\"review_gen_text\"] = review_gen_text[\"response\"]\n        return task_data # Return the augmented dictionary\n\n    # Create and gather review tasks\n    review_tasks = [\n         asyncio.create_task(review_semaphore_helper(\n            task_inp\n        ))\n       for task_inp in review_tasks_input\n    ]\n\n    final_results_raw = await asyncio.gather(*review_tasks)\n\n    # Sort final results back into original order (although gather might preserve order for tasks added sequentially)\n    final_results_raw.sort(key=lambda x: x[\"original_index\"])\n\n    # &lt;--- Process final results ---&gt;\n    for result_data in final_results_raw:\n        unit = result_data[\"unit\"]\n        initial_gen = result_data[\"initial_gen_text\"]\n        review_gen = result_data[\"review_gen_text\"]\n\n        # Combine based on review mode\n        if self.review_mode == \"revision\":\n            final_gen_text = review_gen\n        elif self.review_mode == \"addition\":\n            final_gen_text = initial_gen + '\\n' + review_gen\n        else: # Should not happen due to init check\n            final_gen_text = review_gen # Default to revision if mode is somehow invalid\n\n        # Create final result object\n        unit.set_generated_text(final_gen_text)\n        unit.set_status(\"success\")\n\n    if return_messages_log:\n        return units, messages_logger.get_messages_log()\n    else:\n        return units\n</code></pre>"},{"location":"api/extractors/#convenience-frame-extractors","title":"Convenience Frame Extractors","text":""},{"location":"api/extractors/#llm_ie.extractors.BasicFrameExtractor","title":"llm_ie.extractors.BasicFrameExtractor","text":"<pre><code>BasicFrameExtractor(\n    inference_engine: InferenceEngine,\n    prompt_template: str,\n    system_prompt: str = None,\n)\n</code></pre> <p>               Bases: <code>DirectFrameExtractor</code></p> <p>This class diretly prompt LLM for frame extraction. Input system prompt (optional), prompt template (with instruction, few-shot examples),  and specify a LLM.</p> Parameters: <p>inference_engine : InferenceEngine     the LLM inferencing engine object. Must implements the chat() method. prompt_template : str     prompt template with \"{{}}\" placeholder. system_prompt : str, Optional     system prompt. Source code in <code>package/llm-ie/src/llm_ie/extractors.py</code> <pre><code>def __init__(self, inference_engine:InferenceEngine, prompt_template:str, system_prompt:str=None):\n    \"\"\"\n    This class diretly prompt LLM for frame extraction.\n    Input system prompt (optional), prompt template (with instruction, few-shot examples), \n    and specify a LLM.\n\n    Parameters:\n    ----------\n    inference_engine : InferenceEngine\n        the LLM inferencing engine object. Must implements the chat() method.\n    prompt_template : str\n        prompt template with \"{{&lt;placeholder name&gt;}}\" placeholder.\n    system_prompt : str, Optional\n        system prompt.\n    \"\"\"\n    super().__init__(inference_engine=inference_engine, \n                     unit_chunker=WholeDocumentUnitChunker(),\n                     prompt_template=prompt_template, \n                     system_prompt=system_prompt, \n                     context_chunker=NoContextChunker())\n</code></pre>"},{"location":"api/extractors/#llm_ie.extractors.SentenceFrameExtractor","title":"llm_ie.extractors.SentenceFrameExtractor","text":"<pre><code>SentenceFrameExtractor(\n    inference_engine: InferenceEngine,\n    prompt_template: str,\n    system_prompt: str = None,\n    context_sentences: Union[str, int] = \"all\",\n)\n</code></pre> <p>               Bases: <code>DirectFrameExtractor</code></p> <p>This class performs sentence-by-sentence information extraction. The process is as follows:     1. system prompt (optional)     2. user prompt with instructions (schema, background, full text, few-shot example...)     3. feed a sentence (start with first sentence)     4. LLM extract entities and attributes from the sentence     5. iterate to the next sentence and repeat steps 3-4 until all sentences are processed.</p> <p>Input system prompt (optional), prompt template (with user instructions),  and specify a LLM.</p> Parameters: <p>inference_engine : InferenceEngine     the LLM inferencing engine object. Must implements the chat() method. prompt_template : str     prompt template with \"{{}}\" placeholder. system_prompt : str, Optional     system prompt. context_sentences : Union[str, int], Optional     number of sentences before and after the given sentence to provide additional context.      if \"all\", the full text will be provided in the prompt as context.      if 0, no additional context will be provided.         This is good for tasks that does not require context beyond the given sentence.      if &gt; 0, the number of sentences before and after the given sentence to provide as context.         This is good for tasks that require context beyond the given sentence. Source code in <code>package/llm-ie/src/llm_ie/extractors.py</code> <pre><code>def __init__(self, inference_engine:InferenceEngine, prompt_template:str, system_prompt:str=None,\n             context_sentences:Union[str, int]=\"all\"):\n    \"\"\"\n    This class performs sentence-by-sentence information extraction.\n    The process is as follows:\n        1. system prompt (optional)\n        2. user prompt with instructions (schema, background, full text, few-shot example...)\n        3. feed a sentence (start with first sentence)\n        4. LLM extract entities and attributes from the sentence\n        5. iterate to the next sentence and repeat steps 3-4 until all sentences are processed.\n\n    Input system prompt (optional), prompt template (with user instructions), \n    and specify a LLM.\n\n    Parameters:\n    ----------\n    inference_engine : InferenceEngine\n        the LLM inferencing engine object. Must implements the chat() method.\n    prompt_template : str\n        prompt template with \"{{&lt;placeholder name&gt;}}\" placeholder.\n    system_prompt : str, Optional\n        system prompt.\n    context_sentences : Union[str, int], Optional\n        number of sentences before and after the given sentence to provide additional context. \n        if \"all\", the full text will be provided in the prompt as context. \n        if 0, no additional context will be provided.\n            This is good for tasks that does not require context beyond the given sentence. \n        if &gt; 0, the number of sentences before and after the given sentence to provide as context.\n            This is good for tasks that require context beyond the given sentence. \n    \"\"\"\n    if not isinstance(context_sentences, int) and context_sentences != \"all\":\n        raise ValueError('context_sentences must be an integer (&gt;= 0) or \"all\".')\n\n    if isinstance(context_sentences, int) and context_sentences &lt; 0:\n        raise ValueError(\"context_sentences must be a positive integer.\")\n\n    if isinstance(context_sentences, int):\n        context_chunker = SlideWindowContextChunker(window_size=context_sentences)\n    elif context_sentences == \"all\":\n        context_chunker = WholeDocumentContextChunker()\n\n    super().__init__(inference_engine=inference_engine, \n                     unit_chunker=SentenceUnitChunker(),\n                     prompt_template=prompt_template, \n                     system_prompt=system_prompt, \n                     context_chunker=context_chunker)\n</code></pre>"},{"location":"api/extractors/#llm_ie.extractors.BasicReviewFrameExtractor","title":"llm_ie.extractors.BasicReviewFrameExtractor","text":"<pre><code>BasicReviewFrameExtractor(\n    inference_engine: InferenceEngine,\n    prompt_template: str,\n    review_mode: str,\n    review_prompt: str = None,\n    system_prompt: str = None,\n)\n</code></pre> <p>               Bases: <code>ReviewFrameExtractor</code></p> <p>This class add a review step after the BasicFrameExtractor. The Review process asks LLM to review its output and:     1. add more frames while keep current. This is efficient for boosting recall.      2. or, regenerate frames (add new and delete existing).  Use the review_mode parameter to specify. Note that the review_prompt should instruct LLM accordingly.</p> Parameters: <p>inference_engine : InferenceEngine     the LLM inferencing engine object. Must implements the chat() method. prompt_template : str     prompt template with \"{{}}\" placeholder. review_prompt : str: Optional     the prompt text that ask LLM to review. Specify addition or revision in the instruction.     if not provided, a default review prompt will be used.  review_mode : str     review mode. Must be one of {\"addition\", \"revision\"}     addition mode only ask LLM to add new frames, while revision mode ask LLM to regenerate. system_prompt : str, Optional     system prompt. Source code in <code>package/llm-ie/src/llm_ie/extractors.py</code> <pre><code>def __init__(self, inference_engine:InferenceEngine, prompt_template:str, review_mode:str, review_prompt:str=None, system_prompt:str=None):\n    \"\"\"\n    This class add a review step after the BasicFrameExtractor.\n    The Review process asks LLM to review its output and:\n        1. add more frames while keep current. This is efficient for boosting recall. \n        2. or, regenerate frames (add new and delete existing). \n    Use the review_mode parameter to specify. Note that the review_prompt should instruct LLM accordingly.\n\n    Parameters:\n    ----------\n    inference_engine : InferenceEngine\n        the LLM inferencing engine object. Must implements the chat() method.\n    prompt_template : str\n        prompt template with \"{{&lt;placeholder name&gt;}}\" placeholder.\n    review_prompt : str: Optional\n        the prompt text that ask LLM to review. Specify addition or revision in the instruction.\n        if not provided, a default review prompt will be used. \n    review_mode : str\n        review mode. Must be one of {\"addition\", \"revision\"}\n        addition mode only ask LLM to add new frames, while revision mode ask LLM to regenerate.\n    system_prompt : str, Optional\n        system prompt.\n    \"\"\"\n    super().__init__(inference_engine=inference_engine, \n                     unit_chunker=WholeDocumentUnitChunker(),\n                     prompt_template=prompt_template, \n                     review_mode=review_mode,\n                     review_prompt=review_prompt,\n                     system_prompt=system_prompt, \n                     context_chunker=NoContextChunker())\n</code></pre>"},{"location":"api/extractors/#llm_ie.extractors.SentenceReviewFrameExtractor","title":"llm_ie.extractors.SentenceReviewFrameExtractor","text":"<pre><code>SentenceReviewFrameExtractor(\n    inference_engine: InferenceEngine,\n    prompt_template: str,\n    review_mode: str,\n    review_prompt: str = None,\n    system_prompt: str = None,\n    context_sentences: Union[str, int] = \"all\",\n)\n</code></pre> <p>               Bases: <code>ReviewFrameExtractor</code></p> <p>This class adds a review step after the SentenceFrameExtractor. For each sentence, the review process asks LLM to review its output and:     1. add more frames while keeping current. This is efficient for boosting recall.      2. or, regenerate frames (add new and delete existing).  Use the review_mode parameter to specify. Note that the review_prompt should instruct LLM accordingly.</p> Parameters: <p>inference_engine : InferenceEngine     the LLM inferencing engine object. Must implements the chat() method. prompt_template : str     prompt template with \"{{}}\" placeholder. review_prompt : str: Optional     the prompt text that ask LLM to review. Specify addition or revision in the instruction.     if not provided, a default review prompt will be used.  review_mode : str     review mode. Must be one of {\"addition\", \"revision\"}     addition mode only ask LLM to add new frames, while revision mode ask LLM to regenerate. system_prompt : str, Optional     system prompt. context_sentences : Union[str, int], Optional     number of sentences before and after the given sentence to provide additional context.      if \"all\", the full text will be provided in the prompt as context.      if 0, no additional context will be provided.         This is good for tasks that does not require context beyond the given sentence.      if &gt; 0, the number of sentences before and after the given sentence to provide as context.         This is good for tasks that require context beyond the given sentence. Source code in <code>package/llm-ie/src/llm_ie/extractors.py</code> <pre><code>def __init__(self, inference_engine:InferenceEngine, prompt_template:str,  \n             review_mode:str, review_prompt:str=None, system_prompt:str=None,\n             context_sentences:Union[str, int]=\"all\"):\n    \"\"\"\n    This class adds a review step after the SentenceFrameExtractor.\n    For each sentence, the review process asks LLM to review its output and:\n        1. add more frames while keeping current. This is efficient for boosting recall. \n        2. or, regenerate frames (add new and delete existing). \n    Use the review_mode parameter to specify. Note that the review_prompt should instruct LLM accordingly.\n\n    Parameters:\n    ----------\n    inference_engine : InferenceEngine\n        the LLM inferencing engine object. Must implements the chat() method.\n    prompt_template : str\n        prompt template with \"{{&lt;placeholder name&gt;}}\" placeholder.\n    review_prompt : str: Optional\n        the prompt text that ask LLM to review. Specify addition or revision in the instruction.\n        if not provided, a default review prompt will be used. \n    review_mode : str\n        review mode. Must be one of {\"addition\", \"revision\"}\n        addition mode only ask LLM to add new frames, while revision mode ask LLM to regenerate.\n    system_prompt : str, Optional\n        system prompt.\n    context_sentences : Union[str, int], Optional\n        number of sentences before and after the given sentence to provide additional context. \n        if \"all\", the full text will be provided in the prompt as context. \n        if 0, no additional context will be provided.\n            This is good for tasks that does not require context beyond the given sentence. \n        if &gt; 0, the number of sentences before and after the given sentence to provide as context.\n            This is good for tasks that require context beyond the given sentence. \n    \"\"\"\n    if not isinstance(context_sentences, int) and context_sentences != \"all\":\n        raise ValueError('context_sentences must be an integer (&gt;= 0) or \"all\".')\n\n    if isinstance(context_sentences, int) and context_sentences &lt; 0:\n        raise ValueError(\"context_sentences must be a positive integer.\")\n\n    if isinstance(context_sentences, int):\n        context_chunker = SlideWindowContextChunker(window_size=context_sentences)\n    elif context_sentences == \"all\":\n        context_chunker = WholeDocumentContextChunker()\n\n    super().__init__(inference_engine=inference_engine, \n                     unit_chunker=SentenceUnitChunker(),\n                     prompt_template=prompt_template,\n                     review_mode=review_mode,\n                     review_prompt=review_prompt, \n                     system_prompt=system_prompt, \n                     context_chunker=context_chunker)\n</code></pre>"},{"location":"api/extractors/#relation-extractors","title":"Relation Extractors","text":""},{"location":"api/extractors/#llm_ie.extractors.BinaryRelationExtractor","title":"llm_ie.extractors.BinaryRelationExtractor","text":"<pre><code>BinaryRelationExtractor(\n    inference_engine: InferenceEngine,\n    prompt_template: str,\n    possible_relation_func: Callable,\n    system_prompt: str = None,\n)\n</code></pre> <p>               Bases: <code>RelationExtractor</code></p> <p>This class extracts binary (yes/no) relations between two entities. Input LLM inference engine, system prompt (optional), prompt template (with instruction, few-shot examples).</p> <p>Parameters:</p> Name Type Description Default <code>inference_engine</code> <code>InferenceEngine</code> <p>the LLM inferencing engine object. Must implements the chat() method.</p> required <code>prompt_template</code> <code>str</code> <p>prompt template with \"{{}}\" placeholder. required <code>possible_relation_func</code> <code>(Callable, Optional)</code> <p>a function that inputs 2 frames and returns a bool indicating possible relations between them.</p> required <code>system_prompt</code> <code>(str, Optional)</code> <p>system prompt.</p> <code>None</code> Source code in <code>package/llm-ie/src/llm_ie/extractors.py</code> <pre><code>def __init__(self, inference_engine:InferenceEngine, prompt_template:str, possible_relation_func: Callable,\n             system_prompt:str=None):\n    \"\"\"\n    This class extracts binary (yes/no) relations between two entities.\n    Input LLM inference engine, system prompt (optional), prompt template (with instruction, few-shot examples).\n\n    Parameters\n    ----------\n    inference_engine : InferenceEngine\n        the LLM inferencing engine object. Must implements the chat() method.\n    prompt_template : str\n        prompt template with \"{{&lt;placeholder name&gt;}}\" placeholder.\n    possible_relation_func : Callable, Optional\n        a function that inputs 2 frames and returns a bool indicating possible relations between them.\n    system_prompt : str, Optional\n        system prompt.\n    \"\"\"\n    super().__init__(inference_engine, prompt_template, system_prompt)\n    if not callable(possible_relation_func):\n        raise TypeError(f\"Expect possible_relation_func as a function, received {type(possible_relation_func)} instead.\")\n\n    sig = inspect.signature(possible_relation_func)\n    if len(sig.parameters) != 2:\n        raise ValueError(\"The possible_relation_func must have exactly two parameters.\")\n\n    if sig.return_annotation not in {bool, inspect.Signature.empty}:\n        warnings.warn(f\"Expected possible_relation_func return annotation to be bool, but got {sig.return_annotation}.\")\n\n    self.possible_relation_func = possible_relation_func\n</code></pre>"},{"location":"api/extractors/#llm_ie.extractors.MultiClassRelationExtractor","title":"llm_ie.extractors.MultiClassRelationExtractor","text":"<pre><code>MultiClassRelationExtractor(\n    inference_engine: InferenceEngine,\n    prompt_template: str,\n    possible_relation_types_func: Callable,\n    system_prompt: str = None,\n)\n</code></pre> <p>               Bases: <code>RelationExtractor</code></p> <p>This class extracts relations with relation types. Input LLM inference engine, system prompt (optional), prompt template (with instruction, few-shot examples).</p> <p>Parameters:</p> Name Type Description Default <code>inference_engine</code> <code>InferenceEngine</code> <p>the LLM inferencing engine object. Must implements the chat() method.</p> required <code>prompt_template</code> <code>str</code> <p>prompt template with \"{{}}\" placeholder. required <code>possible_relation_types_func</code> <code>Callable</code> <p>a function that inputs 2 frames and returns a List of possible relation types between them.  If the two frames must not have relations, this function should return an empty list [].</p> required <code>system_prompt</code> <code>(str, Optional)</code> <p>system prompt.</p> <code>None</code> Source code in <code>package/llm-ie/src/llm_ie/extractors.py</code> <pre><code>def __init__(self, inference_engine:InferenceEngine, prompt_template:str, possible_relation_types_func: Callable, \n             system_prompt:str=None):\n    \"\"\"\n    This class extracts relations with relation types.\n    Input LLM inference engine, system prompt (optional), prompt template (with instruction, few-shot examples).\n\n    Parameters\n    ----------\n    inference_engine : InferenceEngine\n        the LLM inferencing engine object. Must implements the chat() method.\n    prompt_template : str\n        prompt template with \"{{&lt;placeholder name&gt;}}\" placeholder.\n    possible_relation_types_func : Callable\n        a function that inputs 2 frames and returns a List of possible relation types between them. \n        If the two frames must not have relations, this function should return an empty list [].\n    system_prompt : str, Optional\n        system prompt.\n    \"\"\"\n    super().__init__(inference_engine=inference_engine,\n                     prompt_template=prompt_template,\n                     system_prompt=system_prompt)\n\n    if possible_relation_types_func:\n        # Check if possible_relation_types_func is a function\n        if not callable(possible_relation_types_func):\n            raise TypeError(f\"Expect possible_relation_types_func as a function, received {type(possible_relation_types_func)} instead.\")\n\n        sig = inspect.signature(possible_relation_types_func)\n        # Check if frame_1, frame_2 are in input parameters\n        if len(sig.parameters) != 2:\n            raise ValueError(\"The possible_relation_types_func must have exactly frame_1 and frame_2 as parameters.\")\n        if \"frame_1\" not in sig.parameters.keys():\n            raise ValueError(\"The possible_relation_types_func is missing frame_1 as a parameter.\")\n        if \"frame_2\" not in sig.parameters.keys():\n            raise ValueError(\"The possible_relation_types_func is missing frame_2 as a parameter.\")\n        # Check if output is a List\n        if sig.return_annotation not in {inspect._empty, List, List[str]}:\n            raise ValueError(f\"Expect possible_relation_types_func to output a List of string, current type hint suggests {sig.return_annotation} instead.\")\n\n        self.possible_relation_types_func = possible_relation_types_func\n</code></pre>"},{"location":"api/llm_config/","title":"LLM Configuration API","text":""},{"location":"api/llm_config/#llm_ie.engines.BasicLLMConfig","title":"llm_ie.engines.BasicLLMConfig","text":"<pre><code>BasicLLMConfig(\n    max_new_tokens: int = 2048,\n    temperature: float = 0.0,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>LLMConfig</code></p> <p>The basic LLM configuration for most non-reasoning models.</p> Source code in <code>package/llm-ie/src/llm_ie/engines.py</code> <pre><code>def __init__(self, max_new_tokens:int=2048, temperature:float=0.0, **kwargs):\n    \"\"\"\n    The basic LLM configuration for most non-reasoning models.\n    \"\"\"\n    super().__init__(**kwargs)\n    self.max_new_tokens = max_new_tokens\n    self.temperature = temperature\n    self.params[\"max_new_tokens\"] = self.max_new_tokens\n    self.params[\"temperature\"] = self.temperature\n</code></pre>"},{"location":"api/llm_config/#llm_ie.engines.BasicLLMConfig.preprocess_messages","title":"preprocess_messages","text":"<pre><code>preprocess_messages(\n    messages: List[Dict[str, str]],\n) -&gt; List[Dict[str, str]]\n</code></pre> <p>This method preprocesses the input messages before passing them to the LLM.</p> Parameters: <p>messages : List[Dict[str,str]]     a list of dict with role and content. role must be one of {\"system\", \"user\", \"assistant\"}</p> Returns: <p>messages : List[Dict[str,str]]     a list of dict with role and content. role must be one of {\"system\", \"user\", \"assistant\"}</p> Source code in <code>package/llm-ie/src/llm_ie/engines.py</code> <pre><code>def preprocess_messages(self, messages:List[Dict[str,str]]) -&gt; List[Dict[str,str]]:\n    \"\"\"\n    This method preprocesses the input messages before passing them to the LLM.\n\n    Parameters:\n    ----------\n    messages : List[Dict[str,str]]\n        a list of dict with role and content. role must be one of {\"system\", \"user\", \"assistant\"}\n\n    Returns:\n    -------\n    messages : List[Dict[str,str]]\n        a list of dict with role and content. role must be one of {\"system\", \"user\", \"assistant\"}\n    \"\"\"\n    return messages.copy()\n</code></pre>"},{"location":"api/llm_config/#llm_ie.engines.BasicLLMConfig.postprocess_response","title":"postprocess_response","text":"<pre><code>postprocess_response(\n    response: Union[\n        str, Dict[str, str], Generator[str, None, None]\n    ],\n) -&gt; Union[\n    Dict[str, str], Generator[Dict[str, str], None, None]\n]\n</code></pre> <p>This method postprocesses the LLM response after it is generated.</p> Parameters: <p>response : Union[str, Dict[str, str], Generator[str, None, None]]     the LLM response. Can be a string or a generator.</p> <p>Returns: Union[Dict[str,str], Generator[Dict[str, str], None, None]]     the postprocessed LLM response.      If input is a string, the output will be a dict {\"response\": }.      if input is a generator, the output will be a generator {\"type\": \"response\", \"data\": }. Source code in <code>package/llm-ie/src/llm_ie/engines.py</code> <pre><code>def postprocess_response(self, response:Union[str, Dict[str, str], Generator[str, None, None]]) -&gt; Union[Dict[str,str], Generator[Dict[str, str], None, None]]:\n    \"\"\"\n    This method postprocesses the LLM response after it is generated.\n\n    Parameters:\n    ----------\n    response : Union[str, Dict[str, str], Generator[str, None, None]]\n        the LLM response. Can be a string or a generator.\n\n    Returns: Union[Dict[str,str], Generator[Dict[str, str], None, None]]\n        the postprocessed LLM response. \n        If input is a string, the output will be a dict {\"response\": &lt;response&gt;}. \n        if input is a generator, the output will be a generator {\"type\": \"response\", \"data\": &lt;content&gt;}.\n    \"\"\"\n    if isinstance(response, str):\n        return {\"response\": response}\n\n    elif isinstance(response, dict):\n        if \"response\" in response:\n            return response\n        else:\n            warnings.warn(f\"Invalid response dict keys: {response.keys()}. Returning default empty dict.\", UserWarning)\n            return {\"response\": \"\"}\n\n    elif isinstance(response, Generator):\n        def _process_stream():\n            for chunk in response:\n                if isinstance(chunk, dict):\n                    yield chunk\n                elif isinstance(chunk, str):\n                    yield {\"type\": \"response\", \"data\": chunk}\n\n        return _process_stream()\n\n    else:\n        warnings.warn(f\"Invalid response type: {type(response)}. Returning default empty dict.\", UserWarning)\n        return {\"response\": \"\"}\n</code></pre>"},{"location":"api/llm_config/#llm_ie.engines.OpenAIReasoningLLMConfig","title":"llm_ie.engines.OpenAIReasoningLLMConfig","text":"<pre><code>OpenAIReasoningLLMConfig(\n    reasoning_effort: str = None, **kwargs\n)\n</code></pre> <p>               Bases: <code>ReasoningLLMConfig</code></p> <p>The OpenAI \"o\" series configuration. 1. The reasoning effort as one of {\"low\", \"medium\", \"high\"}.     For models that do not support setting reasoning effort (e.g., o1-mini, o1-preview), set to None. 2. The temperature parameter is not supported and will be ignored. 3. The system prompt is not supported and will be concatenated to the next user prompt.</p> Parameters: <p>reasoning_effort : str, Optional     the reasoning effort. Must be one of {\"low\", \"medium\", \"high\"}. Default is \"low\".</p> Source code in <code>package/llm-ie/src/llm_ie/engines.py</code> <pre><code>def __init__(self, reasoning_effort:str=None, **kwargs):\n    \"\"\"\n    The OpenAI \"o\" series configuration.\n    1. The reasoning effort as one of {\"low\", \"medium\", \"high\"}.\n        For models that do not support setting reasoning effort (e.g., o1-mini, o1-preview), set to None.\n    2. The temperature parameter is not supported and will be ignored.\n    3. The system prompt is not supported and will be concatenated to the next user prompt.\n\n    Parameters:\n    ----------\n    reasoning_effort : str, Optional\n        the reasoning effort. Must be one of {\"low\", \"medium\", \"high\"}. Default is \"low\".\n    \"\"\"\n    super().__init__(**kwargs)\n    if reasoning_effort is not None:\n        if reasoning_effort not in [\"low\", \"medium\", \"high\"]:\n            raise ValueError(\"reasoning_effort must be one of {'low', 'medium', 'high'}.\")\n\n        self.reasoning_effort = reasoning_effort\n        self.params[\"reasoning_effort\"] = self.reasoning_effort\n\n    if \"temperature\" in self.params:\n        warnings.warn(\"Reasoning models do not support temperature parameter. Will be ignored.\", UserWarning)\n        self.params.pop(\"temperature\")\n</code></pre>"},{"location":"api/llm_config/#llm_ie.engines.OpenAIReasoningLLMConfig.preprocess_messages","title":"preprocess_messages","text":"<pre><code>preprocess_messages(\n    messages: List[Dict[str, str]],\n) -&gt; List[Dict[str, str]]\n</code></pre> <p>Concatenate system prompts to the next user prompt.</p> Parameters: <p>messages : List[Dict[str,str]]     a list of dict with role and content. role must be one of {\"system\", \"user\", \"assistant\"}</p> Returns: <p>messages : List[Dict[str,str]]     a list of dict with role and content. role must be one of {\"system\", \"user\", \"assistant\"}</p> Source code in <code>package/llm-ie/src/llm_ie/engines.py</code> <pre><code>def preprocess_messages(self, messages:List[Dict[str,str]]) -&gt; List[Dict[str,str]]:\n    \"\"\"\n    Concatenate system prompts to the next user prompt.\n\n    Parameters:\n    ----------\n    messages : List[Dict[str,str]]\n        a list of dict with role and content. role must be one of {\"system\", \"user\", \"assistant\"}\n\n    Returns:\n    -------\n    messages : List[Dict[str,str]]\n        a list of dict with role and content. role must be one of {\"system\", \"user\", \"assistant\"}\n    \"\"\"\n    system_prompt_holder = \"\"\n    new_messages = []\n    for i, message in enumerate(messages):\n        # if system prompt, store it in system_prompt_holder\n        if message['role'] == 'system':\n            system_prompt_holder = message['content']\n        # if user prompt, concatenate it with system_prompt_holder\n        elif message['role'] == 'user':\n            if system_prompt_holder:\n                new_message = {'role': message['role'], 'content': f\"{system_prompt_holder} {message['content']}\"}\n                system_prompt_holder = \"\"\n            else:\n                new_message = {'role': message['role'], 'content': message['content']}\n\n            new_messages.append(new_message)\n        # if assistant/other prompt, do nothing\n        else:\n            new_message = {'role': message['role'], 'content': message['content']}\n            new_messages.append(new_message)\n\n    return new_messages\n</code></pre>"},{"location":"api/llm_config/#llm_ie.engines.Qwen3LLMConfig","title":"llm_ie.engines.Qwen3LLMConfig","text":"<pre><code>Qwen3LLMConfig(thinking_mode: bool = True, **kwargs)\n</code></pre> <p>               Bases: <code>ReasoningLLMConfig</code></p> <p>The Qwen3 hybrid thinking LLM configuration.  For Qwen3 thinking 2507, use ReasoningLLMConfig instead; for Qwen3 Instruct, use BasicLLMConfig instead.</p> Parameters: <p>thinking_mode : bool, Optional     if True, a special token \"/think\" will be placed after each system and user prompt. Otherwise, \"/no_think\" will be placed.</p> Source code in <code>package/llm-ie/src/llm_ie/engines.py</code> <pre><code>def __init__(self, thinking_mode:bool=True, **kwargs):\n    \"\"\"\n    The Qwen3 **hybrid thinking** LLM configuration. \n    For Qwen3 thinking 2507, use ReasoningLLMConfig instead; for Qwen3 Instruct, use BasicLLMConfig instead.\n\n    Parameters:\n    ----------\n    thinking_mode : bool, Optional\n        if True, a special token \"/think\" will be placed after each system and user prompt. Otherwise, \"/no_think\" will be placed.\n    \"\"\"\n    super().__init__(**kwargs)\n    self.thinking_mode = thinking_mode\n</code></pre>"},{"location":"api/llm_config/#llm_ie.engines.Qwen3LLMConfig.preprocess_messages","title":"preprocess_messages","text":"<pre><code>preprocess_messages(\n    messages: List[Dict[str, str]],\n) -&gt; List[Dict[str, str]]\n</code></pre> <p>Append a special token to the system and user prompts. The token is \"/think\" if thinking_mode is True, otherwise \"/no_think\".</p> Parameters: <p>messages : List[Dict[str,str]]     a list of dict with role and content. role must be one of {\"system\", \"user\", \"assistant\"}</p> Returns: <p>messages : List[Dict[str,str]]     a list of dict with role and content. role must be one of {\"system\", \"user\", \"assistant\"}</p> Source code in <code>package/llm-ie/src/llm_ie/engines.py</code> <pre><code>def preprocess_messages(self, messages:List[Dict[str,str]]) -&gt; List[Dict[str,str]]:\n    \"\"\"\n    Append a special token to the system and user prompts.\n    The token is \"/think\" if thinking_mode is True, otherwise \"/no_think\".\n\n    Parameters:\n    ----------\n    messages : List[Dict[str,str]]\n        a list of dict with role and content. role must be one of {\"system\", \"user\", \"assistant\"}\n\n    Returns:\n    -------\n    messages : List[Dict[str,str]]\n        a list of dict with role and content. role must be one of {\"system\", \"user\", \"assistant\"}\n    \"\"\"\n    thinking_token = \"/think\" if self.thinking_mode else \"/no_think\"\n    new_messages = []\n    for message in messages:\n        if message['role'] in ['system', 'user']:\n            new_message = {'role': message['role'], 'content': f\"{message['content']} {thinking_token}\"}\n        else:\n            new_message = {'role': message['role'], 'content': message['content']}\n\n        new_messages.append(new_message)\n\n    return new_messages\n</code></pre>"},{"location":"api/llm_inference_engine/","title":"Engines API","text":""},{"location":"api/llm_inference_engine/#llm_ie.engines.InferenceEngine","title":"llm_ie.engines.InferenceEngine","text":"<pre><code>InferenceEngine(config: LLMConfig, **kwrs)\n</code></pre> <p>This is an abstract class to provide interfaces for LLM inference engines.  Children classes that inherts this class can be used in extrators. Must implement chat() method.</p> Parameters: <p>config : LLMConfig     the LLM configuration. Must be a child class of LLMConfig.</p> Source code in <code>package/llm-ie/src/llm_ie/engines.py</code> <pre><code>@abc.abstractmethod\ndef __init__(self, config:LLMConfig, **kwrs):\n    \"\"\"\n    This is an abstract class to provide interfaces for LLM inference engines. \n    Children classes that inherts this class can be used in extrators. Must implement chat() method.\n\n    Parameters:\n    ----------\n    config : LLMConfig\n        the LLM configuration. Must be a child class of LLMConfig.\n    \"\"\"\n    return NotImplemented\n</code></pre>"},{"location":"api/llm_inference_engine/#llm_ie.engines.InferenceEngine.chat","title":"chat  <code>abstractmethod</code>","text":"<pre><code>chat(\n    messages: List[Dict[str, str]],\n    verbose: bool = False,\n    stream: bool = False,\n    messages_logger: MessagesLogger = None,\n) -&gt; Union[\n    Dict[str, str], Generator[Dict[str, str], None, None]\n]\n</code></pre> <p>This method inputs chat messages and outputs LLM generated text.</p> Parameters: <p>messages : List[Dict[str,str]]     a list of dict with role and content. role must be one of {\"system\", \"user\", \"assistant\"} verbose : bool, Optional     if True, LLM generated text will be printed in terminal in real-time. stream : bool, Optional     if True, returns a generator that yields the output in real-time. Messages_logger : MessagesLogger, Optional     the message logger that logs the chat messages.</p> Returns: <p>response : Union[Dict[str,str], Generator[Dict[str, str], None, None]]     a dict {\"reasoning\": , \"response\": } or Generator {\"type\": , \"data\": } Source code in <code>package/llm-ie/src/llm_ie/engines.py</code> <pre><code>@abc.abstractmethod\ndef chat(self, messages:List[Dict[str,str]], verbose:bool=False, stream:bool=False, \n         messages_logger:MessagesLogger=None) -&gt; Union[Dict[str,str], Generator[Dict[str, str], None, None]]:\n    \"\"\"\n    This method inputs chat messages and outputs LLM generated text.\n\n    Parameters:\n    ----------\n    messages : List[Dict[str,str]]\n        a list of dict with role and content. role must be one of {\"system\", \"user\", \"assistant\"}\n    verbose : bool, Optional\n        if True, LLM generated text will be printed in terminal in real-time.\n    stream : bool, Optional\n        if True, returns a generator that yields the output in real-time.  \n    Messages_logger : MessagesLogger, Optional\n        the message logger that logs the chat messages.\n\n    Returns:\n    -------\n    response : Union[Dict[str,str], Generator[Dict[str, str], None, None]]\n        a dict {\"reasoning\": &lt;reasoning&gt;, \"response\": &lt;response&gt;} or Generator {\"type\": &lt;reasoning or response&gt;, \"data\": &lt;content&gt;}\n    \"\"\"\n    return NotImplemented\n</code></pre>"},{"location":"api/llm_inference_engine/#llm_ie.engines.OllamaInferenceEngine","title":"llm_ie.engines.OllamaInferenceEngine","text":"<pre><code>OllamaInferenceEngine(\n    model_name: str,\n    num_ctx: int = 4096,\n    keep_alive: int = 300,\n    config: LLMConfig = None,\n    **kwrs\n)\n</code></pre> <p>               Bases: <code>InferenceEngine</code></p> <p>The Ollama inference engine.</p> Parameters: <p>model_name : str     the model name exactly as shown in &gt;&gt; ollama ls num_ctx : int, Optional     context length that LLM will evaluate. keep_alive : int, Optional     seconds to hold the LLM after the last API call. config : LLMConfig     the LLM configuration.</p> Source code in <code>package/llm-ie/src/llm_ie/engines.py</code> <pre><code>def __init__(self, model_name:str, num_ctx:int=4096, keep_alive:int=300, config:LLMConfig=None, **kwrs):\n    \"\"\"\n    The Ollama inference engine.\n\n    Parameters:\n    ----------\n    model_name : str\n        the model name exactly as shown in &gt;&gt; ollama ls\n    num_ctx : int, Optional\n        context length that LLM will evaluate.\n    keep_alive : int, Optional\n        seconds to hold the LLM after the last API call.\n    config : LLMConfig\n        the LLM configuration. \n    \"\"\"\n    if importlib.util.find_spec(\"ollama\") is None:\n        raise ImportError(\"ollama-python not found. Please install ollama-python (```pip install ollama```).\")\n\n    from ollama import Client, AsyncClient\n    super().__init__(config)\n    self.client = Client(**kwrs)\n    self.async_client = AsyncClient(**kwrs)\n    self.model_name = model_name\n    self.num_ctx = num_ctx\n    self.keep_alive = keep_alive\n    self.config = config if config else BasicLLMConfig()\n    self.formatted_params = self._format_config()\n</code></pre>"},{"location":"api/llm_inference_engine/#llm_ie.engines.OllamaInferenceEngine.chat","title":"chat","text":"<pre><code>chat(\n    messages: List[Dict[str, str]],\n    verbose: bool = False,\n    stream: bool = False,\n    messages_logger: MessagesLogger = None,\n) -&gt; Union[\n    Dict[str, str], Generator[Dict[str, str], None, None]\n]\n</code></pre> <p>This method inputs chat messages and outputs VLM generated text.</p> Parameters: <p>messages : List[Dict[str,str]]     a list of dict with role and content. role must be one of {\"system\", \"user\", \"assistant\"} verbose : bool, Optional     if True, VLM generated text will be printed in terminal in real-time. stream : bool, Optional     if True, returns a generator that yields the output in real-time. Messages_logger : MessagesLogger, Optional     the message logger that logs the chat messages.</p> Returns: <p>response : Union[Dict[str,str], Generator[Dict[str, str], None, None]]     a dict {\"reasoning\": , \"response\": } or Generator {\"type\": , \"data\": } Source code in <code>package/llm-ie/src/llm_ie/engines.py</code> <pre><code>def chat(self, messages:List[Dict[str,str]], verbose:bool=False, stream:bool=False, \n         messages_logger:MessagesLogger=None) -&gt; Union[Dict[str,str], Generator[Dict[str, str], None, None]]:\n    \"\"\"\n    This method inputs chat messages and outputs VLM generated text.\n\n    Parameters:\n    ----------\n    messages : List[Dict[str,str]]\n        a list of dict with role and content. role must be one of {\"system\", \"user\", \"assistant\"}\n    verbose : bool, Optional\n        if True, VLM generated text will be printed in terminal in real-time.\n    stream : bool, Optional\n        if True, returns a generator that yields the output in real-time.\n    Messages_logger : MessagesLogger, Optional\n        the message logger that logs the chat messages.\n\n    Returns:\n    -------\n    response : Union[Dict[str,str], Generator[Dict[str, str], None, None]]\n        a dict {\"reasoning\": &lt;reasoning&gt;, \"response\": &lt;response&gt;} or Generator {\"type\": &lt;reasoning or response&gt;, \"data\": &lt;content&gt;}\n    \"\"\"\n    processed_messages = self.config.preprocess_messages(messages)\n\n    options={'num_ctx': self.num_ctx, **self.formatted_params}\n    if stream:\n        def _stream_generator():\n            response_stream = self.client.chat(\n                model=self.model_name, \n                messages=processed_messages, \n                options=options,\n                stream=True, \n                keep_alive=self.keep_alive\n            )\n            res = {\"reasoning\": \"\", \"response\": \"\"}\n            for chunk in response_stream:\n                if hasattr(chunk.message, 'thinking') and chunk.message.thinking:\n                    content_chunk = getattr(getattr(chunk, 'message', {}), 'thinking', '')\n                    res[\"reasoning\"] += content_chunk\n                    yield {\"type\": \"reasoning\", \"data\": content_chunk}\n                else:\n                    content_chunk = getattr(getattr(chunk, 'message', {}), 'content', '')\n                    res[\"response\"] += content_chunk\n                    yield {\"type\": \"response\", \"data\": content_chunk}\n\n                if chunk.done_reason == \"length\":\n                    warnings.warn(\"Model stopped generating due to context length limit.\", RuntimeWarning)\n\n            # Postprocess response\n            res_dict = self.config.postprocess_response(res)\n            # Write to messages log\n            if messages_logger:\n                processed_messages.append({\"role\": \"assistant\",\n                                            \"content\": res_dict.get(\"response\", \"\"),\n                                            \"reasoning\": res_dict.get(\"reasoning\", \"\")})\n                messages_logger.log_messages(processed_messages)\n\n        return self.config.postprocess_response(_stream_generator())\n\n    elif verbose:\n        response = self.client.chat(\n                        model=self.model_name, \n                        messages=processed_messages, \n                        options=options,\n                        stream=True,\n                        keep_alive=self.keep_alive\n                    )\n\n        res = {\"reasoning\": \"\", \"response\": \"\"}\n        phase = \"\"\n        for chunk in response:\n            if hasattr(chunk.message, 'thinking') and chunk.message.thinking:\n                if phase != \"reasoning\":\n                    print(\"\\n--- Reasoning ---\")\n                    phase = \"reasoning\"\n\n                content_chunk = getattr(getattr(chunk, 'message', {}), 'thinking', '')\n                res[\"reasoning\"] += content_chunk\n            else:\n                if phase != \"response\":\n                    print(\"\\n--- Response ---\")\n                    phase = \"response\"\n                content_chunk = getattr(getattr(chunk, 'message', {}), 'content', '')\n                res[\"response\"] += content_chunk\n\n            print(content_chunk, end='', flush=True)\n\n            if chunk.done_reason == \"length\":\n                warnings.warn(\"Model stopped generating due to context length limit.\", RuntimeWarning)\n        print('\\n')\n\n    else:\n        response = self.client.chat(\n                            model=self.model_name, \n                            messages=processed_messages, \n                            options=options,\n                            stream=False,\n                            keep_alive=self.keep_alive\n                        )\n        res = {\"reasoning\": getattr(getattr(response, 'message', {}), 'thinking', ''),\n               \"response\": getattr(getattr(response, 'message', {}), 'content', '')}\n\n        if response.done_reason == \"length\":\n            warnings.warn(\"Model stopped generating due to context length limit.\", RuntimeWarning)\n\n    # Postprocess response\n    res_dict = self.config.postprocess_response(res)\n    # Write to messages log\n    if messages_logger:\n        processed_messages.append({\"role\": \"assistant\", \n                                \"content\": res_dict.get(\"response\", \"\"), \n                                \"reasoning\": res_dict.get(\"reasoning\", \"\")})\n        messages_logger.log_messages(processed_messages)\n\n    return res_dict\n</code></pre>"},{"location":"api/llm_inference_engine/#llm_ie.engines.OllamaInferenceEngine.chat_async","title":"chat_async  <code>async</code>","text":"<pre><code>chat_async(\n    messages: List[Dict[str, str]],\n    messages_logger: MessagesLogger = None,\n) -&gt; Dict[str, str]\n</code></pre> <p>Async version of chat method. Streaming is not supported.</p> Source code in <code>package/llm-ie/src/llm_ie/engines.py</code> <pre><code>async def chat_async(self, messages:List[Dict[str,str]], messages_logger:MessagesLogger=None) -&gt; Dict[str,str]:\n    \"\"\"\n    Async version of chat method. Streaming is not supported.\n    \"\"\"\n    processed_messages = self.config.preprocess_messages(messages)\n\n    response = await self.async_client.chat(\n                        model=self.model_name, \n                        messages=processed_messages, \n                        options={'num_ctx': self.num_ctx, **self.formatted_params},\n                        stream=False,\n                        keep_alive=self.keep_alive\n                    )\n\n    res = {\"reasoning\": getattr(getattr(response, 'message', {}), 'thinking', ''),\n           \"response\": getattr(getattr(response, 'message', {}), 'content', '')}\n\n    if response.done_reason == \"length\":\n        warnings.warn(\"Model stopped generating due to context length limit.\", RuntimeWarning)\n    # Postprocess response\n    res_dict = self.config.postprocess_response(res)\n    # Write to messages log\n    if messages_logger:\n        processed_messages.append({\"role\": \"assistant\", \n                                    \"content\": res_dict.get(\"response\", \"\"), \n                                    \"reasoning\": res_dict.get(\"reasoning\", \"\")})\n        messages_logger.log_messages(processed_messages)\n\n    return res_dict\n</code></pre>"},{"location":"api/llm_inference_engine/#llm_ie.engines.OpenAIInferenceEngine","title":"llm_ie.engines.OpenAIInferenceEngine","text":"<pre><code>OpenAIInferenceEngine(\n    model: str, config: LLMConfig = None, **kwrs\n)\n</code></pre> <p>               Bases: <code>InferenceEngine</code></p> <p>The OpenAI API inference engine.  For parameters and documentation, refer to https://platform.openai.com/docs/api-reference/introduction</p> Parameters: <p>model_name : str     model name as described in https://platform.openai.com/docs/models</p> Source code in <code>package/llm-ie/src/llm_ie/engines.py</code> <pre><code>def __init__(self, model:str, config:LLMConfig=None, **kwrs):\n    \"\"\"\n    The OpenAI API inference engine. \n    For parameters and documentation, refer to https://platform.openai.com/docs/api-reference/introduction\n\n    Parameters:\n    ----------\n    model_name : str\n        model name as described in https://platform.openai.com/docs/models\n    \"\"\"\n    if importlib.util.find_spec(\"openai\") is None:\n        raise ImportError(\"OpenAI Python API library not found. Please install OpanAI (```pip install openai```).\")\n\n    from openai import OpenAI, AsyncOpenAI\n    super().__init__(config)\n    self.client = OpenAI(**kwrs)\n    self.async_client = AsyncOpenAI(**kwrs)\n    self.model = model\n    self.config = config if config else BasicLLMConfig()\n    self.formatted_params = self._format_config()\n</code></pre>"},{"location":"api/llm_inference_engine/#llm_ie.engines.OpenAIInferenceEngine.chat","title":"chat","text":"<pre><code>chat(\n    messages: List[Dict[str, str]],\n    verbose: bool = False,\n    stream: bool = False,\n    messages_logger: MessagesLogger = None,\n) -&gt; Union[\n    Dict[str, str], Generator[Dict[str, str], None, None]\n]\n</code></pre> <p>This method inputs chat messages and outputs LLM generated text.</p> Parameters: <p>messages : List[Dict[str,str]]     a list of dict with role and content. role must be one of {\"system\", \"user\", \"assistant\"} verbose : bool, Optional     if True, VLM generated text will be printed in terminal in real-time. stream : bool, Optional     if True, returns a generator that yields the output in real-time. messages_logger : MessagesLogger, Optional     the message logger that logs the chat messages.</p> Returns: <p>response : Union[Dict[str,str], Generator[Dict[str, str], None, None]]     a dict {\"reasoning\": , \"response\": } or Generator {\"type\": , \"data\": } Source code in <code>package/llm-ie/src/llm_ie/engines.py</code> <pre><code>def chat(self, messages:List[Dict[str,str]], verbose:bool=False, stream:bool=False, messages_logger:MessagesLogger=None) -&gt; Union[Dict[str, str], Generator[Dict[str, str], None, None]]:\n    \"\"\"\n    This method inputs chat messages and outputs LLM generated text.\n\n    Parameters:\n    ----------\n    messages : List[Dict[str,str]]\n        a list of dict with role and content. role must be one of {\"system\", \"user\", \"assistant\"}\n    verbose : bool, Optional\n        if True, VLM generated text will be printed in terminal in real-time.\n    stream : bool, Optional\n        if True, returns a generator that yields the output in real-time.\n    messages_logger : MessagesLogger, Optional\n        the message logger that logs the chat messages.\n\n    Returns:\n    -------\n    response : Union[Dict[str,str], Generator[Dict[str, str], None, None]]\n        a dict {\"reasoning\": &lt;reasoning&gt;, \"response\": &lt;response&gt;} or Generator {\"type\": &lt;reasoning or response&gt;, \"data\": &lt;content&gt;}\n    \"\"\"\n    processed_messages = self.config.preprocess_messages(messages)\n\n    if stream:\n        def _stream_generator():\n            response_stream = self.client.chat.completions.create(\n                                    model=self.model,\n                                    messages=processed_messages,\n                                    stream=True,\n                                    **self.formatted_params\n                                )\n            res_text = \"\"\n            for chunk in response_stream:\n                if len(chunk.choices) &gt; 0:\n                    chunk_text = chunk.choices[0].delta.content\n                    if chunk_text is not None:\n                        res_text += chunk_text\n                        yield chunk_text\n                    if chunk.choices[0].finish_reason == \"length\":\n                        warnings.warn(\"Model stopped generating due to context length limit.\", RuntimeWarning)\n\n            # Postprocess response\n            res_dict = self.config.postprocess_response(res_text)\n            # Write to messages log\n            if messages_logger:\n                processed_messages.append({\"role\": \"assistant\",\n                                            \"content\": res_dict.get(\"response\", \"\"),\n                                            \"reasoning\": res_dict.get(\"reasoning\", \"\")})\n                messages_logger.log_messages(processed_messages)\n\n        return self.config.postprocess_response(_stream_generator())\n\n    elif verbose:\n        response = self.client.chat.completions.create(\n            model=self.model,\n            messages=processed_messages,\n            stream=True,\n            **self.formatted_params\n        )\n        res = ''\n        for chunk in response:\n            if len(chunk.choices) &gt; 0:\n                if chunk.choices[0].delta.content is not None:\n                    res += chunk.choices[0].delta.content\n                    print(chunk.choices[0].delta.content, end=\"\", flush=True)\n                if chunk.choices[0].finish_reason == \"length\":\n                    warnings.warn(\"Model stopped generating due to context length limit.\", RuntimeWarning)\n\n        print('\\n')\n\n    else:\n        response = self.client.chat.completions.create(\n            model=self.model,\n            messages=processed_messages,\n            stream=False,\n            **self.formatted_params\n        )\n        res = response.choices[0].message.content\n\n    # Postprocess response\n    res_dict = self.config.postprocess_response(res)\n    # Write to messages log\n    if messages_logger:\n        processed_messages.append({\"role\": \"assistant\", \n                                \"content\": res_dict.get(\"response\", \"\"), \n                                \"reasoning\": res_dict.get(\"reasoning\", \"\")})\n        messages_logger.log_messages(processed_messages)\n\n    return res_dict\n</code></pre>"},{"location":"api/llm_inference_engine/#llm_ie.engines.OpenAIInferenceEngine.chat_async","title":"chat_async  <code>async</code>","text":"<pre><code>chat_async(\n    messages: List[Dict[str, str]],\n    messages_logger: MessagesLogger = None,\n) -&gt; Dict[str, str]\n</code></pre> <p>Async version of chat method. Streaming is not supported.</p> Source code in <code>package/llm-ie/src/llm_ie/engines.py</code> <pre><code>async def chat_async(self, messages:List[Dict[str,str]], messages_logger:MessagesLogger=None) -&gt; Dict[str,str]:\n    \"\"\"\n    Async version of chat method. Streaming is not supported.\n    \"\"\"\n    processed_messages = self.config.preprocess_messages(messages)\n\n    response = await self.async_client.chat.completions.create(\n        model=self.model,\n        messages=processed_messages,\n        stream=False,\n        **self.formatted_params\n    )\n\n    if response.choices[0].finish_reason == \"length\":\n        warnings.warn(\"Model stopped generating due to context length limit.\", RuntimeWarning)\n\n    res = response.choices[0].message.content\n    # Postprocess response\n    res_dict = self.config.postprocess_response(res)\n    # Write to messages log\n    if messages_logger:\n        processed_messages.append({\"role\": \"assistant\", \n                                \"content\": res_dict.get(\"response\", \"\"), \n                                \"reasoning\": res_dict.get(\"reasoning\", \"\")})\n        messages_logger.log_messages(processed_messages)\n\n    return res_dict\n</code></pre>"},{"location":"api/llm_inference_engine/#llm_ie.engines.AzureOpenAIInferenceEngine","title":"llm_ie.engines.AzureOpenAIInferenceEngine","text":"<pre><code>AzureOpenAIInferenceEngine(\n    model: str,\n    api_version: str,\n    config: LLMConfig = None,\n    **kwrs\n)\n</code></pre> <p>               Bases: <code>OpenAIInferenceEngine</code></p> <p>The Azure OpenAI API inference engine. For parameters and documentation, refer to  - https://azure.microsoft.com/en-us/products/ai-services/openai-service - https://learn.microsoft.com/en-us/azure/ai-services/openai/quickstart</p> Parameters: <p>model : str     model name as described in https://platform.openai.com/docs/models api_version : str     the Azure OpenAI API version config : LLMConfig     the LLM configuration.</p> Source code in <code>package/llm-ie/src/llm_ie/engines.py</code> <pre><code>def __init__(self, model:str, api_version:str, config:LLMConfig=None, **kwrs):\n    \"\"\"\n    The Azure OpenAI API inference engine.\n    For parameters and documentation, refer to \n    - https://azure.microsoft.com/en-us/products/ai-services/openai-service\n    - https://learn.microsoft.com/en-us/azure/ai-services/openai/quickstart\n\n    Parameters:\n    ----------\n    model : str\n        model name as described in https://platform.openai.com/docs/models\n    api_version : str\n        the Azure OpenAI API version\n    config : LLMConfig\n        the LLM configuration.\n    \"\"\"\n    if importlib.util.find_spec(\"openai\") is None:\n        raise ImportError(\"OpenAI Python API library not found. Please install OpanAI (```pip install openai```).\")\n\n    from openai import AzureOpenAI, AsyncAzureOpenAI\n    self.model = model\n    self.api_version = api_version\n    self.client = AzureOpenAI(api_version=self.api_version, \n                              **kwrs)\n    self.async_client = AsyncAzureOpenAI(api_version=self.api_version, \n                                         **kwrs)\n    self.config = config if config else BasicLLMConfig()\n    self.formatted_params = self._format_config()\n</code></pre>"},{"location":"api/llm_inference_engine/#llm_ie.engines.HuggingFaceHubInferenceEngine","title":"llm_ie.engines.HuggingFaceHubInferenceEngine","text":"<pre><code>HuggingFaceHubInferenceEngine(\n    model: str = None,\n    token: Union[str, bool] = None,\n    base_url: str = None,\n    api_key: str = None,\n    config: LLMConfig = None,\n    **kwrs\n)\n</code></pre> <p>               Bases: <code>InferenceEngine</code></p> <p>The Huggingface_hub InferenceClient inference engine. For parameters and documentation, refer to https://huggingface.co/docs/huggingface_hub/en/package_reference/inference_client</p> Parameters: <p>model : str     the model name exactly as shown in Huggingface repo token : str, Optional     the Huggingface token. If None, will use the token in os.environ['HF_TOKEN']. base_url : str, Optional     the base url for the LLM server. If None, will use the default Huggingface Hub URL. api_key : str, Optional     the API key for the LLM server.  config : LLMConfig     the LLM configuration.</p> Source code in <code>package/llm-ie/src/llm_ie/engines.py</code> <pre><code>def __init__(self, model:str=None, token:Union[str, bool]=None, base_url:str=None, api_key:str=None, config:LLMConfig=None, **kwrs):\n    \"\"\"\n    The Huggingface_hub InferenceClient inference engine.\n    For parameters and documentation, refer to https://huggingface.co/docs/huggingface_hub/en/package_reference/inference_client\n\n    Parameters:\n    ----------\n    model : str\n        the model name exactly as shown in Huggingface repo\n    token : str, Optional\n        the Huggingface token. If None, will use the token in os.environ['HF_TOKEN'].\n    base_url : str, Optional\n        the base url for the LLM server. If None, will use the default Huggingface Hub URL.\n    api_key : str, Optional\n        the API key for the LLM server. \n    config : LLMConfig\n        the LLM configuration. \n    \"\"\"\n    if importlib.util.find_spec(\"huggingface_hub\") is None:\n        raise ImportError(\"huggingface-hub not found. Please install huggingface-hub (```pip install huggingface-hub```).\")\n\n    from huggingface_hub import InferenceClient, AsyncInferenceClient\n    super().__init__(config)\n    self.model = model\n    self.base_url = base_url\n    self.client = InferenceClient(model=model, token=token, base_url=base_url, api_key=api_key, **kwrs)\n    self.client_async = AsyncInferenceClient(model=model, token=token, base_url=base_url, api_key=api_key, **kwrs)\n    self.config = config if config else BasicLLMConfig()\n    self.formatted_params = self._format_config()\n</code></pre>"},{"location":"api/llm_inference_engine/#llm_ie.engines.HuggingFaceHubInferenceEngine.chat","title":"chat","text":"<pre><code>chat(\n    messages: List[Dict[str, str]],\n    verbose: bool = False,\n    stream: bool = False,\n    messages_logger: MessagesLogger = None,\n) -&gt; Union[\n    Dict[str, str], Generator[Dict[str, str], None, None]\n]\n</code></pre> <p>This method inputs chat messages and outputs LLM generated text.</p> Parameters: <p>messages : List[Dict[str,str]]     a list of dict with role and content. role must be one of {\"system\", \"user\", \"assistant\"} verbose : bool, Optional     if True, VLM generated text will be printed in terminal in real-time. stream : bool, Optional     if True, returns a generator that yields the output in real-time. messages_logger : MessagesLogger, Optional     the message logger that logs the chat messages.</p> Returns: <p>response : Union[Dict[str,str], Generator[Dict[str, str], None, None]]     a dict {\"reasoning\": , \"response\": } or Generator {\"type\": , \"data\": } Source code in <code>package/llm-ie/src/llm_ie/engines.py</code> <pre><code>def chat(self, messages:List[Dict[str,str]], verbose:bool=False, stream:bool=False, \n         messages_logger:MessagesLogger=None) -&gt; Union[Dict[str,str], Generator[Dict[str, str], None, None]]:\n    \"\"\"\n    This method inputs chat messages and outputs LLM generated text.\n\n    Parameters:\n    ----------\n    messages : List[Dict[str,str]]\n        a list of dict with role and content. role must be one of {\"system\", \"user\", \"assistant\"}\n    verbose : bool, Optional\n        if True, VLM generated text will be printed in terminal in real-time.\n    stream : bool, Optional\n        if True, returns a generator that yields the output in real-time.\n    messages_logger : MessagesLogger, Optional\n        the message logger that logs the chat messages.\n\n    Returns:\n    -------\n    response : Union[Dict[str,str], Generator[Dict[str, str], None, None]]\n        a dict {\"reasoning\": &lt;reasoning&gt;, \"response\": &lt;response&gt;} or Generator {\"type\": &lt;reasoning or response&gt;, \"data\": &lt;content&gt;}\n    \"\"\"\n    processed_messages = self.config.preprocess_messages(messages)\n\n    if stream:\n        def _stream_generator():\n            response_stream = self.client.chat.completions.create(\n                                messages=processed_messages,\n                                stream=True,\n                                **self.formatted_params\n                            )\n            res_text = \"\"\n            for chunk in response_stream:\n                content_chunk = chunk.get('choices')[0].get('delta').get('content')\n                if content_chunk:\n                    res_text += content_chunk\n                    yield content_chunk\n\n            # Postprocess response\n            res_dict = self.config.postprocess_response(res_text)\n            # Write to messages log\n            if messages_logger:\n                processed_messages.append({\"role\": \"assistant\",\n                                            \"content\": res_dict.get(\"response\", \"\"),\n                                            \"reasoning\": res_dict.get(\"reasoning\", \"\")})\n                messages_logger.log_messages(processed_messages)\n\n        return self.config.postprocess_response(_stream_generator())\n\n    elif verbose:\n        response = self.client.chat.completions.create(\n                        messages=processed_messages,\n                        stream=True,\n                        **self.formatted_params\n                    )\n\n        res = ''\n        for chunk in response:\n            content_chunk = chunk.get('choices')[0].get('delta').get('content')\n            if content_chunk:\n                res += content_chunk\n                print(content_chunk, end='', flush=True)\n\n\n    else:\n        response = self.client.chat.completions.create(\n                            messages=processed_messages,\n                            stream=False,\n                            **self.formatted_params\n                        )\n        res = response.choices[0].message.content\n\n    # Postprocess response\n    res_dict = self.config.postprocess_response(res)\n    # Write to messages log\n    if messages_logger:\n        processed_messages.append({\"role\": \"assistant\", \n                                \"content\": res_dict.get(\"response\", \"\"), \n                                \"reasoning\": res_dict.get(\"reasoning\", \"\")})\n        messages_logger.log_messages(processed_messages)\n\n    return res_dict\n</code></pre>"},{"location":"api/llm_inference_engine/#llm_ie.engines.HuggingFaceHubInferenceEngine.chat_async","title":"chat_async  <code>async</code>","text":"<pre><code>chat_async(\n    messages: List[Dict[str, str]],\n    messages_logger: MessagesLogger = None,\n) -&gt; Dict[str, str]\n</code></pre> <p>Async version of chat method. Streaming is not supported.</p> Source code in <code>package/llm-ie/src/llm_ie/engines.py</code> <pre><code>async def chat_async(self, messages:List[Dict[str,str]], messages_logger:MessagesLogger=None) -&gt; Dict[str,str]:\n    \"\"\"\n    Async version of chat method. Streaming is not supported.\n    \"\"\"\n    processed_messages = self.config.preprocess_messages(messages)\n\n    response = await self.client_async.chat.completions.create(\n                messages=processed_messages,\n                stream=False,\n                **self.formatted_params\n            )\n\n    res = response.choices[0].message.content\n    # Postprocess response\n    res_dict = self.config.postprocess_response(res)\n    # Write to messages log\n    if messages_logger:\n        processed_messages.append({\"role\": \"assistant\", \n                                    \"content\": res_dict.get(\"response\", \"\"), \n                                    \"reasoning\": res_dict.get(\"reasoning\", \"\")})\n        messages_logger.log_messages(processed_messages)\n\n    return res_dict\n</code></pre>"},{"location":"api/llm_inference_engine/#llm_ie.engines.LiteLLMInferenceEngine","title":"llm_ie.engines.LiteLLMInferenceEngine","text":"<pre><code>LiteLLMInferenceEngine(\n    model: str = None,\n    base_url: str = None,\n    api_key: str = None,\n    config: LLMConfig = None,\n)\n</code></pre> <p>               Bases: <code>InferenceEngine</code></p> <p>The LiteLLM inference engine.  For parameters and documentation, refer to https://github.com/BerriAI/litellm?tab=readme-ov-file</p> Parameters: <p>model : str     the model name base_url : str, Optional     the base url for the LLM server api_key : str, Optional     the API key for the LLM server config : LLMConfig     the LLM configuration.</p> Source code in <code>package/llm-ie/src/llm_ie/engines.py</code> <pre><code>def __init__(self, model:str=None, base_url:str=None, api_key:str=None, config:LLMConfig=None):\n    \"\"\"\n    The LiteLLM inference engine. \n    For parameters and documentation, refer to https://github.com/BerriAI/litellm?tab=readme-ov-file\n\n    Parameters:\n    ----------\n    model : str\n        the model name\n    base_url : str, Optional\n        the base url for the LLM server\n    api_key : str, Optional\n        the API key for the LLM server\n    config : LLMConfig\n        the LLM configuration.\n    \"\"\"\n    if importlib.util.find_spec(\"litellm\") is None:\n        raise ImportError(\"litellm not found. Please install litellm (```pip install litellm```).\")\n\n    import litellm \n    super().__init__(config)\n    self.litellm = litellm\n    self.model = model\n    self.base_url = base_url\n    self.api_key = api_key\n    self.config = config if config else BasicLLMConfig()\n    self.formatted_params = self._format_config()\n</code></pre>"},{"location":"api/llm_inference_engine/#llm_ie.engines.LiteLLMInferenceEngine.chat","title":"chat","text":"<pre><code>chat(\n    messages: List[Dict[str, str]],\n    verbose: bool = False,\n    stream: bool = False,\n    messages_logger: MessagesLogger = None,\n) -&gt; Union[\n    Dict[str, str], Generator[Dict[str, str], None, None]\n]\n</code></pre> <p>This method inputs chat messages and outputs LLM generated text.</p> Parameters: <p>messages : List[Dict[str,str]]     a list of dict with role and content. role must be one of {\"system\", \"user\", \"assistant\"}  verbose : bool, Optional     if True, VLM generated text will be printed in terminal in real-time. stream : bool, Optional     if True, returns a generator that yields the output in real-time. messages_logger: MessagesLogger, Optional     a messages logger that logs the messages.</p> Returns: <p>response : Union[Dict[str,str], Generator[Dict[str, str], None, None]]     a dict {\"reasoning\": , \"response\": } or Generator {\"type\": , \"data\": } Source code in <code>package/llm-ie/src/llm_ie/engines.py</code> <pre><code>def chat(self, messages:List[Dict[str,str]], verbose:bool=False, stream:bool=False, messages_logger:MessagesLogger=None) -&gt; Union[Dict[str,str], Generator[Dict[str, str], None, None]]:\n    \"\"\"\n    This method inputs chat messages and outputs LLM generated text.\n\n    Parameters:\n    ----------\n    messages : List[Dict[str,str]]\n        a list of dict with role and content. role must be one of {\"system\", \"user\", \"assistant\"} \n    verbose : bool, Optional\n        if True, VLM generated text will be printed in terminal in real-time.\n    stream : bool, Optional\n        if True, returns a generator that yields the output in real-time.\n    messages_logger: MessagesLogger, Optional\n        a messages logger that logs the messages.\n\n    Returns:\n    -------\n    response : Union[Dict[str,str], Generator[Dict[str, str], None, None]]\n        a dict {\"reasoning\": &lt;reasoning&gt;, \"response\": &lt;response&gt;} or Generator {\"type\": &lt;reasoning or response&gt;, \"data\": &lt;content&gt;}\n    \"\"\"\n    processed_messages = self.config.preprocess_messages(messages)\n\n    if stream:\n        def _stream_generator():\n            response_stream = self.litellm.completion(\n                model=self.model,\n                messages=processed_messages,\n                stream=True,\n                base_url=self.base_url,\n                api_key=self.api_key,\n                **self.formatted_params\n            )\n            res_text = \"\"\n            for chunk in response_stream:\n                chunk_content = chunk.get('choices')[0].get('delta').get('content')\n                if chunk_content:\n                    res_text += chunk_content\n                    yield chunk_content\n\n            # Postprocess response\n            res_dict = self.config.postprocess_response(res_text)\n            # Write to messages log\n            if messages_logger:\n                processed_messages.append({\"role\": \"assistant\",\n                                            \"content\": res_dict.get(\"response\", \"\"),\n                                            \"reasoning\": res_dict.get(\"reasoning\", \"\")})\n                messages_logger.log_messages(processed_messages)\n\n        return self.config.postprocess_response(_stream_generator())\n\n    elif verbose:\n        response = self.litellm.completion(\n            model=self.model,\n            messages=processed_messages,\n            stream=True,\n            base_url=self.base_url,\n            api_key=self.api_key,\n            **self.formatted_params\n        )\n\n        res = ''\n        for chunk in response:\n            chunk_content = chunk.get('choices')[0].get('delta').get('content')\n            if chunk_content:\n                res += chunk_content\n                print(chunk_content, end='', flush=True)\n\n    else:\n        response = self.litellm.completion(\n                model=self.model,\n                messages=processed_messages,\n                stream=False,\n                base_url=self.base_url,\n                api_key=self.api_key,\n                **self.formatted_params\n            )\n        res = response.choices[0].message.content\n\n    # Postprocess response\n    res_dict = self.config.postprocess_response(res)\n    # Write to messages log\n    if messages_logger:\n        processed_messages.append({\"role\": \"assistant\", \n                                    \"content\": res_dict.get(\"response\", \"\"), \n                                    \"reasoning\": res_dict.get(\"reasoning\", \"\")})\n        messages_logger.log_messages(processed_messages)\n\n    return res_dict\n</code></pre>"},{"location":"api/llm_inference_engine/#llm_ie.engines.LiteLLMInferenceEngine.chat_async","title":"chat_async  <code>async</code>","text":"<pre><code>chat_async(\n    messages: List[Dict[str, str]],\n    messages_logger: MessagesLogger = None,\n) -&gt; Dict[str, str]\n</code></pre> <p>Async version of chat method. Streaming is not supported.</p> Source code in <code>package/llm-ie/src/llm_ie/engines.py</code> <pre><code>async def chat_async(self, messages:List[Dict[str,str]], messages_logger:MessagesLogger=None) -&gt; Dict[str,str]:\n    \"\"\"\n    Async version of chat method. Streaming is not supported.\n    \"\"\"\n    processed_messages = self.config.preprocess_messages(messages)\n\n    response = await self.litellm.acompletion(\n        model=self.model,\n        messages=processed_messages,\n        stream=False,\n        base_url=self.base_url,\n        api_key=self.api_key,\n        **self.formatted_params\n    )\n\n    res = response.get('choices')[0].get('message').get('content')\n\n    # Postprocess response\n    res_dict = self.config.postprocess_response(res)\n    # Write to messages log\n    if messages_logger:\n        processed_messages.append({\"role\": \"assistant\", \n                                \"content\": res_dict.get(\"response\", \"\"), \n                                \"reasoning\": res_dict.get(\"reasoning\", \"\")})\n        messages_logger.log_messages(processed_messages)\n    return res_dict\n</code></pre>"},{"location":"api/prompt_editor/","title":"Prompt Edirot API","text":""},{"location":"api/prompt_editor/#llm_ie.prompt_editor.PromptEditor","title":"llm_ie.prompt_editor.PromptEditor","text":"<pre><code>PromptEditor(\n    inference_engine: InferenceEngine,\n    extractor: FrameExtractor,\n    prompt_guide: str = None,\n)\n</code></pre> <p>This class is a LLM agent that rewrite or comment a prompt draft based on the prompt guide of an extractor.</p> <p>Parameters:</p> Name Type Description Default <code>inference_engine</code> <code>InferenceEngine</code> <p>the LLM inferencing engine object. Must implements the chat() method.</p> required <code>extractor</code> <code>FrameExtractor</code> <p>a FrameExtractor.</p> required <code>prompt_guide</code> <code>str</code> <p>the prompt guide for the extractor.  All built-in extractors have a prompt guide in the asset folder. Passing values to this parameter  will override the built-in prompt guide which is not recommended. For custom extractors, this parameter must be provided.</p> <code>None</code> Source code in <code>package/llm-ie/src/llm_ie/prompt_editor.py</code> <pre><code>def __init__(self, inference_engine:InferenceEngine, extractor:FrameExtractor, prompt_guide:str=None):\n    \"\"\"\n    This class is a LLM agent that rewrite or comment a prompt draft based on the prompt guide of an extractor.\n\n    Parameters\n    ----------\n    inference_engine : InferenceEngine\n        the LLM inferencing engine object. Must implements the chat() method.\n    extractor : FrameExtractor\n        a FrameExtractor. \n    prompt_guide : str, optional\n        the prompt guide for the extractor. \n        All built-in extractors have a prompt guide in the asset folder. Passing values to this parameter \n        will override the built-in prompt guide which is not recommended.\n        For custom extractors, this parameter must be provided.\n    \"\"\"\n    self.inference_engine = inference_engine\n\n    # if prompt_guide is provided, use it anyways\n    if prompt_guide:\n        self.prompt_guide = prompt_guide\n    # if prompt_guide is not provided, get it from the extractor\n    else:\n        self.prompt_guide = extractor.get_prompt_guide()\n        # when extractor does not have a prompt guide (e.g. custom extractor), ValueError\n        if self.prompt_guide is None:\n            raise ValueError(f\"Prompt guide for {extractor.__class__.__name__} is not available. Use `prompt_guide` parameter to provide a prompt guide.\")\n\n    # get system prompt\n    file_path = importlib.resources.files('llm_ie.asset.PromptEditor_prompts').joinpath('system.txt')\n    with open(file_path, 'r') as f:\n        self.system_prompt =  f.read()\n\n    # internal memory (history messages) for the `chat` method\n    self.messages = []\n</code></pre>"},{"location":"api/prompt_editor/#llm_ie.prompt_editor.PromptEditor.rewrite","title":"rewrite","text":"<pre><code>rewrite(draft: str) -&gt; str\n</code></pre> <p>This method inputs a prompt draft and rewrites it following the extractor's guideline. This method is stateless.</p> Source code in <code>package/llm-ie/src/llm_ie/prompt_editor.py</code> <pre><code>def rewrite(self, draft:str) -&gt; str:\n    \"\"\"\n    This method inputs a prompt draft and rewrites it following the extractor's guideline.\n    This method is stateless.\n    \"\"\"\n    file_path = importlib.resources.files('llm_ie.asset.PromptEditor_prompts').joinpath('rewrite.txt')\n    with open(file_path, 'r') as f:\n        rewrite_prompt_template = f.read()\n\n    prompt = self._apply_prompt_template(text_content={\"draft\": draft, \"prompt_guideline\": self.prompt_guide}, \n                                         prompt_template=rewrite_prompt_template)\n    messages = [{\"role\": \"system\", \"content\": self.system_prompt},\n                {\"role\": \"user\", \"content\": prompt}]\n    res = self.inference_engine.chat(messages, verbose=True)\n    return res[\"response\"]\n</code></pre>"},{"location":"api/prompt_editor/#llm_ie.prompt_editor.PromptEditor.comment","title":"comment","text":"<pre><code>comment(draft: str) -&gt; str\n</code></pre> <p>This method inputs a prompt draft and comment following the extractor's guideline. This method is stateless.</p> Source code in <code>package/llm-ie/src/llm_ie/prompt_editor.py</code> <pre><code>def comment(self, draft:str) -&gt; str:\n    \"\"\"\n    This method inputs a prompt draft and comment following the extractor's guideline.\n    This method is stateless.\n    \"\"\"\n    file_path = importlib.resources.files('llm_ie.asset.PromptEditor_prompts').joinpath('comment.txt')\n    with open(file_path, 'r') as f:\n        comment_prompt_template = f.read()\n\n    prompt = self._apply_prompt_template(text_content={\"draft\": draft, \"prompt_guideline\": self.prompt_guide}, \n                                         prompt_template=comment_prompt_template)\n    messages = [{\"role\": \"system\", \"content\": self.system_prompt},\n                {\"role\": \"user\", \"content\": prompt}]\n    res = self.inference_engine.chat(messages, verbose=True)\n    return res[\"response\"]\n</code></pre>"},{"location":"api/prompt_editor/#llm_ie.prompt_editor.PromptEditor.clear_messages","title":"clear_messages","text":"<pre><code>clear_messages()\n</code></pre> <p>Clears the current chat history.</p> Source code in <code>package/llm-ie/src/llm_ie/prompt_editor.py</code> <pre><code>def clear_messages(self):\n    \"\"\"\n    Clears the current chat history.\n    \"\"\"\n    self.messages = []\n</code></pre>"},{"location":"api/prompt_editor/#llm_ie.prompt_editor.PromptEditor.export_chat","title":"export_chat","text":"<pre><code>export_chat(file_path: str)\n</code></pre> <p>Exports the current chat history to a JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>path to the file where the chat history will be saved. Should have a .json extension.</p> required Source code in <code>package/llm-ie/src/llm_ie/prompt_editor.py</code> <pre><code>def export_chat(self, file_path: str):\n    \"\"\"\n    Exports the current chat history to a JSON file.\n\n    Parameters\n    ----------\n    file_path : str\n        path to the file where the chat history will be saved.\n        Should have a .json extension.\n    \"\"\"\n    if not self.messages:\n        raise ValueError(\"Chat history is empty. Nothing to export.\")\n\n    with open(file_path, 'w', encoding='utf-8') as f:\n        json.dump(self.messages, f, indent=4)\n</code></pre>"},{"location":"api/prompt_editor/#llm_ie.prompt_editor.PromptEditor.import_chat","title":"import_chat","text":"<pre><code>import_chat(file_path: str)\n</code></pre> <p>Imports a chat history from a JSON file, overwriting the current history.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>The path to the .json file containing the chat history.</p> required Source code in <code>package/llm-ie/src/llm_ie/prompt_editor.py</code> <pre><code>def import_chat(self, file_path: str):\n    \"\"\"\n    Imports a chat history from a JSON file, overwriting the current history.\n\n    Parameters\n    ----------\n    file_path : str\n        The path to the .json file containing the chat history.\n    \"\"\"\n    with open(file_path, 'r', encoding='utf-8') as f:\n        loaded_messages = json.load(f)\n\n    # Validate the loaded messages format.\n    if not isinstance(loaded_messages, list):\n        raise TypeError(\"Invalid format: The file should contain a JSON list of messages.\")\n    for message in loaded_messages:\n        if not (isinstance(message, dict) and 'role' in message and 'content' in message):\n            raise ValueError(\"Invalid format: Each message must be a dictionary with 'role' and 'content' keys.\")\n\n    self.messages = loaded_messages\n</code></pre>"},{"location":"api/prompt_editor/#llm_ie.prompt_editor.PromptEditor.chat","title":"chat","text":"<pre><code>chat()\n</code></pre> <p>External method that detects the environment and calls the appropriate chat method. This method use and updates the <code>messages</code> list (internal memory). This method is stateful.</p> Source code in <code>package/llm-ie/src/llm_ie/prompt_editor.py</code> <pre><code>def chat(self):\n    \"\"\"\n    External method that detects the environment and calls the appropriate chat method.\n    This method use and updates the `messages` list (internal memory).\n    This method is stateful.\n    \"\"\"\n    # Check if the conversation is empty, if so, load the initial chat prompt template.\n    if len(self.messages) == 0:\n        file_path = importlib.resources.files('llm_ie.asset.PromptEditor_prompts').joinpath('chat.txt')\n        with open(file_path, 'r') as f:\n            chat_prompt_template = f.read()\n\n        guideline = self._apply_prompt_template(text_content={\"prompt_guideline\": self.prompt_guide}, \n                                                prompt_template=chat_prompt_template)\n\n        self.messages = [{\"role\": \"system\", \"content\": self.system_prompt + guideline}]\n\n    if 'ipykernel' in sys.modules:\n        self._IPython_chat()\n    else:\n        self._terminal_chat()\n</code></pre>"},{"location":"api/prompt_editor/#llm_ie.prompt_editor.PromptEditor.chat_stream","title":"chat_stream","text":"<pre><code>chat_stream(\n    messages: List[Dict[str, str]],\n) -&gt; Generator[str, None, None]\n</code></pre> <p>This method processes messages and yields response chunks from the inference engine. This is for frontend App. This method is stateless.</p> Parameters: <p>messages : List[Dict[str, str]]     List of message dictionaries (e.g., [{\"role\": \"user\", \"content\": \"Hi\"}]).</p> Yields: <pre><code>Chunks of the assistant's response.\n</code></pre> Source code in <code>package/llm-ie/src/llm_ie/prompt_editor.py</code> <pre><code>def chat_stream(self, messages: List[Dict[str, str]]) -&gt; Generator[str, None, None]:\n    \"\"\"\n    This method processes messages and yields response chunks from the inference engine.\n    This is for frontend App.\n    This method is stateless.\n\n    Parameters:\n    ----------\n    messages : List[Dict[str, str]]\n        List of message dictionaries (e.g., [{\"role\": \"user\", \"content\": \"Hi\"}]).\n\n    Yields:\n    -------\n        Chunks of the assistant's response.\n    \"\"\"\n    # Validate messages\n    if not isinstance(messages, list) or not all(isinstance(m, dict) and 'role' in m and 'content' in m for m in messages):\n         raise ValueError(\"Messages must be a list of dictionaries with 'role' and 'content' keys.\")\n\n    # Always append system prompt and initial user message\n    file_path = importlib.resources.files('llm_ie.asset.PromptEditor_prompts').joinpath('chat.txt')\n    with open(file_path, 'r') as f:\n        chat_prompt_template = f.read()\n\n    guideline = self._apply_prompt_template(text_content={\"prompt_guideline\": self.prompt_guide}, \n                                            prompt_template=chat_prompt_template)\n\n    messages = [{\"role\": \"system\", \"content\": self.system_prompt + guideline}] + messages\n\n    stream_generator = self.inference_engine.chat(messages, stream=True)\n    yield from stream_generator\n</code></pre>"},{"location":"release_notes/v1.0.0/","title":"V1.0.0","text":""},{"location":"release_notes/v1.0.0/#highlights","title":"\u26a1Highlights","text":""},{"location":"release_notes/v1.0.0/#documentation-site","title":"Documentation Site","text":"<p>User guide, API reference, and documentation are moved to a dedicated Documentation Page.</p>"},{"location":"release_notes/v1.0.0/#web-application","title":"Web Application","text":"<p>We provide a drag-and-drop web Application for no-code access to the LLM-IE. The web Application streamlines the workflow:</p> <ol> <li>Prompt engineering with LLM agent</li> <li>Prompting algorithm design</li> <li>Visualization &amp; Validation</li> <li>Repeat step #1-#3 until achieves high accuracy</li> </ol>"},{"location":"release_notes/v1.0.0/#prompt-editor-tab","title":"Prompt Editor Tab","text":"<p>Select an LLM API (e.g., OpenAI, Azure, Ollama, Huggingface Hub) and LLM. Describe your task and the Prompt Editor LLM agent behind the scene will help construct a structured prompt template.</p> <p></p>"},{"location":"release_notes/v1.0.0/#frame-extraction-tab","title":"Frame Extraction Tab","text":"<p>Select an inferencing API and specify an LLM. In the \"Input Text\" textbox, paste the document text that you want to process. In the \"Prompt Template\" textbox, paste the prompt template you obtained from the previous step. Click on the \"Start Extraction\" button and watch LLM processes unit-by-unit on the right panel. </p> <p></p>"},{"location":"release_notes/v1.0.0/#result-viewer-tab","title":"Result Viewer Tab","text":"<p>Drop the result file from the previous step. Optionally, select the attribute key in the dropdown for color coding.</p> <p></p>"},{"location":"release_notes/v1.0.0/#refactored-frame-extractor","title":"Refactored Frame Extractor","text":"<p>The <code>FrameExtractor</code> has been developed over time. As new methods been added, we kept adding new classes (e.g., Sentence Review) which is exhausive. The new design separates chunking methods (e.g., while document, sentence, paragraph) and prompting method (e.g., direct and review). Chunking is now defined in <code>UnitChunker</code> and <code>ContextChunker</code>, while <code>FrameExtractor</code> defines prompting method. </p> <p>For example, what used to be  <code>SentenceReviewFrameExtractor</code> for sentence-level extraction and review.</p> <pre><code>from llm_ie import SentenceReviewFrameExtractor\n\nextractor = SentenceReviewFrameExtractor(inference_engine, prompt_temp, context_sentences=2, review_mode=\"revision\")\n</code></pre> <p>Is now implemented in a more flexible modular design. Users can specify using sentence as unit (extract from one sentence at a time), using a slide window of 2 sentences as context (additional context for each unit), and use review as prompting method. </p> <pre><code>from llm_ie import ReviewFrameExtractor, SentenceUnitChunker, SlideWindowContextChunker\n\nunit_chunker = SentenceUnitChunker()\ncontext_chunker = SlideWindowContextChunker(window_size=2)\nextractor = ReviewFrameExtractor(inference_engine=llm, \n                                 unit_chunker=unit_chunker,\n                                 context_chunker=context_chunker,\n                                 prompt_template=prompt_template,\n                                 review_mode=\"revision\")\n</code></pre> <p>Note that <code>BasicFrameExtractor</code>, <code>BasicReviewFrameExtractor</code> (previously <code>ReviewFrameExtractor</code>), <code>SentenceFrameExtractor</code>, and <code>SentenceReviewFrameExtractor</code> are still available as pre-packaged \"Convenience Frame Extractors\".</p>"},{"location":"release_notes/v1.0.0/#features-and-changes","title":"\ud83d\udce3Features and Changes","text":""},{"location":"release_notes/v1.0.0/#optimized-concurrent-processing","title":"Optimized concurrent processing","text":"<p>We now reimplemented concurrent processing in FrameExtractor with Semaphore. This allows dynamic allocation of available computation resources. Instead of waiting for a batch inferencing (e.g., sentences) to complete, the next units can start when a slot becomes available. This optimization increases thoughput while maintaning batch size. </p>"},{"location":"release_notes/v1.0.0/#simplified-frameextractor-schema","title":"Simplified FrameExtractor schema","text":"<p>\u26a0\ufe0f<code>entity_key</code> parameter in <code>extract_frames</code> method is now deprecated!</p> <p>We now regulate the <code>FrameExtractor</code> prompt template and post-processor to use <code>entity_text</code> for entity text and <code>attr</code> for attributes. This avoids the confusing <code>entity_key</code> parameter in <code>extract_frames</code> method and further strengthen accuracy of post-processing. </p> <p>The new LLM output schema is:</p> <pre><code>[\n    {\n        \"entity_text\": \"&lt;entity text&gt;\", \n        \"attr\": {\n                    \"&lt;attribute 1&gt;\": \"&lt;value 1&gt;\", \n                    \"&lt;attribute 2&gt;\": \"&lt;value 2&gt;\"\n                }\n    },\n    ...\n]\n</code></pre> <p>After post-processing, it becomes:</p> <pre><code>[\n    {\n        \"entity_text\": \"&lt;entity text&gt;\", \n        \"start\": &lt;start char&gt;,\n        \"end\": &lt;end char&gt;, \n        \"attr\": {\n                    \"&lt;attribute 1&gt;\": \"&lt;value 1&gt;\", \n                    \"&lt;attribute 2&gt;\": \"&lt;value 2&gt;\"\n                }\n    },\n    ...\n]\n</code></pre>"},{"location":"release_notes/v1.0.0/#added-streaming-method-to-frameextractor","title":"Added streaming method to FrameExtractor","text":"<p>We added streaming method, <code>FrameExtractor.stream</code> that returns a generator to support frontend development. To avoid confusion, the <code>stream</code> parameter in <code>FrameExtractor.extract</code> and <code>FrameExtractor.extract_frames</code> is now renamed to <code>verbose</code>. </p>"},{"location":"release_notes/v1.0.0/#removed-cot-frameextractor","title":"Removed CoT FrameExtractor","text":"<p>As reasoning models become more mature, we decide to reply on them instead. We added support to OpenAI's reasoning models (\"o\" serise) in version (v0.4.5). We will continue adding support to other reasoning models.</p> <p>\u26a0\ufe0f<code>SentenceCoTFrameExtractor</code> is now deprecated.</p>"},{"location":"release_notes/v1.1.0/","title":"V1.1.0","text":""},{"location":"release_notes/v1.1.0/#documentation-site","title":"\ud83d\udcd0Documentation Site","text":"<p>User guide, API reference, and documentation are available at Documentation Page.</p>"},{"location":"release_notes/v1.1.0/#highlights","title":"\u26a1Highlights","text":""},{"location":"release_notes/v1.1.0/#llm-specific-configuration-to-support-reasoning-models","title":"LLM-specific configuration to support reasoning models","text":"<p>To use reasoning models such as OpenAI o-series (e.g., o1, o3, o3-mini, o4-mini), some special processing is required. We provide dedicated configuration classes for them.</p>"},{"location":"release_notes/v1.1.0/#openai-o-series-reasoning-models","title":"OpenAI o-series reasoning models","text":"<p>OpenAI o-series reasoning model API does not allow setting system prompts. Contents in the system should be included in user prompts. Also, custom temperature is not allowed. We provide a dedicated configuration class <code>OpenAIReasoningLLMConfig</code> for these models. </p> <p><pre><code>from llm_ie.engines import OpenAIInferenceEngine, OpenAIReasoningLLMConfig\n\ninference_engine = OpenAIInferenceEngine(model=\"o1-mini\", \n                                         config=OpenAIReasoningLLMConfig(reasoning_effort=\"low\"))\n</code></pre> \u26a0\ufe0fThe <code>reasoning_model</code> parameter in <code>OpenAIInferenceEngine</code> is deprecated. Pass <code>OpenAIReasoningLLMConfig</code> class instead.</p>"},{"location":"release_notes/v1.1.0/#qwen3-hybrid-thinking-mode","title":"Qwen3 (hybrid thinking mode)","text":"<p>Qwen3 has a special way to manage reasoning behavior. The same models have thinking mode and non-thinking mode, controled by the prompting template. When a special token \"/think\" is appended to the user prompt, the models generate thinking tokens in a <code>&lt;think&gt;... &lt;/think&gt;</code> block. When  a special token \"/no_think\" is appended to the user prompt, the models generate an empty <code>&lt;think&gt;... &lt;/think&gt;</code> block. We provide a dedicated configuration class <code>Qwen3LLMConfig</code> for these models. </p> <pre><code>from llm_ie.engines import OpenAIInferenceEngine, Qwen3LLMConfig\n\n# Thinking mode\nllm = OpenAIInferenceEngine(base_url=\"http://localhost:8000/v1\", \n                            model=\"Qwen/Qwen3-30B-A3B\", \n                            api_key=\"EMPTY\", \n                            config=Qwen3LLMConfig(thinking_mode=True, temperature=0.8, max_tokens=8192))\n\n# Non-thinking mode\nllm = OpenAIInferenceEngine(base_url=\"http://localhost:8000/v1\", \n                            model=\"Qwen/Qwen3-30B-A3B\", \n                            api_key=\"EMPTY\", \n                            config=Qwen3LLMConfig(thinking_mode=False, temperature=0.0, max_tokens=2048))\n</code></pre>"},{"location":"release_notes/v1.1.0/#features-and-changes","title":"\ud83d\udce3Features and Changes","text":""},{"location":"release_notes/v1.1.0/#cleaner-ways-to-set-llm-sampling-configuration","title":"Cleaner ways to set LLM sampling configuration","text":"<p>LLM sampling parameters such as temperature, top-p, top-k, and maximum new tokens can be set by passing a <code>LLMConfig</code> class to the <code>InferenceEngine</code> constructor.</p> <pre><code>from llm_ie.engines import OpenAIInferenceEngine, BasicLLMConfig\n\nconfig = BasicLLMConfig(temperature=0.2, max_new_tokens=4096)\ninference_engine = OpenAIInferenceEngine(model=\"gpt-4o-mini\", config=config)\n</code></pre> <p>\u26a0\ufe0fThe <code>temperature</code> and <code>max_new_tokens</code> parameters in <code>FrameExtractor.extract_frames</code> method is deprecated. Use <code>LLMConfig</code> to set sampling parameters instead. </p>"},{"location":"release_notes/v1.2.0/","title":"V1.2.0","text":""},{"location":"release_notes/v1.2.0/#documentation-site","title":"\ud83d\udcd0Documentation Site","text":"<p>User guide, API reference, and documentation are available at Documentation Page.</p>"},{"location":"release_notes/v1.2.0/#highlights","title":"\u26a1Highlights","text":""},{"location":"release_notes/v1.2.0/#attribute-extractor","title":"Attribute Extractor","text":"<p>Added a dedicated attribute extractor to offload complicated attribute extraction tasks from frame extraction process. </p> <p>Although the <code>FrameExtractor</code> can extract both named entity and attributes in a single prompt, for some use cases where many attributes are required, offloading some attributes to the <code>AttributeExtractor</code> can improve the accuracy. For examlpe, in clinical notes, we want to extract treatments and diagnoses. For treatment frames, attributes like drug name, strength, dosage, frequency, and route are required; for diagnosis frames, diagnosed date, status, and ICD code are required. In such case, having a frame extractor to handle everything would be challenging. We can divide the treatment frames and diagnosis frames to their own attribute extractors</p> <p>To define an <code>AttributeExtractor</code> <pre><code>from llm_ie import AttributeExtractor\n\nextractor = AttributeExtractor(\n    inference_engine=llm,\n    prompt_template=prompt,\n)\n</code></pre></p> <p>To extract, input a list of frames. There could be some existing attributes. The <code>AttributeExtractor</code> prompts LLM with a frame and the context to extract attributes. Setting <code>inplace==True</code>, the new attibutes will be added/updated to the input frames, while setting <code>inplace=False</code>, new frames with attributes will be returned.  <pre><code>frame1 = LLMInformationExtractionFrame(frame_id=\"id1\", start=1026, end=1040, entity_text='Hyperlipidemia', attr={\"Date\": \"not available\"})\nframe2 = LLMInformationExtractionFrame(frame_id=\"id2\", start=1063, end=1087, entity_text='Type 2 Diabetes Mellitus')\nframe3 = LLMInformationExtractionFrame(frame_id=\"id3\", start=2046, end=2066, entity_text='No acute infiltrates')\n\n# extract attributes                               \nextractor.extract_attributes([frame1, frame2, frame3], note_text, verbose=True, inplace=True)\n# Use concurrent\n# extractor.extract_attributes([frame1, frame2, frame3], note_text, concurrent=True, inplace=True)\n</code></pre></p>"},{"location":"release_notes/v1.2.0/#features-and-changes","title":"\ud83d\udce3Features and Changes","text":""},{"location":"release_notes/v1.2.0/#optimized-relation-extractors","title":"Optimized Relation Extractors","text":"<p>Use Semaphore to manage batches and better utilize GPU power. </p>"},{"location":"release_notes/v1.2.1/","title":"V1.2.1","text":""},{"location":"release_notes/v1.2.1/#documentation-site","title":"\ud83d\udcd0Documentation Site","text":"<p>User guide, API reference, and documentation are available at Documentation Page.</p>"},{"location":"release_notes/v1.2.1/#highlights","title":"\u26a1Highlights","text":""},{"location":"release_notes/v1.2.1/#added-exportingimporting-chat-history-functionality-to-the-prompt-editor","title":"Added exporting/importing chat history functionality to the Prompt Editor","text":"<p>Issue #1 pointed out the need for accessing chat history. To address this, we have made the <code>PromptEditor.chat</code> stateful, and further added <code>export_chat</code> and <code>import_chat</code> methods. Now, users can easily export their chat history as a JSON file. The next time (even in a different session), import the chat history and continue the conversation. See documentation for more details. <pre><code>from llm_ie import OllamaInferenceEngine, PromptEditor, DirectFrameExtractor\n\n# Define an LLM inference engine\ninference_engine = OllamaInferenceEngine(model_name=\"llama4:scout\")\n\n# Define editor\neditor = PromptEditor(inference_engine, DirectFrameExtractor)\n\n# Start chat session\neditor.chat()\n\n# After a conversation, you can view the chat history\nprint(editor.messages)\n\n# Export chat history as a JSON file\neditor.export_chat(\"&lt;your chat history file name&gt;.json\")\n\n# In a new session, you can load the chat history and continue\nnew_editor = PromptEditor(inference_engine, DirectFrameExtractor)\n\n# Import chat history from a JSON file\nnew_editor.import_chat(\"&lt;your chat history file name&gt;.json\")\n\n# Continue the chat\nnew_editor.chat()\n\n# To delete the chat history\neditor.clear_messages()\n</code></pre></p>"},{"location":"release_notes/v1.2.1/#features-and-changes","title":"\ud83d\udce3Features and Changes","text":""},{"location":"release_notes/v1.2.1/#fixed-issues-with-reasoning_effort-in-openaireasoningllmconfig","title":"Fixed issues with <code>reasoning_effort</code> in <code>OpenAIReasoningLLMConfig</code>","text":"<p>Previously, the <code>reasoning_effort</code> parameter must be set as one of {\"low\", \"medium\", \"high\"}. However, some OpenAI reasoning models (e.g., o1-mini, o1-preview) do not support this parameter and would return errors. Now, for models that do not support setting reasoning effort, set <code>reasoning_effort</code> to None.</p>"},{"location":"release_notes/v1.2.2/","title":"V1.2.2","text":""},{"location":"release_notes/v1.2.2/#documentation-site","title":"\ud83d\udcd0Documentation Site","text":"<p>User guide, API reference, and documentation are available at Documentation Page.</p>"},{"location":"release_notes/v1.2.2/#highlights","title":"\u26a1Highlights","text":""},{"location":"release_notes/v1.2.2/#added-configs-for-reasoning-llms","title":"Added configs for reasoning LLMs","text":"<p>As reasoning LLMs show remarkable performance, we added <code>ReasoningLLMConfig</code> for general support. </p> Config class LLMs <code>BasicLLMConfig</code> Most non-reasoning LLMs  <li>Llama4 <li>Qwen3-30B-A3B-Instruct-2507 <li>... <code>ReasoningLLMConfig</code> Most reasoning LLMs  <li>Qwen3-30B-A3B-Thinking-2507 <li>gpt-oss-120b <li>... <code>Qwen3LLMConfig</code> Qwen3 hybrid thinking <li>Qwen3-30B-A3B <li>Qwen3-32B <li>... <code>OpenAIReasoningLLMConfig</code> OpenAI API reasoning models  <li>\"o\" series (o1, o3, o4) <li>... <pre><code>from llm_ie.engines import OpenAIInferenceEngine, ReasoningLLMConfig, Qwen3LLMConfig\n\n# Reasoning LLM\nllm = OpenAIInferenceEngine(base_url=\"http://localhost:8000/v1\", model=\"Qwen/Qwen3-30B-A3B-Thinking-2507\", api_key=\"EMPTY\", \n                            config=ReasoningLLMConfig(thinking_token_start=\"&lt;think&gt;\", thinking_token_end=\"&lt;/think&gt;\"))\n\n# Qwen3 hybrid thinking\nllm = OpenAIInferenceEngine(base_url=\"http://localhost:8000/v1\", model=\"Qwen/Qwen3-30B-A3B\", api_key=\"EMPTY\", \n                            config=Qwen3LLMConfig(thinking_mode=False, temperature=0.0, max_tokens=2048))\n</code></pre>"},{"location":"release_notes/v1.2.2/#messages-log-now-stores-reasoning-tokens","title":"Messages log now stores reasoning tokens","text":"<p>In <code>FrameExtractor.extract_frames</code>, <code>AttributeExtractor.extract_attributes</code>, and <code>RelationExtractor.extract_relations</code>, passing <code>return_messages_log=True</code> will return messages log with reasoning tokens.</p> <pre><code># extract frames\nframes, messages_log = extractor.extract_frames(text_content=note_text, verbose=True, return_messages_log=True)\n\nimport json\nprint(json.dumps(messages_log, indent=4))\n\n\"\"\"\n&gt; ...\n&gt; {\n&gt;    \"role\": \"assistant\",\n&gt;    \"content\": \"...\",\n&gt;    \"reasoning\": \"&lt;think&gt;... &lt;/think&gt;\"\n&gt; }\n\"\"\"\n</code></pre>"},{"location":"release_notes/v1.2.2/#features-and-changes","title":"\ud83d\udce3Features and Changes","text":""},{"location":"release_notes/v1.2.2/#inferenceenginechat-and-inferenceenginechat_async-methods-interface-update","title":"InferenceEngine.chat and InferenceEngine.chat_async methods interface update","text":"<p><code>InferenceEngine.chat</code> now returns <code>Union[Dict[str, str], Generator[Dict[str, str], None, None]]</code>. A dict {\"reasoning\": , \"response\": } or Generator {\"type\": , \"data\": } <p><code>InferenceEngine.chat_async</code> now returns <code>Dict[str,str]</code>. A dict {\"reasoning\": , \"response\": }"},{"location":"release_notes/v1.2.3/","title":"V1.2.3","text":""},{"location":"release_notes/v1.2.3/#documentation-site","title":"\ud83d\udcd0Documentation Site","text":"<p>User guide, API reference, and documentation are available at Documentation Page.</p>"},{"location":"release_notes/v1.2.3/#highlights","title":"\u26a1Highlights","text":""},{"location":"release_notes/v1.2.3/#added-specific-inference-engines-for-vllm-and-openrouter","title":"Added specific inference engines for vLLM and OpenRouter","text":"<p>Due to the increasing interface differences between OpenAI/Azure and OpenAI-compatible services, we now separate the support. vLLM OpenAI-compatible server is now supported through <code>VLLMInferenceEngine</code>. OpenRouter is supported through <code>OpenRouterInferenceEngine</code>.</p>"},{"location":"release_notes/v1.2.3/#example-gpt-oss-120b","title":"Example: gpt-oss-120b","text":"<p>Start the server in command line. Specify <code>--reasoning-parser GptOss</code> to enable the reasoning parser.  <pre><code>vllm serve openai/gpt-oss-120b \\\n    --tensor-parallel-size 4 \\\n    --enable-prefix-caching \\\n    --reasoning-parser GptOss\n</code></pre> Define inference engine <pre><code>from llm_ie.engines import VLLMInferenceEngine, ReasoningLLMConfig\n\ninference_engine = VLLMInferenceEngine(model=\"openai/gpt-oss-120b\", \n                                       config=ReasoningLLMConfig(temperature=1.0, top_p=1.0, top_k=0))\n</code></pre></p>"},{"location":"release_notes/v1.2.3/#features-and-changes","title":"\ud83d\udce3Features and Changes","text":""},{"location":"release_notes/v1.2.3/#strengthen-reasoning-model-support-for-ollama","title":"Strengthen reasoning model support for Ollama","text":"<p>The <code>OllamaInferenceEngine</code> now streams reasoning tokens and stores them in messages_log with <code>ReasoningLLMConfig</code>.</p>"},{"location":"release_notes/v1.2.3/#fixed-empty-output-bug-in-reasoning-llms","title":"Fixed empty output bug in reasoning LLMs","text":"<p>Previously, when a reasoning LLM hits max_new_tokens during reasoning and fails to generate final outputs, the <code>InferenceEngine.chat</code> output would be None. This causes downstream errors in extractors. We added a default empty dictionary <code>{\"reasoning\": \"\", \"response\": \"\"}</code> to handle it.  Changes are made in <code>ReasoningLLMConfig.postprocess_response</code>.</p>"},{"location":"release_notes/v1.2.3/#added-separatorunitchunker","title":"Added SeparatorUnitChunker","text":"<p>A unit chunker that chunks by a text patter. For example, <code>\"\\n\\n\"</code> for paragraph chunking. This is useful for chunking documents with a clear separator pattern.</p>"},{"location":"release_notes/v1.2.3/#removed-frameextractionunitresults","title":"Removed FrameExtractionUnitResults","text":"<p>The internal dataclass <code>FrameExtractionUnitResults</code> is merged with <code>FrameExtractionUnit</code>. <code>FrameExtractionUnit</code> now has <code>gen_text</code> and <code>status</code>. This update makes <code>FrameExtractor</code> cleaner. </p>"},{"location":"release_notes/v1.2.3/#added-messageslogger-that-listens-to-inferenceengine","title":"Added MessagesLogger that listens to InferenceEngine","text":"<p>A <code>MessagesLogger</code> object can be send to <code>InferenceEngine.chat</code> and <code>InferenceEngine.chat_async</code> to log input, output, and reasoning tokens. This simplifies <code>return_messages_log</code> implementation in <code>Extractor.extract_</code> methods.</p>"}]}